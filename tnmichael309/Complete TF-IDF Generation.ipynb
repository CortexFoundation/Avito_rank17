{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc; gc.enable()\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "import nltk; nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords                \n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['data/train.csv', 'data/test.csv', 'data/train_active.csv', 'data/test_active.csv']\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = pd.read_csv(train_path, usecols=['item_id']).shape[0]\n",
    "test_len = pd.read_csv(test_path, usecols=['item_id']).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = RussianStemmer(ignore_stopwords=False)\n",
    "def clean_text(txt):\n",
    "    txt = str(txt).lower().strip().split(\" \\t\\r.,!?^+-*/@~:;/\\\\\\\"\\'&{}[]()#$%\") #str(txt).split(\" \") #\n",
    "    txt = [stemmer.stem(wrd) for wrd in txt \\\n",
    "                if wrd not in stopwords.words('russian') and len(wrd) > 1]\n",
    "    txt = u\" \".join(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_chunk(df, is_title):\n",
    "    if is_title:\n",
    "        df['title'].fillna('unknowntitle', inplace=True)\n",
    "    else:\n",
    "        df['description'].fillna('unknowndescription', inplace=True)\n",
    "        print('text cleaning!')\n",
    "        df['description'] = [clean_text(text) for text in tqdm(df['description'].values)]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_generator(is_title, is_partial=False):\n",
    "    for f in files:\n",
    "        print('Processing file:', f)\n",
    "\n",
    "        if is_title:\n",
    "            target_col = 'title'\n",
    "        else:\n",
    "            target_col = 'description'\n",
    "\n",
    "        usecols = [target_col, 'activation_date']\n",
    "        for chunk in pd.read_csv(f, usecols=usecols, chunksize=2000000, parse_dates=['activation_date']):\n",
    "            if f == train_path:\n",
    "                chunk = chunk.sort_values('activation_date').reset_index(drop=True)\n",
    "                print('Index reset!')\n",
    "\n",
    "            chunk = chunk.drop('activation_date', axis=1)   \n",
    "            chunk = process_chunk(chunk, is_title); gc.collect()\n",
    "\n",
    "            for s in tqdm(chunk[target_col].values):\n",
    "                yield s    \n",
    "                \n",
    "            del chunk; gc.collect()\n",
    "            \n",
    "            if is_partial and f == test_path:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer_title = CountVectorizer(stop_words=stopwords.words('russian'), lowercase=True, min_df=25)\n",
    "count_vectorizer_title.fit(text_generator(is_title=True))\n",
    "title_feature = count_vectorizer_title.transform(text_generator(is_title=True, is_partial=True))\n",
    "\n",
    "with open('complete_title_count_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(title_feature, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('title text features saved')\n",
    "    del title_feature; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            lowercase=True,\n",
    "                            analyzer='word',\n",
    "                            smooth_idf=True,\n",
    "                            sublinear_tf=True,\n",
    "                            max_features=30000,\n",
    "                            max_df=0.9, stop_words=stopwords.words('russian'),\n",
    "                            norm='l2')\n",
    "tfidf_vec.fit(text_generator(is_title=False))\n",
    "desc_tf_idf_features = tfidf_vec.transform(text_generator(is_title=False, is_partial=True))\n",
    "\n",
    "print('TFIDF transformation done.')\n",
    "with open('complete_desc_tfidf_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(desc_tf_idf_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('desc text features saved')\n",
    "    del desc_tf_idf_features; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text_sparse = hstack([\n",
    "    csr_matrix(normalize(pickle.load(open('complete_title_count_vec.pickle', 'rb')), norm='l2', axis=1)),\n",
    "    csr_matrix(pickle.load(open('complete_desc_tfidf_vec.pickle', 'rb')))\n",
    "]).tocsr()\n",
    "\n",
    "with open('train_complete_text_sparse_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_text_sparse[:train_len,:], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test_complete_text_sparse_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_text_sparse[train_len:,:], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text_dense = TruncatedSVD(n_components=150).fit_tranform(all_text_sparse)\n",
    "\n",
    "with open('train_complete_text_dense_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_text_dense[:train_len,:], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('test_complete_text_dense_vec.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_text_dense[train_len:,:], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
