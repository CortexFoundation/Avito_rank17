{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: \n",
      "norm = True \n",
      "min_df = 5 \n",
      "max_features = 262144\n",
      "CounterVectorizer Params:\n",
      " {'ngram_range': (2, 4), 'analyzer': 'char_wb', 'min_df': 5, 'max_df': 0.85, 'max_features': 262144, 'lowercase': True, 'dtype': <class 'numpy.float32'>}\n",
      "\n",
      "Vocabulary collection done. Vocabulary size = 1302267\n",
      "\n",
      "\n",
      "Total batches to execute: 2\n",
      "Processed features #: 1048576 , Remaining features #:  345792 \n",
      "\n",
      "Features Saved.\n",
      "Feature Names Saved.\n",
      "Processed features #: 253691 , Remaining features #:  83595 \n",
      "\n",
      "Features Saved.\n",
      "Feature Names Saved.\n",
      "Total batches to execute: 2\n",
      "processing 0\n",
      "Updated min df = 6\n",
      "Updated min df = 7\n",
      "Updated min df = 8\n",
      "Updated min df = 9\n",
      "Updated min df = 10\n",
      "processing 1\n",
      "Updated min df = 11\n",
      "Updated min df = 12\n",
      "Updated min df = 13\n",
      "Updated min df = 14\n",
      "Updated min df = 15\n",
      "Final shape: (2011862, 261585)\n",
      "All Features Merged and Saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import gc; gc.enable()   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor as thread_pool\n",
    "import itertools\n",
    "from copy import deepcopy as cp\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import os\n",
    "\n",
    "class CustomCounterVectorizer():\n",
    "    def __init__(self, save_folder, norm=True, **counter_vectorizer_params):\n",
    "        self.save_folder = save_folder\n",
    "        if not os.path.exists(save_folder):\n",
    "            os.makedirs(save_folder)\n",
    "    \n",
    "        self.default_params = counter_vectorizer_params\n",
    "        self.norm = norm\n",
    "        self.min_df = counter_vectorizer_params['min_df'] if 'min_df' in counter_vectorizer_params else 1\n",
    "        self.max_features = counter_vectorizer_params['max_features'] if 'max_features' in counter_vectorizer_params else 2**20\n",
    "        \n",
    "        print('Params: \\nnorm =', self.norm, '\\nmin_df =', self.min_df, '\\nmax_features =', self.max_features)\n",
    "        print('CounterVectorizer Params:\\n', counter_vectorizer_params)\n",
    "        print('')\n",
    "    \n",
    "    def collect_vocab(self, X):\n",
    "        cv = CountVectorizer(**self.default_params)\n",
    "        self.analyze_func = cv.build_analyzer()\n",
    "        self.voc_pool = set()\n",
    "        \n",
    "        '''\n",
    "        cpu_count = 4\n",
    "        with thread_pool(max_workers=cpu_count) as executor:\n",
    "            for i, res in tqdm(enumerate(executor.map(self.analyze_func, X))):\n",
    "                self.voc_pool.update(res)\n",
    "        '''\n",
    "        for x in X:\n",
    "            self.voc_pool.update(self.analyze_func(x))\n",
    "        \n",
    "        print('Vocabulary collection done. Vocabulary size =', len(self.voc_pool))  \n",
    "        print('\\n')\n",
    "        #print(X, '\\n\\tafter analyzed ==>', self.voc_pool)\n",
    "    \n",
    "    def counter_vectorize(self, data):\n",
    "        X = data[0]\n",
    "        vocs = data[1]\n",
    "        params = cp(self.default_params)\n",
    "        params.update({'vocabulary': list(vocs)})\n",
    "        cv = CountVectorizer(**params)\n",
    "        res = cv.fit_transform(X)\n",
    "        return cv, res\n",
    "        \n",
    "    def minibatch_counter_vectorize(self, X):\n",
    "        \n",
    "        chunksize = 2**20\n",
    "        self.voc_pool = list(self.voc_pool)\n",
    "        voc_chunks = [self.voc_pool[i*chunksize: (i+1)*chunksize] for i in range(len(self.voc_pool)//chunksize + 1)] \n",
    "        print('Total batches to execute:', len(voc_chunks))\n",
    "        \n",
    "        cpu_count = 4\n",
    "        for i, voc_chunk in enumerate(voc_chunks):\n",
    "            #if i == 0:\n",
    "            #    continue\n",
    "                \n",
    "            cv, res = self.counter_vectorize((X, voc_chunk))\n",
    "            processed_feature_names = cv.get_feature_names()\n",
    "            cond_filter = np.array(np.clip(res.getnnz(axis=0) - self.min_df, 0, 1), dtype=bool)\n",
    "            remaining_feature_names = np.array(processed_feature_names)[cond_filter]\n",
    "            res = res[:, cond_filter]; gc.collect()\n",
    "            print('Processed features #:', len(processed_feature_names), ', Remaining features #: ', len(remaining_feature_names), '\\n')\n",
    "    \n",
    "            if res is None:\n",
    "                print('No results remain!')\n",
    "                continue\n",
    "            \n",
    "            with open('{}text_features_{}.pickle'.format(self.save_folder, i+1), 'wb') as handle:\n",
    "                pickle.dump(res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print('Features Saved.')\n",
    "            \n",
    "            with open('{}text_feature_names_{}.pickle'.format(self.save_folder, i+1), 'wb') as handle:\n",
    "                pickle.dump(remaining_feature_names, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print('Feature Names Saved.')\n",
    "                \n",
    "            del res, processed_feature_names, cond_filter, remaining_feature_names\n",
    "            gc.collect()\n",
    "    \n",
    "                \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.collect_vocab(X)\n",
    "        self.minibatch_counter_vectorize(X)\n",
    "        \n",
    "        chunksize = 2**20\n",
    "        self.voc_pool = list(self.voc_pool)\n",
    "        voc_chunks = [self.voc_pool[i*chunksize: (i+1)*chunksize] for i in range(len(self.voc_pool)//chunksize + 1)] \n",
    "        print('Total batches to execute:', len(voc_chunks))\n",
    "        \n",
    "        cpu_count = 4\n",
    "        final_res = None\n",
    "        for i, _ in enumerate(voc_chunks):\n",
    "            print('processing', i)\n",
    "            with open('{}text_features_{}.pickle'.format(self.save_folder, i+1), 'rb') as handle:\n",
    "                if i == 0:\n",
    "                    final_res = pickle.load(handle)\n",
    "                else:\n",
    "                    old_final_res = final_res\n",
    "                    final_res = hstack([final_res, pickle.load(handle)]).tocsr()\n",
    "                    del old_final_res; gc.collect()\n",
    "                    cond_filter = np.array(np.clip(final_res.getnnz(axis=0) - self.min_df, 0, 1), dtype=bool)\n",
    "                    final_res = final_res[:, cond_filter]; gc.collect()\n",
    "                    \n",
    "                while final_res.shape[1] > self.max_features:\n",
    "                    self.min_df += 1\n",
    "                    print('Updated min df =', self.min_df)\n",
    "                    cond_filter = np.array(np.clip(final_res.getnnz(axis=0) - self.min_df, 0, 1), dtype=bool)\n",
    "                    final_res = final_res[:, cond_filter]; gc.collect()\n",
    "                        \n",
    "        print('Final shape:', final_res.shape)\n",
    "        with open('{}text_features_all.pickle'.format(self.save_folder), 'wb') as handle:\n",
    "            pickle.dump(final_res, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('All Features Merged and Saved.')\n",
    "        del final_res; gc.collect()\n",
    "\n",
    "params = {\n",
    "    'ngram_range': (2, 4), \n",
    "    'analyzer': 'char_wb',\n",
    "    'min_df': 5,\n",
    "    'max_df': .85,\n",
    "    'max_features': 2**18,\n",
    "    'lowercase': True,\n",
    "    'dtype': np.float32    \n",
    "}\n",
    "\n",
    "with open('desc_text_feature.pickle', 'rb') as handle:\n",
    "    text_feature = pickle.load(handle)\n",
    "ccv = CustomCounterVectorizer('0530_CV_CHAR_WB_NGRAM_24/', norm=True, **params)\n",
    "ccv.fit_transform(text_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comm_feature = 'all_features.pickle'\n",
    "text_feature = 'CV_CHAR_WB_NGRAM_24/text_features_all.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc; gc.enable()\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "with open(comm_feature, 'rb') as handle1:\n",
    "    comm_feature = pickle.load(handle1).astype(np.float32)\n",
    "    print(type(comm_feature))\n",
    "    \n",
    "with open(text_feature, 'rb') as handle2:\n",
    "    text_feature = pickle.load(handle2)\n",
    "    text_feature = normalize(text_feature)\n",
    "    print(type(text_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = hstack([comm_feature, text_feature]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features = all_features.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2011862, 290474)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_npz('all_features_cv_charwb24.npz', all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
