{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, GRU, Bidirectional, LSTM, Dense, Embedding, concatenate, Embedding, Flatten, Activation, BatchNormalization, regularizers\n",
    "from keras.initializers import Orthogonal\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback, Callback, LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import gc; gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3905638275003752137\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104844492\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8885604699395045465\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/gpu:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'cc.ru.300.vec'\n",
    "TRAIN_CSV = 'train.csv'\n",
    "TEST_CSV = 'test.csv'\n",
    "DENSE_FEATURE_PATH = 'all_features_dense.pickle'\n",
    "RAW_TEXT_PATH = 'text_feature_space_split_only.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickled_data(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "fitting tokenizer\n",
      "getting embeddings\n",
      "setup max info\n",
      "train-test-val split\n",
      "Sample num sanity test (should be equal) 1052396 1052396 1052396\n",
      "Sample num sanity test (should be equal) 451028 451028 451028\n",
      "<class 'numpy.ndarray'> <class 'list'>\n",
      "convert to sequences\n",
      "padding\n",
      "['user_id', 'region', 'city', 'parent_category_name', 'category_name', 'param_1', 'param_2', 'param_3', 'price', 'item_seq_number', 'user_type', 'image_top_1', 'same_activate_cnt', 'duration_1_mean', 'duration_1_med', 'duration_1_min', 'duration_1_max', 'duration_1_std', 'duration_2_mean', 'duration_2_med', 'duration_2_min', 'duration_2_max', 'duration_2_std', 'duration_3_mean', 'duration_3_med', 'duration_3_min', 'duration_3_max', 'duration_3_std', 'renewed_count_mean', 'renewed_count_med', 'renewed_count_min', 'renewed_count_max', 'renewed_count_std', 'is_renewed_mean', 'is_renewed_med', 'is_renewed_std', 'param_1_is_na', 'param_2_is_na', 'param_3_is_na', 'description_is_na', 'price_is_na', 'image_top_1_is_na', 'region_in_title', 'region_in_title_counts', 'city_in_title', 'city_in_title_counts', 'parent_category_name_in_title', 'parent_category_name_in_title_counts', 'category_name_in_title', 'category_name_in_title_counts', 'region_in_description', 'region_in_description_counts', 'city_in_description', 'city_in_description_counts', 'parent_category_name_in_description', 'parent_category_name_in_description_counts', 'category_name_in_description', 'category_name_in_description_counts', 'title_in_description', 'title_in_description_counts', 'desc_char_count', 'space_count', 'surprise_count', 'question_count', 'quote_count', 'quote_count2', 'desc_unique_words_count', 'desc_percnt_unique_words', 'region_city', 'parent_category_name_category_name', 'parent_category_name_param_1', 'parent_category_name_param_2', 'parent_category_name_param_3', 'category_name_param_1', 'category_name_param_2', 'category_name_param_3', 'parent_category_name_region', 'category_name_region', 'Weekday']\n"
     ]
    }
   ],
   "source": [
    "max_features = 100000\n",
    "maxlen = 150\n",
    "embed_size = 300\n",
    "\n",
    "print('loading data')\n",
    "train = pd.read_csv(TRAIN_CSV, parse_dates=['activation_date'])\n",
    "train = train.sort_values('activation_date').reset_index(drop=True)\n",
    "labels = train['deal_probability'].values\n",
    "train_len = len(labels)\n",
    "del train; gc.collect()\n",
    "dense_features = load_pickled_data(DENSE_FEATURE_PATH); \n",
    "texts = list(load_pickled_data(RAW_TEXT_PATH))\n",
    "\n",
    "print('fitting tokenizer')\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "print('getting embeddings')\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding = 'utf8'))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('setup max info')\n",
    "max_info = dict((col, dense_features[col].max()) for col in dense_features.columns)\n",
    "\n",
    "print('train-test-val split')\n",
    "dense_train = dense_features.loc[:train_len-1,:]; del dense_features; gc.collect()\n",
    "texts_train = texts[:train_len]; del texts; gc.collect()\n",
    "dense_val = dense_train.loc[int(train_len*.7):,].reset_index(drop=True)\n",
    "dense_train = dense_train.loc[:int(train_len*.7)-1,].reset_index(drop=True)\n",
    "texts_val = texts_train[int(train_len*.7):]\n",
    "texts_train = texts_train[:int(train_len*.7)]\n",
    "y_val = labels[int(train_len*.7):]\n",
    "y_train = labels[:int(train_len*.7)]\n",
    "print('Sample num sanity test (should be equal)', dense_train.shape[0], len(y_train), len(texts_train))\n",
    "print('Sample num sanity test (should be equal)', dense_val.shape[0], len(y_val), len(texts_val))\n",
    "print(type(y_train), type(texts_train))\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(train['description'].values, labels['deal_probability'].values, test_size = 0.1, random_state = 23)\n",
    "\n",
    "print('convert to sequences')\n",
    "texts_train = tokenizer.texts_to_sequences(texts_train)\n",
    "texts_val = tokenizer.texts_to_sequences(texts_val)\n",
    "\n",
    "print('padding')\n",
    "texts_train = sequence.pad_sequences(texts_train, maxlen=maxlen)\n",
    "texts_val = sequence.pad_sequences(texts_val, maxlen=maxlen)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.clip(y_pred, 0., 1.) - y_true), axis=-1))\n",
    "print(dense_train.columns.tolist())\n",
    "\n",
    "X_train = dict((col, dense_train[col].values) for col in dense_train.columns)\n",
    "X_train['text'] = texts_train\n",
    "X_val = dict((col, dense_val[col].values) for col in dense_val.columns)\n",
    "X_val['text'] = texts_val\n",
    "\n",
    "cat_features_to_embed = ['user_id', 'user_type', 'region', 'city', 'parent_category_name', 'category_name', \n",
    "                        'param_1', 'param_2', 'param_3', 'item_seq_number',  'image_top_1',  'region_city', \n",
    "                        'parent_category_name_category_name', 'parent_category_name_param_1', 'parent_category_name_param_2', 'parent_category_name_param_3', \n",
    "                        'category_name_param_1', 'category_name_param_2', 'category_name_param_3', 'parent_category_name_region', 'category_name_region', \n",
    "                        #'parent_category_name_city', 'category_name_city', 'parent_category_name_image_top_1', 'category_name_image_top_1', \n",
    "                        'Weekday'] #, 'dom']\n",
    "                        \n",
    "other_features = ['price', 'region_in_title', 'region_in_title_counts', 'city_in_title', 'city_in_title_counts', \n",
    "                'parent_category_name_in_title', 'parent_category_name_in_title_counts', 'category_name_in_title', 'category_name_in_title_counts', \n",
    "                'region_in_description', 'region_in_description_counts', 'city_in_description', 'city_in_description_counts', \n",
    "                'parent_category_name_in_description', 'parent_category_name_in_description_counts', 'category_name_in_description', 'category_name_in_description_counts', \n",
    "                'title_in_description', 'title_in_description_counts', 'desc_char_count', 'space_count', 'surprise_count', 'question_count', 'quote_count', 'quote_count2',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(X):\n",
    "    \n",
    "    # non-cat features\n",
    "    non_cat_inputs = []\n",
    "    for col in other_features:\n",
    "        f = Input(shape=[1], name=col)\n",
    "        non_cat_inputs.append(f)\n",
    "        \n",
    "    # cat features\n",
    "    cat_inputs = []\n",
    "    cat_embeds = []\n",
    "    for col in cat_features_to_embed:\n",
    "        f = Input(shape=[1], name=col)\n",
    "        embed_dim = max_info[col].max()+1\n",
    "        reduced_dim = max(2, embed_dim//4)\n",
    "        embed_f = Embedding(embed_dim, reduced_dim)(f)\n",
    "        flatten_f = Flatten()(embed_f)\n",
    "        cat_inputs.append(f)\n",
    "        cat_embeds.append(flatten_f)\n",
    "      \n",
    "    # text features\n",
    "    text_inp = Input(shape = (maxlen, ), name='text')\n",
    "    '''\n",
    "    text_emb = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n",
    "                    input_length = maxlen, trainable = False)(text_inp)\n",
    "    text_emb = SpatialDropout1D(0.1)(text_emb)\n",
    "    text_gru = Bidirectional(GRU(32,return_sequences = True))(text_emb)\n",
    "    text_gru = GlobalAveragePooling1D()(text_gru)\n",
    "    text_gru = Dropout(0.1)(text_gru)\n",
    "    '''\n",
    "    text_emb = Embedding(nb_words, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(text_inp)\n",
    "    text_emb = Bidirectional(LSTM(128, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(text_emb)\n",
    "    text_gru = Attention(maxlen)(text_emb)\n",
    "    \n",
    "    concat_main = non_cat_inputs+cat_embeds+[text_gru]\n",
    "    main = concatenate(concat_main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = BatchNormalization()(Dense(256, activation='relu')(main))\n",
    "    main = BatchNormalization()(Dense(128, activation='relu')(main))\n",
    "    main = BatchNormalization()(Dense(64, activation='relu')(main))\n",
    "    out = Dense(1, activation = \"linear\")(main)\n",
    "\n",
    "    concat_input = non_cat_inputs+cat_inputs+[text_inp]\n",
    "    model = Model(concat_input, out)\n",
    "    model.regularizers = [regularizers.l2(0.0001)]\n",
    "    model.compile(optimizer = Adam(lr=0.0005), loss = 'mean_squared_error',\n",
    "                  metrics =[root_mean_squared_error])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "def clip_rmse(true, prediction):\n",
    "    return np.sqrt(metrics.mean_squared_error(true, np.clip(prediction, 0., 1.)))\n",
    "    \n",
    "class NBatchEvalLogger(Callback):\n",
    "    def __init__(self, display, val_X, val_y, save_path=None, save_start=1000):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y\n",
    "        self.best_loss = None\n",
    "        self.save_path = save_path\n",
    "        self.save_start = save_start\n",
    "        self.record_count = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        if self.step % self.display == 0 and self.step >= self.save_start:\n",
    "            #loss, metric = self.model.evaluate(self.val_X, self.val_y, batch_size=128, verbose=1)\n",
    "            prediction = self.model.predict(self.val_X, batch_size=128, verbose=0)\n",
    "            loss = clip_rmse(self.val_y, prediction)\n",
    "            \n",
    "            if self.best_loss is None:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "                    if self.save_path is not None:\n",
    "                        self.model.save(self.save_path.replace('model','model_'+str(self.record_count)), overwrite=True)\n",
    "                        self.record_count += 1\n",
    "                    \n",
    "            print('\\nstep: {} val loss={:.5f}, best loss={:.5f}'.format(self.step, loss, self.best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1052396 samples, validate on 451028 samples\n",
      "Epoch 1/8\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "\n",
      "step: 2000 val loss=0.23400, best loss=0.23400\n",
      "\n",
      "step: 3000 val loss=0.23245, best loss=0.23245\n",
      "\n",
      "step: 4000 val loss=0.23193, best loss=0.23193\n",
      "\n",
      "step: 5000 val loss=0.23577, best loss=0.23193\n",
      "\n",
      "step: 6000 val loss=0.23270, best loss=0.23193\n",
      "\n",
      "step: 7000 val loss=0.23096, best loss=0.23096\n",
      "\n",
      "step: 8000 val loss=0.23227, best loss=0.23096\n",
      " - 7201s - loss: 0.0577 - root_mean_squared_error: 0.1595 - val_loss: 0.0532 - val_root_mean_squared_error: 0.1494\n",
      "Epoch 2/8\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.001.\n",
      "\n",
      "step: 9000 val loss=0.23053, best loss=0.23053\n",
      "\n",
      "step: 10000 val loss=0.23463, best loss=0.23053\n",
      "\n",
      "step: 11000 val loss=0.23030, best loss=0.23030\n",
      "\n",
      "step: 12000 val loss=0.22970, best loss=0.22970\n",
      "\n",
      "step: 13000 val loss=0.22934, best loss=0.22934\n",
      "\n",
      "step: 14000 val loss=0.22879, best loss=0.22879\n",
      "\n",
      "step: 15000 val loss=0.23082, best loss=0.22879\n",
      "\n",
      "step: 16000 val loss=0.22907, best loss=0.22879\n",
      " - 7510s - loss: 0.0519 - root_mean_squared_error: 0.1519 - val_loss: 0.0518 - val_root_mean_squared_error: 0.1538\n",
      "Epoch 3/8\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.001.\n",
      "\n",
      "step: 17000 val loss=0.22963, best loss=0.22879\n",
      "\n",
      "step: 18000 val loss=0.22961, best loss=0.22879\n",
      "\n",
      "step: 19000 val loss=0.23141, best loss=0.22879\n",
      "\n",
      "step: 20000 val loss=0.22761, best loss=0.22761\n",
      "\n",
      "step: 21000 val loss=0.22919, best loss=0.22761\n",
      "\n",
      "step: 22000 val loss=0.22831, best loss=0.22761\n",
      "\n",
      "step: 23000 val loss=0.22802, best loss=0.22761\n",
      "\n",
      "step: 24000 val loss=0.22778, best loss=0.22761\n",
      " - 7436s - loss: 0.0505 - root_mean_squared_error: 0.1487 - val_loss: 0.0515 - val_root_mean_squared_error: 0.1515\n",
      "Epoch 4/8\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.001.\n",
      "\n",
      "step: 25000 val loss=0.22924, best loss=0.22761\n",
      "\n",
      "step: 26000 val loss=0.22793, best loss=0.22761\n",
      "\n",
      "step: 27000 val loss=0.22803, best loss=0.22761\n",
      "\n",
      "step: 28000 val loss=0.22985, best loss=0.22761\n",
      "\n",
      "step: 29000 val loss=0.22797, best loss=0.22761\n",
      "\n",
      "step: 30000 val loss=0.22791, best loss=0.22761\n",
      "\n",
      "step: 31000 val loss=0.22713, best loss=0.22713\n",
      "\n",
      "step: 32000 val loss=0.22831, best loss=0.22713\n",
      " - 7441s - loss: 0.0490 - root_mean_squared_error: 0.1455 - val_loss: 0.0514 - val_root_mean_squared_error: 0.1446\n",
      "Epoch 5/8\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "\n",
      "step: 33000 val loss=0.22541, best loss=0.22541\n",
      "\n",
      "step: 34000 val loss=0.22547, best loss=0.22541\n",
      "\n",
      "step: 35000 val loss=0.22527, best loss=0.22527\n",
      "\n",
      "step: 36000 val loss=0.22545, best loss=0.22527\n",
      "\n",
      "step: 37000 val loss=0.22552, best loss=0.22527\n",
      "\n",
      "step: 38000 val loss=0.22567, best loss=0.22527\n",
      "\n",
      "step: 39000 val loss=0.22579, best loss=0.22527\n",
      "\n",
      "step: 40000 val loss=0.22565, best loss=0.22527\n",
      "\n",
      "step: 41000 val loss=0.22551, best loss=0.22527\n",
      " - 7805s - loss: 0.0459 - root_mean_squared_error: 0.1387 - val_loss: 0.0509 - val_root_mean_squared_error: 0.1468\n",
      "Epoch 6/8\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "\n",
      "step: 42000 val loss=0.22604, best loss=0.22527\n",
      "\n",
      "step: 43000 val loss=0.22590, best loss=0.22527\n",
      "\n",
      "step: 44000 val loss=0.22661, best loss=0.22527\n",
      "\n",
      "step: 45000 val loss=0.22578, best loss=0.22527\n",
      "\n",
      "step: 46000 val loss=0.22613, best loss=0.22527\n",
      "\n",
      "step: 47000 val loss=0.22579, best loss=0.22527\n",
      "\n",
      "step: 48000 val loss=0.22618, best loss=0.22527\n",
      "\n",
      "step: 49000 val loss=0.22609, best loss=0.22527\n",
      " - 7438s - loss: 0.0450 - root_mean_squared_error: 0.1367 - val_loss: 0.0514 - val_root_mean_squared_error: 0.1475\n",
      "Epoch 7/8\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "\n",
      "step: 50000 val loss=0.22659, best loss=0.22527\n",
      "\n",
      "step: 51000 val loss=0.22695, best loss=0.22527\n",
      "\n",
      "step: 52000 val loss=0.22668, best loss=0.22527\n",
      "\n",
      "step: 53000 val loss=0.22658, best loss=0.22527\n",
      "\n",
      "step: 54000 val loss=0.22667, best loss=0.22527\n",
      "\n",
      "step: 55000 val loss=0.22669, best loss=0.22527\n",
      "\n",
      "step: 56000 val loss=0.22692, best loss=0.22527\n",
      "\n",
      "step: 57000 val loss=0.22679, best loss=0.22527\n",
      " - 7431s - loss: 0.0444 - root_mean_squared_error: 0.1353 - val_loss: 0.0513 - val_root_mean_squared_error: 0.1469\n",
      "Epoch 8/8\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "\n",
      "step: 58000 val loss=0.22675, best loss=0.22527\n",
      "\n",
      "step: 59000 val loss=0.22741, best loss=0.22527\n",
      "\n",
      "step: 60000 val loss=0.22716, best loss=0.22527\n",
      "\n",
      "step: 61000 val loss=0.22718, best loss=0.22527\n",
      "\n",
      "step: 62000 val loss=0.22704, best loss=0.22527\n",
      "\n",
      "step: 63000 val loss=0.22729, best loss=0.22527\n",
      "\n",
      "step: 64000 val loss=0.22711, best loss=0.22527\n",
      "\n",
      "step: 65000 val loss=0.22670, best loss=0.22527\n",
      " - 7431s - loss: 0.0439 - root_mean_squared_error: 0.1342 - val_loss: 0.0517 - val_root_mean_squared_error: 0.1460\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "model = build_model(X_train)\n",
    "file_path = \"rnn_weights/model.hdf5\"\n",
    "  \n",
    "#check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "lr_schd = LearningRateScheduler(lambda epoch: 0.001*(0.1**(epoch//4)), verbose=1)\n",
    "check_point = NBatchEvalLogger(1000, X_val, y_val, save_path=file_path, save_start=2000)\n",
    "history = model.fit(X_train, y_train, batch_size = 128, epochs = EPOCHS, validation_data = (X_val, y_val),\n",
    "                verbose = 2, callbacks = [lr_schd, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_bg_count = 5\n",
    "bagging_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame()\n",
    "for i in range(max_bg_count-bagging_count, max_bg_count, 1):\n",
    "    model.load_weights(file_path.replace('model', 'model_'+str(i)))\n",
    "    prediction = model.predict(X_val)\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_val, np.clip(prediction, 0., 1.))))\n",
    "    val_df.loc[:,'rnn_pred_'+str(i)] = prediction\n",
    "    \n",
    "val_df.to_csv('rnn_val_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate submissions\n",
    "'''\n",
    "del X_train, X_val, dense_train, dense_val; gc.collect()\n",
    "dense_features = load_pickled_data(DENSE_FEATURE_PATH)\n",
    "texts = list(load_pickled_data(RAW_TEXT_PATH))\n",
    "\n",
    "print('train-test-val split')\n",
    "dense_test = dense_features.loc[train_len:,:].reset_index(drop=True); del dense_features; gc.collect()\n",
    "texts_test = texts[train_len:]; del texts; gc.collect()\n",
    "\n",
    "print('convert to sequences')\n",
    "texts_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "print('padding')\n",
    "texts_test = sequence.pad_sequences(texts_test, maxlen=maxlen)\n",
    "\n",
    "X_test = dict((col, dense_test[col].values) for col in dense_train.columns)\n",
    "X_test['text'] = texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path.replace('model', 'model_'+str(max_bg_count-1)))\n",
    "prediction = model.predict(X_test, batch_size = 128, verbose = 1)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv', index_col = 0)\n",
    "submission = sample_submission.copy()\n",
    "submission['deal_probability'] = np.clip(prediction, 0., 1.)\n",
    "submission.to_csv('rnn_submission.csv')\n",
    "pd.DataFrame(data=prediction, columns=['rnn_pred']).to_csv('rnn_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "for i in range(max_bg_count-bagging_count, max_bg_count, 1):\n",
    "    model.load_weights(file_path.replace('model', 'model_'+str(i)))\n",
    "    prediction = model.predict(X_val)\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_val, np.clip(prediction, 0., 1.))))\n",
    "    test_df.loc[:,'rnn_pred_'+str(i)] = prediction\n",
    "    \n",
    "test_df.to_csv('rnn_test_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
