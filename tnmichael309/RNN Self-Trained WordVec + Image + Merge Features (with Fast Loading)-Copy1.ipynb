{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download rnn_merged.zip & rnn_embed.zip from https://drive.google.com/drive/folders/1yO_W-m0fF_PludrnScdgyTGsPFoDsA6_?usp=sharing and unzip to the same folder of this file\n",
    "\n",
    "## Also download train_jpg.zip & test_jpg.zip from competition website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, \\\n",
    "                            CuDNNGRU, GRU, Bidirectional, LSTM, Dense, Embedding, concatenate, Embedding, \\\n",
    "                            Flatten, Activation, BatchNormalization, regularizers, Conv1D, Conv2D, MaxPooling2D\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "from keras.initializers import Orthogonal\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LambdaCallback, Callback, LearningRateScheduler\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import gc; gc.enable()\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords                \n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from scipy.stats import boxcox\n",
    "import re\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 339512226104842527\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3174131302\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17746058336949755705\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 411\n",
    "rnn_train_epochs = 10\n",
    "batch_size=128 # 32 or 64 is good (too huge for my PC), 128 is worse in the past experiments\n",
    "cpu_count=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'y_train', 'categorical', 'test'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pickle.load(open('rnn_merged.pkl', 'rb'))\n",
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features['train']\n",
    "test = features['test']\n",
    "\n",
    "renamed_cols = []\n",
    "count = 0\n",
    "for col in train.columns:\n",
    "    if 'cat_features_user_id_category_name' in col:\n",
    "        col = 'cat_features_user_id_category_name_'+str(count)\n",
    "        count += 1\n",
    "    renamed_cols.append(col)\n",
    "train.columns = renamed_cols\n",
    "test.columns = renamed_cols\n",
    "\n",
    "train_len = train.shape[0]\n",
    "train_y = features['y_train']\n",
    "categorical = features['categorical']\n",
    "numerical = [f for f in train.columns if f not in categorical]\n",
    "features = numerical + categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price',\n",
       " 'image_top_1',\n",
       " 'item_seq_number',\n",
       " 'activation_weekday',\n",
       " 'avg_days_up_user',\n",
       " 'avg_times_up_user',\n",
       " 'n_user_items',\n",
       " 'cat_features_user_id_category_name_0',\n",
       " 'cat_features_user_id_category_name_1',\n",
       " 'cat_features_user_id_category_name_2',\n",
       " 'cat_features_user_id_category_name_3',\n",
       " 'cat_features_user_id_category_name_4',\n",
       " 'cat_features_user_id_category_name_5',\n",
       " 'cat_features_user_id_category_name_6',\n",
       " 'cat_features_user_id_category_name_7',\n",
       " 'cat_features_user_id_category_name_8',\n",
       " 'cat_features_user_id_category_name_9',\n",
       " 'cat_features_user_id_category_name_10',\n",
       " 'cat_features_user_id_category_name_11',\n",
       " 'cat_features_user_id_category_name_12',\n",
       " 'cat_features_user_id_category_name_13',\n",
       " 'cat_features_user_id_category_name_14',\n",
       " 'cat_features_user_id_category_name_15',\n",
       " 'cat_features_user_id_category_name_16',\n",
       " 'cat_features_user_id_category_name_17',\n",
       " 'cat_features_user_id_category_name_18',\n",
       " 'cat_features_user_id_category_name_19',\n",
       " 'cat_features_user_id_category_name_20',\n",
       " 'cat_features_user_id_category_name_21',\n",
       " 'cat_features_user_id_category_name_22',\n",
       " 'cat_features_user_id_category_name_23',\n",
       " 'cat_features_user_id_category_name_24',\n",
       " 'cat_features_user_id_category_name_25',\n",
       " 'cat_features_user_id_category_name_26',\n",
       " 'cat_features_user_id_category_name_27',\n",
       " 'cat_features_user_id_category_name_28',\n",
       " 'cat_features_user_id_category_name_29',\n",
       " 'cat_features_user_id_category_name_30',\n",
       " 'cat_features_user_id_category_name_31',\n",
       " 'cat_features_user_id_category_name_32',\n",
       " 'cat_features_user_id_category_name_33',\n",
       " 'cat_features_user_id_category_name_34',\n",
       " 'cat_features_user_id_category_name_35',\n",
       " 'cat_features_user_id_category_name_36',\n",
       " 'cat_features_user_id_category_name_37',\n",
       " 'cat_features_user_id_category_name_38',\n",
       " 'cat_features_user_id_category_name_39',\n",
       " 'cat_features_user_id_category_name_40',\n",
       " 'cat_features_user_id_category_name_41',\n",
       " 'cat_features_user_id_category_name_42',\n",
       " 'cat_features_user_id_category_name_43',\n",
       " 'cat_features_user_id_category_name_44',\n",
       " 'cat_features_user_id_category_name_45',\n",
       " 'cat_features_user_id_category_name_46',\n",
       " 'price_pred',\n",
       " 'price_pred_onlydescription',\n",
       " 'price_pred_all',\n",
       " 'ridge_preds',\n",
       " 'ridge_preds_title',\n",
       " 'ridge_preds_description',\n",
       " 'ga',\n",
       " 'region',\n",
       " 'city',\n",
       " 'parent_category_name',\n",
       " 'category_name',\n",
       " 'user_type',\n",
       " 'param_1',\n",
       " 'param_2',\n",
       " 'param_3',\n",
       " 'title_tfidf_svd_1',\n",
       " 'title_tfidf_svd_2',\n",
       " 'title_tfidf_svd_3',\n",
       " 'title_tfidf_svd_4',\n",
       " 'title_tfidf_svd_5',\n",
       " 'description_tfidf_svd_1',\n",
       " 'description_tfidf_svd_2',\n",
       " 'description_tfidf_svd_3',\n",
       " 'description_tfidf_svd_4',\n",
       " 'description_tfidf_svd_5',\n",
       " 'region_mean_price',\n",
       " 'region_mean_image_top_1',\n",
       " 'region_mean_item_seq_number',\n",
       " 'region_mean_price_pred',\n",
       " 'region_mean_price_pred_all',\n",
       " 'region_mean_ridge_preds',\n",
       " 'city_mean_price',\n",
       " 'city_mean_image_top_1',\n",
       " 'city_mean_item_seq_number',\n",
       " 'city_mean_price_pred',\n",
       " 'city_mean_price_pred_all',\n",
       " 'city_mean_ridge_preds',\n",
       " 'parent_category_name_mean_price',\n",
       " 'parent_category_name_mean_image_top_1',\n",
       " 'parent_category_name_mean_item_seq_number',\n",
       " 'parent_category_name_mean_price_pred',\n",
       " 'parent_category_name_mean_price_pred_all',\n",
       " 'parent_category_name_mean_ridge_preds',\n",
       " 'category_name_mean_price',\n",
       " 'category_name_mean_image_top_1',\n",
       " 'category_name_mean_item_seq_number',\n",
       " 'category_name_mean_price_pred',\n",
       " 'category_name_mean_price_pred_all',\n",
       " 'category_name_mean_ridge_preds',\n",
       " 'user_type_mean_price',\n",
       " 'user_type_mean_image_top_1',\n",
       " 'user_type_mean_item_seq_number',\n",
       " 'user_type_mean_price_pred',\n",
       " 'user_type_mean_price_pred_all',\n",
       " 'user_type_mean_ridge_preds',\n",
       " 'param_1_mean_price',\n",
       " 'param_1_mean_image_top_1',\n",
       " 'param_1_mean_item_seq_number',\n",
       " 'param_1_mean_price_pred',\n",
       " 'param_1_mean_price_pred_all',\n",
       " 'param_1_mean_ridge_preds',\n",
       " 'param_2_mean_price',\n",
       " 'param_2_mean_image_top_1',\n",
       " 'param_2_mean_item_seq_number',\n",
       " 'param_2_mean_price_pred',\n",
       " 'param_2_mean_price_pred_all',\n",
       " 'param_2_mean_ridge_preds',\n",
       " 'param_3_mean_price',\n",
       " 'param_3_mean_image_top_1',\n",
       " 'param_3_mean_item_seq_number',\n",
       " 'param_3_mean_price_pred',\n",
       " 'param_3_mean_price_pred_all',\n",
       " 'param_3_mean_ridge_preds',\n",
       " 'user_id_nunique_parent_category_name',\n",
       " 'user_id_nunique_category_name',\n",
       " 'user_id_nunique_param_1',\n",
       " 'user_id_nunique_param_2',\n",
       " 'user_id_nunique_param_3',\n",
       " 'user_id_nunique_activation_date',\n",
       " 'user_id_activation_date_count_item_id',\n",
       " 'image_top_1_nunique_item_id',\n",
       " 'image_top_1_nunique_user_id',\n",
       " 'image_top_1_nunique_category_name',\n",
       " 'image_top_1_nunique_param_1',\n",
       " 'image_top_1_nunique_item_seq_number',\n",
       " 'image_top_1_mean_price_pred',\n",
       " 'image_top_1_std_price_pred',\n",
       " 'image_top_1_mean_item_seq_number',\n",
       " 'user_id_mean_ridge_preds',\n",
       " 'user_id_category_name_mean_ridge_preds',\n",
       " 'user_id_image_top_1_mean_ridge_preds',\n",
       " 'user_id_category_name_sum_ridge_preds',\n",
       " 'region_te',\n",
       " 'city_te',\n",
       " 'parent_category_name_te',\n",
       " 'category_name_te',\n",
       " 'user_type_te',\n",
       " 'image_top_1_te',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'lat_lon_hdbscan_cluster_05_03',\n",
       " 'lat_lon_hdbscan_cluster_10_03',\n",
       " 'lat_lon_hdbscan_cluster_20_03',\n",
       " 'cityxcatxusertypeitem_num',\n",
       " 'cityxcatxusertypecity_fm_factor_0',\n",
       " 'cityxcatxusertypecity_fm_factor_1',\n",
       " 'cityxcatxusertypecategory_name_fm_factor_0',\n",
       " 'cityxcatxusertypecategory_name_fm_factor_1',\n",
       " 'cityxcatxusertypeuser_type_fm_factor_0',\n",
       " 'cityxcatxusertypeuser_type_fm_factor_1',\n",
       " 'cityxcatxusertypecity_fm_bias',\n",
       " 'cityxcatxusertypecategory_name_fm_bias',\n",
       " 'cityxcatxusertypeuser_type_fm_bias',\n",
       " 'imgxcityxcatitem_num',\n",
       " 'imgxcityxcatimage_top_1_fm_factor_0',\n",
       " 'imgxcityxcatimage_top_1_fm_factor_1',\n",
       " 'imgxcityxcatcity_fm_factor_0',\n",
       " 'imgxcityxcatcity_fm_factor_1',\n",
       " 'imgxcityxcatcategory_name_fm_factor_0',\n",
       " 'imgxcityxcatcategory_name_fm_factor_1',\n",
       " 'imgxcityxcatimage_top_1_fm_bias',\n",
       " 'imgxcityxcatcity_fm_bias',\n",
       " 'imgxcityxcatcategory_name_fm_bias',\n",
       " 'imgxisqnxusertypeitem_num',\n",
       " 'imgxisqnxusertypeimage_top_1_fm_factor_0',\n",
       " 'imgxisqnxusertypeimage_top_1_fm_factor_1',\n",
       " 'imgxisqnxusertypeitem_seq_number_fm_factor_0',\n",
       " 'imgxisqnxusertypeitem_seq_number_fm_factor_1',\n",
       " 'imgxisqnxusertypeuser_type_fm_factor_0',\n",
       " 'imgxisqnxusertypeimage_top_1_fm_bias',\n",
       " 'imgxisqnxusertypeitem_seq_number_fm_bias',\n",
       " 'img_size',\n",
       " 'b_intensity_mean',\n",
       " 'b_intensity_median',\n",
       " 'b_intensity_std',\n",
       " 'g_intensity_mean',\n",
       " 'g_intensity_median',\n",
       " 'g_intensity_std',\n",
       " 'gray_intensity_mean',\n",
       " 'gray_intensity_median',\n",
       " 'gray_intensity_std',\n",
       " 'height',\n",
       " 'r_intensity_mean',\n",
       " 'r_intensity_median',\n",
       " 'r_intensity_std',\n",
       " 'width',\n",
       " 'nasnet_nima_med',\n",
       " 'nasnet_nima_std',\n",
       " 'nasnet_nima_max',\n",
       " 'nasnet_nima_min',\n",
       " 'nasnet_nima_1_quartile',\n",
       " 'nasnet_nima_3_quartile',\n",
       " 'nasnet_nima_13_quartile_diff',\n",
       " 'nasnet_nima_max_min_diff',\n",
       " 'nasnet_nima_non_max_mean',\n",
       " 'nasnet_nima_max_non_max_mean_diff',\n",
       " 'active_price_pred',\n",
       " 'active_duration_pred',\n",
       " 'active_ad_count_pred']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features: text, image, other embeddings\\feature engineerings\n",
    "remove_cols = [\n",
    "    'cat_features_user_id_category_name_0',\n",
    "    'cat_features_user_id_category_name_1',\n",
    "    'cat_features_user_id_category_name_2',\n",
    "    'cat_features_user_id_category_name_3',\n",
    "    'cat_features_user_id_category_name_4',\n",
    "    'cat_features_user_id_category_name_5',\n",
    "    'cat_features_user_id_category_name_6',\n",
    "    'cat_features_user_id_category_name_7',\n",
    "    'cat_features_user_id_category_name_8',\n",
    "    'cat_features_user_id_category_name_9',\n",
    "    'cat_features_user_id_category_name_10',\n",
    "    'cat_features_user_id_category_name_11',\n",
    "    'cat_features_user_id_category_name_12',\n",
    "    'cat_features_user_id_category_name_13',\n",
    "    'cat_features_user_id_category_name_14',\n",
    "    'cat_features_user_id_category_name_15',\n",
    "    'cat_features_user_id_category_name_16',\n",
    "    'cat_features_user_id_category_name_17',\n",
    "    'cat_features_user_id_category_name_18',\n",
    "    'cat_features_user_id_category_name_19',\n",
    "    'cat_features_user_id_category_name_20',\n",
    "    'cat_features_user_id_category_name_21',\n",
    "    'cat_features_user_id_category_name_22',\n",
    "    'cat_features_user_id_category_name_23',\n",
    "    'cat_features_user_id_category_name_24',\n",
    "    'cat_features_user_id_category_name_25',\n",
    "    'cat_features_user_id_category_name_26',\n",
    "    'cat_features_user_id_category_name_27',\n",
    "    'cat_features_user_id_category_name_28',\n",
    "    'cat_features_user_id_category_name_29',\n",
    "    'cat_features_user_id_category_name_30',\n",
    "    'cat_features_user_id_category_name_31',\n",
    "    'cat_features_user_id_category_name_32',\n",
    "    'cat_features_user_id_category_name_33',\n",
    "    'cat_features_user_id_category_name_34',\n",
    "    'cat_features_user_id_category_name_35',\n",
    "    'cat_features_user_id_category_name_36',\n",
    "    'cat_features_user_id_category_name_37',\n",
    "    'cat_features_user_id_category_name_38',\n",
    "    'cat_features_user_id_category_name_39',\n",
    "    'cat_features_user_id_category_name_40',\n",
    "    'cat_features_user_id_category_name_41',\n",
    "    'cat_features_user_id_category_name_42',\n",
    "    'cat_features_user_id_category_name_43',\n",
    "    'cat_features_user_id_category_name_44',\n",
    "    'cat_features_user_id_category_name_45',\n",
    "    'cat_features_user_id_category_name_46',\n",
    "    'title_tfidf_svd_1',\n",
    "    'title_tfidf_svd_2',\n",
    "    'title_tfidf_svd_3',\n",
    "    'title_tfidf_svd_4',\n",
    "    'title_tfidf_svd_5',\n",
    "    'description_tfidf_svd_1',\n",
    "    'description_tfidf_svd_2',\n",
    "    'description_tfidf_svd_3',\n",
    "    'description_tfidf_svd_4',\n",
    "    'description_tfidf_svd_5',\n",
    "    'region_mean_price',\n",
    "    'region_mean_image_top_1',\n",
    "    'region_mean_item_seq_number',\n",
    "    'region_mean_price_pred',\n",
    "    'region_mean_price_pred_all',\n",
    "    'region_mean_ridge_preds',\n",
    "    'city_mean_price',\n",
    "    'city_mean_image_top_1',\n",
    "    'city_mean_item_seq_number',\n",
    "    'city_mean_price_pred',\n",
    "    'city_mean_price_pred_all',\n",
    "    'city_mean_ridge_preds',\n",
    "    'parent_category_name_mean_price',\n",
    "    'parent_category_name_mean_image_top_1',\n",
    "    'parent_category_name_mean_item_seq_number',\n",
    "    'parent_category_name_mean_price_pred',\n",
    "    'parent_category_name_mean_price_pred_all',\n",
    "    'parent_category_name_mean_ridge_preds',\n",
    "    'category_name_mean_price',\n",
    "    'category_name_mean_image_top_1',\n",
    "    'category_name_mean_item_seq_number',\n",
    "    'category_name_mean_price_pred',\n",
    "    'category_name_mean_price_pred_all',\n",
    "    'category_name_mean_ridge_preds',\n",
    "    'user_type_mean_price',\n",
    "    'user_type_mean_image_top_1',\n",
    "    'user_type_mean_item_seq_number',\n",
    "    'user_type_mean_price_pred',\n",
    "    'user_type_mean_price_pred_all',\n",
    "    'user_type_mean_ridge_preds',\n",
    "    'param_1_mean_price',\n",
    "    'param_1_mean_image_top_1',\n",
    "    'param_1_mean_item_seq_number',\n",
    "    'param_1_mean_price_pred',\n",
    "    'param_1_mean_price_pred_all',\n",
    "    'param_1_mean_ridge_preds',\n",
    "    'param_2_mean_price',\n",
    "    'param_2_mean_image_top_1',\n",
    "    'param_2_mean_item_seq_number',\n",
    "    'param_2_mean_price_pred',\n",
    "    'param_2_mean_price_pred_all',\n",
    "    'param_2_mean_ridge_preds',\n",
    "    'param_3_mean_price',\n",
    "    'param_3_mean_image_top_1',\n",
    "    'param_3_mean_item_seq_number',\n",
    "    'param_3_mean_price_pred',\n",
    "    'param_3_mean_price_pred_all',\n",
    "    'param_3_mean_ridge_preds',\n",
    "    'user_id_nunique_parent_category_name',\n",
    "    'user_id_nunique_category_name',\n",
    "    'user_id_nunique_param_1',\n",
    "    'user_id_nunique_param_2',\n",
    "    'user_id_nunique_param_3',\n",
    "    'user_id_nunique_activation_date',\n",
    "    'user_id_activation_date_count_item_id',\n",
    "    'image_top_1_nunique_item_id',\n",
    "    'image_top_1_nunique_user_id',\n",
    "    'image_top_1_nunique_category_name',\n",
    "    'image_top_1_nunique_param_1',\n",
    "    'image_top_1_nunique_item_seq_number',\n",
    "    'image_top_1_mean_price_pred',\n",
    "    'image_top_1_std_price_pred',\n",
    "    'image_top_1_mean_item_seq_number',\n",
    "    'user_id_mean_ridge_preds',\n",
    "    'user_id_category_name_mean_ridge_preds',\n",
    "    'user_id_image_top_1_mean_ridge_preds',\n",
    "    'user_id_category_name_sum_ridge_preds',\n",
    "    'cityxcatxusertypeitem_num',\n",
    "    'cityxcatxusertypecity_fm_factor_0',\n",
    "    'cityxcatxusertypecity_fm_factor_1',\n",
    "    'cityxcatxusertypecategory_name_fm_factor_0',\n",
    "    'cityxcatxusertypecategory_name_fm_factor_1',\n",
    "    'cityxcatxusertypeuser_type_fm_factor_0',\n",
    "    'cityxcatxusertypeuser_type_fm_factor_1',\n",
    "    'cityxcatxusertypecity_fm_bias',\n",
    "    'cityxcatxusertypecategory_name_fm_bias',\n",
    "    'cityxcatxusertypeuser_type_fm_bias',\n",
    "    'imgxcityxcatitem_num',\n",
    "    'imgxcityxcatimage_top_1_fm_factor_0',\n",
    "    'imgxcityxcatimage_top_1_fm_factor_1',\n",
    "    'imgxcityxcatcity_fm_factor_0',\n",
    "    'imgxcityxcatcity_fm_factor_1',\n",
    "    'imgxcityxcatcategory_name_fm_factor_0',\n",
    "    'imgxcityxcatcategory_name_fm_factor_1',\n",
    "    'imgxcityxcatimage_top_1_fm_bias',\n",
    "    'imgxcityxcatcity_fm_bias',\n",
    "    'imgxcityxcatcategory_name_fm_bias',\n",
    "    'imgxisqnxusertypeitem_num',\n",
    "    'imgxisqnxusertypeimage_top_1_fm_factor_0',\n",
    "    'imgxisqnxusertypeimage_top_1_fm_factor_1',\n",
    "    'imgxisqnxusertypeitem_seq_number_fm_factor_0',\n",
    "    'imgxisqnxusertypeitem_seq_number_fm_factor_1',\n",
    "    'imgxisqnxusertypeuser_type_fm_factor_0',\n",
    "    'imgxisqnxusertypeimage_top_1_fm_bias',\n",
    "    'imgxisqnxusertypeitem_seq_number_fm_bias',\n",
    "    'b_intensity_mean',\n",
    "    'b_intensity_median',\n",
    "    'b_intensity_std',\n",
    "    'g_intensity_mean',\n",
    "    'g_intensity_median',\n",
    "    'g_intensity_std',\n",
    "    'gray_intensity_mean',\n",
    "    'gray_intensity_median',\n",
    "    'gray_intensity_std',\n",
    "    'r_intensity_mean',\n",
    "    'r_intensity_median',\n",
    "    'r_intensity_std',\n",
    "    'nasnet_nima_med',\n",
    "    'nasnet_nima_std',\n",
    "    'nasnet_nima_max',\n",
    "    'nasnet_nima_min',\n",
    "    'nasnet_nima_1_quartile',\n",
    "    'nasnet_nima_3_quartile',\n",
    "    'nasnet_nima_13_quartile_diff',\n",
    "    'nasnet_nima_max_min_diff',\n",
    "    'nasnet_nima_non_max_mean',\n",
    "    'nasnet_nima_max_non_max_mean_diff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(remove_cols, axis=1, inplace=True)\n",
    "test.drop(remove_cols, axis=1, inplace=True)\n",
    "\n",
    "for col in remove_cols:\n",
    "    if col in categorical:\n",
    "        categorical.remove(col)\n",
    "    if col in numerical:\n",
    "        numerical.remove(col)\n",
    "        \n",
    "features = numerical + categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'image'] = pd.read_csv('train.csv', usecols=['activation_date', 'image'], parse_dates=['activation_date']) \\\n",
    "                          .sort_values('activation_date').reset_index(drop=True)['image'].fillna('no-image')\n",
    "test.loc[:, 'image'] = pd.read_csv('test.csv', usecols=['image'])['image'].fillna('no-image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 500000\n",
    "maxlen = 150\n",
    "embed_size = 300\n",
    "\n",
    "title_max_features = 200000\n",
    "title_maxlen = 80\n",
    "title_embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['desc_embed_info', 'title_embed_info'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_info = pickle.load(open('rnn_embed.pkl', 'rb'))\n",
    "embed_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_embed_info = embed_info['desc_embed_info']\n",
    "title_embed_info = embed_info['title_embed_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup max info for embedding in categorical variables\n"
     ]
    }
   ],
   "source": [
    "print('setup max info for embedding in categorical variables')\n",
    "max_info = dict((col, train[col].max()+1) for col in categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_rmse(true, prediction):\n",
    "    return np.sqrt(metrics.mean_squared_error(true, np.clip(prediction, 0., 1.)))\n",
    "    \n",
    "class NBatchEvalLogger(Callback):\n",
    "    def __init__(self, display, val_X, val_y, save_path=None, save_start=1000):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y\n",
    "        self.best_loss = None\n",
    "        self.save_path = save_path\n",
    "        self.save_start = save_start\n",
    "        self.record_count = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        if self.step % self.display == 0 and self.step >= self.save_start:\n",
    "            #loss, metric = self.model.evaluate(self.val_X, self.val_y, batch_size=128, verbose=1)\n",
    "            prediction = self.model.predict(self.val_X, batch_size=128, verbose=0)\n",
    "            loss = clip_rmse(self.val_y, prediction)\n",
    "            \n",
    "            if self.best_loss is None:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "                    if self.save_path is not None:\n",
    "                        self.model.save(self.save_path, overwrite=True)\n",
    "                        self.record_count += 1\n",
    "                    \n",
    "            print('\\rstep: {} val loss={:.5f}, best loss={:.5f}'.format(self.step, loss, self.best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from copy import deepcopy as cp\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import bag, threaded\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import concurrent.futures\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    #'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, X, y, img_arch, img_path, batch_size=32, shuffle=True, is_train=True):\n",
    "        #'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.img_path = img_path\n",
    "        self.is_train = is_train\n",
    "        self.on_epoch_end()\n",
    "        self.zipped = ZipFile(img_arch)\n",
    "        #print('file names:\\n', self.zipped.namelist()[1:10], '\\n...')\n",
    "        self.img_path = img_path\n",
    "        \n",
    "        global cpu_count\n",
    "        self.pool = ThreadPool(cpu_count)\n",
    "    \n",
    "    def __getstate__(self):\n",
    "        \"\"\" This is called before pickling. \"\"\"\n",
    "        state = self.__dict__.copy()\n",
    "        del state['zipped']\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\" This is called while unpickling. \"\"\"\n",
    "        self.__dict__.update(state)\n",
    "        \n",
    "    def __len__(self):\n",
    "        #'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        start = index*self.batch_size\n",
    "        end = min((index+1)*self.batch_size, len(self.indexes))\n",
    "        indexes = self.indexes[start: end]\n",
    "\n",
    "        # Generate data\n",
    "        return self.__data_generation(indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #'Updates indexes after each epoch'\n",
    "        self.indexes = cp(list(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def load_img_from_zipped(self, img_id, i, imgs_holder):\n",
    "        \n",
    "        invalid_img_ids = ['4f029e2a00e892aa2cac27d98b52ef8b13d91471f613c8d3c38e3f29d4da0b0c', \n",
    "                           '8513a91e55670c709069b5f85e12a59095b802877715903abef16b7a6f306e58', \n",
    "                           '60d310a42e87cdf799afcd89dc1b11ae3fdc3d0233747ec7ef78d82c87002e83', \n",
    "                           'b98b291bd04c3d92165ca515e00468fd9756af9a8f1df42505deed1dcfb5d7ae']\n",
    "        try:\n",
    "            if img_id in invalid_img_ids or img_id == 'no-image':\n",
    "                pass\n",
    "            else:\n",
    "                exfile = self.zipped.read(self.img_path+img_id+'.jpg')\n",
    "                arr = np.frombuffer(exfile, np.uint8)\n",
    "                imz = cv2.imdecode(arr, flags=cv2.IMREAD_UNCHANGED)\n",
    "                imz = cv2.resize(imz, (224,224), interpolation=cv2.INTER_AREA)\n",
    "                imgs_holder[i] = img_to_array(imz)\n",
    "        except:\n",
    "            print(img_id, ' is invalid')\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def parallel_load_imgs(self, img_ids, wait=True):\n",
    "     \n",
    "        imgs_holder = np.zeros((len(img_ids), 224, 224, 3))\n",
    "        '''\n",
    "        for i, im_id in enumerate(img_ids):\n",
    "            self.load_img_from_zipped(im_id, i, imgs_holder)\n",
    "        '''    \n",
    "        self.res = [self.pool.apply_async(self.load_img_from_zipped, (im_id, i, imgs_holder)) for i, im_id in enumerate(img_ids)]\n",
    "        if wait:\n",
    "            for r in self.res:\n",
    "                r.get()\n",
    "           \n",
    "        #print(imgs_holder)\n",
    "        imgs_holder = preprocess_input(imgs_holder) # adjust to mean of rgb to some value\n",
    "        \n",
    "        return imgs_holder\n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        #'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        \n",
    "        # Generate data\n",
    "        X = dict((col, self.X.loc[list_IDs_temp, col].values) for col in features)\n",
    "        X['desc'] = desc_embed_info['text'][list_IDs_temp,:]\n",
    "        X['title'] = title_embed_info['text'][list_IDs_temp,:]\n",
    "        X['imgs'] = self.parallel_load_imgs(self.X.loc[list_IDs_temp, 'image'].values)\n",
    "        \n",
    "        if self.is_train:\n",
    "            y = cp(self.y[list_IDs_temp])\n",
    "            return X, y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nzipped = ZipFile('train_jpg.zip')\\nprint(zipped.namelist()[1:10])\\n\\nimg_id = '2809fd6afd6d3cae4dd4ad93a7f905a0db32292f4df4b3f19fa5492e08cbfd90'\\ntarget_size=(224,224)\\ntry:\\n    exfile = zipped.read('data/competition_files/train_jpg/'+img_id+'.jpg')\\n    arr = np.frombuffer(exfile, np.uint8)\\n    imz = cv2.imdecode(arr, flags=cv2.IMREAD_UNCHANGED)\\n    imz = cv2.resize(imz, target_size, interpolation=cv2.INTER_AREA)\\nexcept:\\n    print(img_id, ' is invalid')\\n    imz = None\\nimz\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'train_jpg.zip', 'data/competition_files/train_jpg/', \n",
    "# debug use\n",
    "'''\n",
    "zipped = ZipFile('train_jpg.zip')\n",
    "print(zipped.namelist()[1:10])\n",
    "\n",
    "img_id = '2809fd6afd6d3cae4dd4ad93a7f905a0db32292f4df4b3f19fa5492e08cbfd90'\n",
    "target_size=(224,224)\n",
    "try:\n",
    "    exfile = zipped.read('data/competition_files/train_jpg/'+img_id+'.jpg')\n",
    "    arr = np.frombuffer(exfile, np.uint8)\n",
    "    imz = cv2.imdecode(arr, flags=cv2.IMREAD_UNCHANGED)\n",
    "    imz = cv2.resize(imz, target_size, interpolation=cv2.INTER_AREA)\n",
    "except:\n",
    "    print(img_id, ' is invalid')\n",
    "    imz = None\n",
    "imz\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(categorical_features, numerical_features):\n",
    "    \n",
    "    # non-cat features\n",
    "    non_cat_inputs = []\n",
    "    for col in numerical_features:\n",
    "        f = Input(shape=[1], name=col)\n",
    "        non_cat_inputs.append(f)\n",
    "        \n",
    "    # cat features\n",
    "    cat_inputs = []\n",
    "    cat_embeds = []\n",
    "    for col in categorical_features:\n",
    "        f = Input(shape=[1], name=col)\n",
    "        embed_dim = max_info[col].max()\n",
    "        if max_info[col] > 10:\n",
    "            reduced_dim = 10\n",
    "        else:\n",
    "            reduced_dim = 1\n",
    "        embed_f = Embedding(embed_dim, reduced_dim)(f)\n",
    "        flatten_f = Flatten()(embed_f)\n",
    "        cat_inputs.append(f)\n",
    "        cat_embeds.append(flatten_f)\n",
    "      \n",
    "    # text features: architecture of text to try here!!!\n",
    "    \n",
    "    # description\n",
    "    text_inp = Input(shape = (maxlen, ), name='desc')\n",
    "    text_emb = Embedding(desc_embed_info['nb_words'], embed_size, weights = [desc_embed_info['emb_matrix']],\n",
    "                    input_length = maxlen, trainable = False)(text_inp)\n",
    "    text_emb = SpatialDropout1D(0.3)(text_emb)\n",
    "    text_gru = Bidirectional(CuDNNGRU(128, return_sequences = True))(text_emb)\n",
    "    text_gru = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(text_gru)\n",
    "    text_gru_avg = GlobalAveragePooling1D()(text_gru)\n",
    "    text_gru_max = GlobalMaxPooling1D()(text_gru)\n",
    "    text_gru = concatenate([text_gru_avg, text_gru_max]) \n",
    "    text_gru = Dropout(0.1)(text_gru)\n",
    "    \n",
    "    # title\n",
    "    title_inp = Input(shape = (title_maxlen, ), name='title')\n",
    "    title_emb = Embedding(title_embed_info['nb_words'], title_embed_size, weights = [title_embed_info['emb_matrix']],\n",
    "                    input_length = title_maxlen, trainable = False)(title_inp)\n",
    "    title_emb = SpatialDropout1D(0.1)(title_emb)\n",
    "    title_gru = Bidirectional(CuDNNGRU(32, return_sequences = True))(title_emb)\n",
    "    title_gru = Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(title_gru)\n",
    "    title_gru_avg = GlobalAveragePooling1D()(title_gru)\n",
    "    title_gru_max = GlobalMaxPooling1D()(title_gru)\n",
    "    title_gru = concatenate([title_gru_avg, title_gru_max]) \n",
    "    title_gru = Dropout(0.1)(title_gru)\n",
    "    \n",
    "    # add image architecture\n",
    "    # reference: https://keras.io/getting-started/functional-api-guide/#more-examples, Visual question answering model\n",
    "    '''\n",
    "    img_inp = Input(shape = (224, 224, 3 ), name='imgs')\n",
    "    img_ch = Conv2D(64, (3, 3), activation='relu', padding='same', W_constraint=max_norm(3))(img_inp)\n",
    "    img_ch = Conv2D(64, (3, 3), activation='relu')(img_ch)\n",
    "    img_ch = MaxPooling2D((2, 2))(img_ch)\n",
    "    #img_ch = Conv2D(128, (3, 3), activation='relu', padding='same', W_constraint=max_norm(3))(img_ch)\n",
    "    #img_ch = Conv2D(128, (3, 3), activation='relu')(img_ch)\n",
    "    #img_ch = MaxPooling2D((2, 2))(img_ch)\n",
    "    #img_ch = Conv2D(256, (3, 3), activation='relu', padding='same', W_constraint=max_norm(3))(img_ch)\n",
    "    #img_ch = Conv2D(256, (3, 3), activation='relu')(img_ch)\n",
    "    #img_ch = Conv2D(256, (3, 3), activation='relu')(img_ch)\n",
    "    #img_ch = MaxPooling2D((2, 2))(img_ch)\n",
    "    img_ch = Flatten()(img_ch)\n",
    "    img_ch = Dense(64, activation='relu')(img_ch)\n",
    "    '''\n",
    "    \n",
    "    # merge each branch: non-cat, cat, text, img\n",
    "    concat_main = non_cat_inputs+cat_embeds+[text_gru, title_gru]\n",
    "    main = concatenate(concat_main)\n",
    "    main = BatchNormalization()(main)\n",
    "    main = Dropout(0.1)(main)\n",
    "    main = BatchNormalization()(Dense(256, activation='relu')(main))\n",
    "    main = Dropout(0.1)(main)\n",
    "    main = BatchNormalization()(Dense(64, activation='relu')(main))\n",
    "    out = Dense(1, activation = \"sigmoid\")(main)\n",
    "\n",
    "    concat_input = non_cat_inputs+cat_inputs+[text_inp, title_inp]\n",
    "    model = Model(concat_input, out)\n",
    "    model.regularizers = [regularizers.l2(0.0001)]\n",
    "    model.compile(optimizer = Adam(lr=0.001), loss = root_mean_squared_error,\n",
    "                  metrics =[root_mean_squared_error])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import warnings; warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.arange(0, train_len)\n",
    "test_indices = np.arange(0, test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_fold = 0 # <= 0 for invalid, train from fold 1, > 0: used to train from fold=start_fold\n",
    "resume_file_prefix = '0619_rnn' # whatever we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "desc (InputLayer)               (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title (InputLayer)              (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 150, 300)     150000000   desc[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 80, 100)      20000000    title[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 80, 100)      0           embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 80, 64)       25728       spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 148, 64)      49216       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 78, 16)       3088        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "image_top_1 (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "region (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "city (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parent_category_name (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_type (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "param_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "param_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "param_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lat_lon_hdbscan_cluster_05_03 ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lat_lon_hdbscan_cluster_10_03 ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lat_lon_hdbscan_cluster_20_03 ( (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 16)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 16)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 10)        1582600     image_top_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        280         region[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 10)        17520       city[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 1)         9           parent_category_name[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 10)        470         category_name[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 1)         3           user_type[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 10)        3720        param_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 10)        2780        param_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 1, 10)        12760       param_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 1, 10)        670         lat_lon_hdbscan_cluster_05_03[0][\n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 1, 10)        460         lat_lon_hdbscan_cluster_10_03[0][\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 1, 10)        230         lat_lon_hdbscan_cluster_20_03[0][\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32)           0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "price (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_seq_number (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "activation_weekday (InputLayer) (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "avg_days_up_user (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "avg_times_up_user (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "n_user_items (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price_pred (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price_pred_onlydescription (Inp (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "price_pred_all (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ridge_preds (InputLayer)        (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ridge_preds_title (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ridge_preds_description (InputL (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ga (InputLayer)                 (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "region_te (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "city_te (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "parent_category_name_te (InputL (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name_te (InputLayer)   (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_type_te (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_top_1_te (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "latitude (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "longitude (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "img_size (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "height (InputLayer)             (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "width (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "active_price_pred (InputLayer)  (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "active_duration_pred (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "active_ad_count_pred (InputLaye (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 10)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 10)           0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 10)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 10)           0           embedding_10[0][0]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 10)           0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 10)           0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 289)          0           price[0][0]                      \n",
      "                                                                 item_seq_number[0][0]            \n",
      "                                                                 activation_weekday[0][0]         \n",
      "                                                                 avg_days_up_user[0][0]           \n",
      "                                                                 avg_times_up_user[0][0]          \n",
      "                                                                 n_user_items[0][0]               \n",
      "                                                                 price_pred[0][0]                 \n",
      "                                                                 price_pred_onlydescription[0][0] \n",
      "                                                                 price_pred_all[0][0]             \n",
      "                                                                 ridge_preds[0][0]                \n",
      "                                                                 ridge_preds_title[0][0]          \n",
      "                                                                 ridge_preds_description[0][0]    \n",
      "                                                                 ga[0][0]                         \n",
      "                                                                 region_te[0][0]                  \n",
      "                                                                 city_te[0][0]                    \n",
      "                                                                 parent_category_name_te[0][0]    \n",
      "                                                                 category_name_te[0][0]           \n",
      "                                                                 user_type_te[0][0]               \n",
      "                                                                 image_top_1_te[0][0]             \n",
      "                                                                 latitude[0][0]                   \n",
      "                                                                 longitude[0][0]                  \n",
      "                                                                 img_size[0][0]                   \n",
      "                                                                 height[0][0]                     \n",
      "                                                                 width[0][0]                      \n",
      "                                                                 active_price_pred[0][0]          \n",
      "                                                                 active_duration_pred[0][0]       \n",
      "                                                                 active_ad_count_pred[0][0]       \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 289)          1156        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 289)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          74240       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64)           256         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            65          batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 172,122,963\n",
      "Trainable params: 2,121,745\n",
      "Non-trainable params: 170,001,218\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e339019f2c2e4cc994b5afbfca80c12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15273040e2634275b08132bf3c266004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=9397), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-1ab7a24d477f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m                                   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn_train_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                   \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                   callbacks = [lr_schd, check_point, TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2192\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2193\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2194\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2196\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if start_fold > 0:\n",
    "    import pickle\n",
    "    ret = pickle.load(open(resume_file_prefix+'_oof_val_pred', 'rb'))\n",
    "    ret_test = pickle.load(open(resume_file_prefix+'_oof_test_pred', 'rb'))\n",
    "    print(ret)\n",
    "    print(ret_test)\n",
    "else:\n",
    "    ret = np.zeros((train.shape[0],))\n",
    "    ret_test = np.zeros((test.shape[0],))\n",
    "\n",
    "fold = 0    \n",
    "for tr_ix, val_ix in KFold(5, shuffle=True, random_state=seed).split(train_indices):\n",
    "    fold += 1\n",
    "    \n",
    "    if start_fold > 0 and fold < start_fold:\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    model = build_model(categorical, numerical)\n",
    "    file_path = \"rnn_weights/model_final_fold_{}.hdf5\".format(fold)\n",
    "     \n",
    "    # customized batch loader\n",
    "    training_generator = DataGenerator(tr_ix, train, train_y, \n",
    "                                       'train_jpg.zip', 'data/competition_files/train_jpg/', \n",
    "                                       batch_size=batch_size, shuffle=True)\n",
    "    validation_generator = DataGenerator(val_ix, train, train_y, \n",
    "                                         'train_jpg.zip', 'data/competition_files/train_jpg/', \n",
    "                                         batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    lr_schd = LearningRateScheduler(lambda epoch: 0.001*(0.2**(epoch//6)), verbose=1)\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "    history = model.fit_generator(generator=training_generator,\n",
    "                                  validation_data=validation_generator,\n",
    "                                  use_multiprocessing=False,\n",
    "                                  workers=1,\n",
    "                                  epochs=rnn_train_epochs,\n",
    "                                  verbose = 0, \n",
    "                                  callbacks = [lr_schd, check_point, TQDMNotebookCallback(leave_inner=True, leave_outer=True)])\n",
    "    \n",
    "    \n",
    "    # Predict val + test oofs\n",
    "    model.load_weights(file_path) # load weight with best validation score\n",
    "    \n",
    "    del validation_generator\n",
    "    validation_generator = DataGenerator(val_ix, train, None, \n",
    "                                         'train_jpg.zip', 'data/competition_files/train_jpg/', \n",
    "                                         batch_size=batch_size, shuffle=False, is_train=False)\n",
    "    test_generator = DataGenerator(test_indices, test, None, \n",
    "                                   'test_jpg.zip', 'data/competition_files/test_jpg/',       \n",
    "                                   batch_size=batch_size, shuffle=False, is_train=False)\n",
    "    \n",
    "    ret[val_ix] = model.predict_generator(validation_generator, use_multiprocessing=False, workers=1).reshape((len(val_ix),))\n",
    "    ret_test += model.predict_generator(test_generator, use_multiprocessing=False, workers=1).reshape((ret_test.shape[0],))\n",
    "    \n",
    "    del model, history, training_generator, validation_generator, test_generator; gc.collect()\n",
    "    \n",
    "ret_test /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these to dump files if OOM (out-of-mem) happens\n",
    "import pickle\n",
    "pickle.dump(ret, open(resume_file_prefix+'_oof_val_pred', 'wb'))\n",
    "pickle.dump(ret_test, open(resume_file_prefix+'_oof_test_pred', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public kernel:  cv = .2220, lb = .2247 \n",
    "# bigru-conv1d: cv =.2185 , lb = .2235\n",
    "# bigru-attention: cv =.2186 , lb = .2235\n",
    "# 2gru: lb: .2239\n",
    "# self-trained wordvec: cv .217232, lb: .2229\n",
    "# +partial new features: cv .216326, lb: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate OOFs and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'selftrained_bigru_conv1d_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=ret, columns=[prefix+'_rnn_pred']).to_csv(prefix+'_rnn_oof_val_pred.csv', index=False)\n",
    "pd.DataFrame(data=ret_test, columns=[prefix+'_rnn_pred']).to_csv(prefix+'_rnn_oof_test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = pd.read_csv('sample_submission.csv')\n",
    "subm['deal_probability'] = np.clip(ret_test, 0, 1)\n",
    "subm.to_csv(prefix+'_rnn_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.5 tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
