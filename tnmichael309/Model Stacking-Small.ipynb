{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc; gc.enable()\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from GridSearcher import data_loader, model_loader, fit_params, get_oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_mean_enc_lgb_oof_test_pred.csv',\n",
       " 'all_mean_enc_lgb_oof_val_pred.csv',\n",
       " 'all_mean_enc_user_feat2_lgb_oof_test_pred.csv',\n",
       " 'all_mean_enc_user_feat2_lgb_oof_val_pred.csv',\n",
       " 'all_mean_enc_user_feat_lgb_oof_test_pred.csv',\n",
       " 'all_mean_enc_user_feat_lgb_oof_val_pred.csv',\n",
       " 'alpha_0001_oof_test_pred.csv',\n",
       " 'alpha_0001_oof_val_pred.csv',\n",
       " 'alpha_10_oof_test_pred.csv',\n",
       " 'alpha_10_oof_val_pred.csv',\n",
       " 'alpha_160_oof_test_pred.csv',\n",
       " 'alpha_160_oof_val_pred.csv',\n",
       " 'alpha_320_oof_test_pred.csv',\n",
       " 'alpha_320_oof_val_pred.csv',\n",
       " 'baseline_xgb_oof_test_pred.csv',\n",
       " 'baseline_xgb_oof_val_pred.csv',\n",
       " 'catboost1_without_text_oof_test_pred',\n",
       " 'catboost1_without_text_oof_test_pred.csv',\n",
       " 'catboost1_without_text_oof_val_pred',\n",
       " 'catboost1_without_text_oof_val_pred.csv',\n",
       " 'catboost_oof_test_pred.csv',\n",
       " 'catboost_oof_val_pred.csv',\n",
       " 'cat_interact_lgb_oof_test_pred.csv',\n",
       " 'cat_interact_lgb_oof_val_pred.csv',\n",
       " 'cls05_lgb_oof_test_pred.csv',\n",
       " 'cls05_lgb_oof_val_pred.csv',\n",
       " 'cls0_lgb_oof_test_pred.csv',\n",
       " 'cls0_lgb_oof_val_pred.csv',\n",
       " 'drive-download-20180616T163303Z-001.zip',\n",
       " 'fused_text_lgb_oof_test_pred.csv',\n",
       " 'fused_text_lgb_oof_val_pred.csv',\n",
       " 'img_meta_nima_fm_lgb_oof_test_pred.csv',\n",
       " 'img_meta_nima_fm_lgb_oof_val_pred.csv',\n",
       " 'img_meta_nima_xgb_oof_test_pred.csv',\n",
       " 'img_meta_nima_xgb_oof_val_pred.csv',\n",
       " 'img_meta_xgb_oof_test_pred.csv',\n",
       " 'img_meta_xgb_oof_val_pred.csv',\n",
       " 'lgb411_dart_tune_oof_test_pred.csv',\n",
       " 'lgb411_dart_tune_oof_val_pred.csv',\n",
       " 'lgb411_tune_oof_test_pred.csv',\n",
       " 'lgb411_tune_oof_val_pred.csv',\n",
       " 'lr_l1_05_oof_test_pred.csv',\n",
       " 'lr_l1_05_oof_val_pred.csv',\n",
       " 'lr_l1_1_oof_test_pred.csv',\n",
       " 'lr_l1_1_oof_val_pred.csv',\n",
       " 'lr_l2_01_oof_test_pred.csv',\n",
       " 'lr_l2_01_oof_val_pred.csv',\n",
       " 'lr_l2_1_oof_test_pred.csv',\n",
       " 'lr_l2_1_oof_val_pred.csv',\n",
       " 'marcus_lgb_oof_test_pred.csv',\n",
       " 'marcus_lgb_oof_val_pred.csv',\n",
       " 'mcl_cgb_oof_test_pred.csv',\n",
       " 'mcl_cgb_oof_val_pred.csv',\n",
       " 'mean_enc_lgb_oof_test_pred.csv',\n",
       " 'mean_enc_lgb_oof_val_pred.csv',\n",
       " 'mixed_features_text_proprocessing_lgb_oof_test_pred.csv',\n",
       " 'mixed_features_text_proprocessing_lgb_oof_val_pred.csv',\n",
       " 'mlp_oof_test_pred.csv',\n",
       " 'mlp_oof_val_pred.csv',\n",
       " 'multiclass3_lgb_oof_test_pred.csv',\n",
       " 'multiclass3_lgb_oof_val_pred.csv',\n",
       " 'multiclass_lgb_oof_test_pred.csv',\n",
       " 'multiclass_lgb_oof_val_pred.csv',\n",
       " 'nima_features_xgb_oof_test_pred.csv',\n",
       " 'nima_features_xgb_oof_val_pred.csv',\n",
       " 'plants_lgb_oof_test_pred.csv',\n",
       " 'plants_lgb_oof_val_pred.csv',\n",
       " 'plants_with_img_meta_nima_fm_geo_active_lgb_oof_test_pred.csv',\n",
       " 'plants_with_img_meta_nima_fm_geo_active_lgb_oof_val_pred.csv',\n",
       " 'plants_with_img_meta_nima_fm_geo_active_obj_xentropy_lgb_oof_test_pred.csv',\n",
       " 'plants_with_img_meta_nima_fm_geo_active_obj_xentropy_lgb_oof_val_pred.csv',\n",
       " 'poisson_lgb_oof_test_pred.csv',\n",
       " 'poisson_lgb_oof_val_pred.csv',\n",
       " 'pretrained_2gru_rnn_oof_test_pred.csv',\n",
       " 'pretrained_2gru_rnn_oof_val_pred.csv',\n",
       " 'pretrained_bigru_attention_rnn_oof_test_pred.csv',\n",
       " 'pretrained_bigru_attention_rnn_oof_val_pred.csv',\n",
       " 'pretrained_bigru_cv1d_rnn_oof_test_pred.csv',\n",
       " 'pretrained_bigru_cv1d_rnn_oof_val_pred.csv',\n",
       " 'ranking_xgb_oof_test_pred.csv',\n",
       " 'ranking_xgb_oof_val_pred.csv',\n",
       " 'select_dense_features_lgb_oof_test_pred.csv',\n",
       " 'select_dense_features_lgb_oof_val_pred.csv',\n",
       " 'select_sparse_features_lgb_oof_test_pred.csv',\n",
       " 'select_sparse_features_lgb_oof_val_pred.csv',\n",
       " 'selftrained_bigru_conv1d_rnn_oof_test_pred.csv',\n",
       " 'selftrained_bigru_conv1d_rnn_oof_val_pred.csv',\n",
       " 'simple_feature_lgb_oof_test_pred.csv',\n",
       " 'simple_feature_lgb_oof_val_pred.csv',\n",
       " 'small_features_v4_xgb_oof_test_pred.csv',\n",
       " 'small_features_v4_xgb_oof_val_pred.csv',\n",
       " 'small_features_v5_xgb_oof_test_pred.csv',\n",
       " 'small_features_v5_xgb_oof_val_pred.csv',\n",
       " 'text_cwb_rg_oof_test_pred.csv',\n",
       " 'text_cwb_rg_oof_val_pred.csv',\n",
       " 'text_fm_oof_test_pred.csv',\n",
       " 'text_fm_oof_val_pred.csv',\n",
       " 'text_lgb_oof_test_pred.csv',\n",
       " 'text_lgb_oof_val_pred.csv',\n",
       " 'text_rg_oof_test_pred.csv',\n",
       " 'text_rg_oof_val_pred.csv',\n",
       " 'xentropy_small_lr_cat_lgb_oof_test_pred.csv',\n",
       " 'xentropy_small_lr_cat_lgb_oof_val_pred.csv',\n",
       " 'xentropy_small_lr_lgb_oof_test_pred.csv',\n",
       " 'xentropy_small_lr_lgb_oof_val_pred.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = 'final oofs/'\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prefixs = [\n",
    "    'lgb411_tune',\n",
    "    'lgb411_dart_tune'\n",
    "    'poisson_lgb',\n",
    "    'img_meta_xgb',\n",
    "    'baseline_xgb',\n",
    "    'mcl_cgb',\n",
    "    'selftrained_bigru_conv1d_rnn',\n",
    "    'text_lgb',\n",
    "    'mlp',\n",
    "    'rg_alpha_0001',\n",
    "    'lr_l2_01',\n",
    "]\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'lgb',\n",
    "        'prefixs': [\n",
    "            'lgb411_tune',\n",
    "            'plants_lgb', #411\n",
    "            'plants_with_img_meta_nima_fm_geo_active_lgb',\n",
    "            #'plants_with_img_meta_nima_fm_geo_active_obj_xentropy_lgb',\n",
    "            #'xentropy_small_lr_lgb', #lgb411_tune\n",
    "            'xentropy_small_lr_cat_lgb',\n",
    "            'simple_feature_lgb', #411\n",
    "            'all_mean_enc_lgb', #411\n",
    "            'all_mean_enc_user_feat_lgb', #411\n",
    "            'all_mean_enc_user_feat2_lgb', #411\n",
    "            'cat_interact_lgb', #411\n",
    "            'mean_enc_lgb', #411\n",
    "            'marcus_lgb', #o411\n",
    "            'fused_text_lgb', #o411\n",
    "            'mixed_features_text_proprocessing_lgb', #o411,\n",
    "            'select_dense_features_lgb', #411\n",
    "            'select_sparse_features_lgb', #411\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'name': 'lgb_dart',\n",
    "        'prefixs': [\n",
    "            'lgb411_dart_tune',\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'name': 'lgb_pois',\n",
    "        'prefixs': [\n",
    "            'poisson_lgb', #o411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'xgb_lg',\n",
    "        'prefixs': [\n",
    "            'small_features_v5_xgb', #o411\n",
    "            'small_features_v4_xgb', #o411\n",
    "            'nima_features_xgb', #o411\n",
    "            'img_meta_xgb', #o411\n",
    "            'img_meta_nima_xgb', #o411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'xgb_dw',\n",
    "        'prefixs': [\n",
    "            'baseline_xgb', #o411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'xgb_ranking',\n",
    "        'prefixs': [\n",
    "            'ranking_xgb', #o411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'catboost',\n",
    "        'prefixs': [\n",
    "            'catboost', #411\n",
    "            'catboost1_without_text',\n",
    "            'mcl_cgb',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'rnn',\n",
    "        'prefixs': [\n",
    "            'pretrained_bigru_cv1d_rnn', #411\n",
    "            'pretrained_bigru_attention_rnn', #411\n",
    "            'pretrained_2gru_rnn', #411\n",
    "            'selftrained_bigru_conv1d_rnn', #411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'text',\n",
    "        'prefixs': [\n",
    "            'text_lgb',#411\n",
    "            'text_cwb_rg',#411\n",
    "            'text_fm', #411\n",
    "            'text_rg', #411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'regression_other',\n",
    "        'prefixs': [\n",
    "            'mlp',#411\n",
    "            'alpha_0001',#411\n",
    "            'alpha_160',#411\n",
    "            'alpha_10',#411\n",
    "            'alpha_320'#411\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'classfication_other',\n",
    "        'prefixs': [\n",
    "            'lr_l1_05',#411\n",
    "            'lr_l1_1',#411\n",
    "            'lr_l2_01',#411\n",
    "            'lr_l2_1',#411\n",
    "            'cls05_lgb',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'classfication_0',\n",
    "        'prefixs': [\n",
    "            'cls0_lgb',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'multiclass',\n",
    "        'prefixs': [\n",
    "            'multiclass_lgb',\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'name': 'multiclass3',\n",
    "        'prefixs': [\n",
    "            'multiclass3_lgb',\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing group:  lgb\n",
      "Add  lgb411_tune\n",
      "Add  plants_lgb\n",
      "Add  plants_with_img_meta_nima_fm_geo_active_lgb\n",
      "Add  xentropy_small_lr_cat_lgb\n",
      "Add  simple_feature_lgb\n",
      "Add  all_mean_enc_lgb\n",
      "Add  all_mean_enc_user_feat_lgb\n",
      "Add  all_mean_enc_user_feat2_lgb\n",
      "Add  cat_interact_lgb\n",
      "Add  mean_enc_lgb\n",
      "Add  marcus_lgb\n",
      "Add  fused_text_lgb\n",
      "Add  mixed_features_text_proprocessing_lgb\n",
      "Add  select_dense_features_lgb\n",
      "Add  select_sparse_features_lgb\n",
      "Processing group:  lgb_dart\n",
      "Add  lgb411_dart_tune\n",
      "Processing group:  lgb_pois\n",
      "Add  poisson_lgb\n",
      "Processing group:  xgb_lg\n",
      "Add  small_features_v5_xgb\n",
      "Add  small_features_v4_xgb\n",
      "Add  nima_features_xgb\n",
      "Add  img_meta_xgb\n",
      "Add  img_meta_nima_xgb\n",
      "Processing group:  xgb_dw\n",
      "Add  baseline_xgb\n",
      "Processing group:  xgb_ranking\n",
      "Add  ranking_xgb\n",
      "Processing group:  catboost\n",
      "Add  catboost\n",
      "Add  catboost1_without_text\n",
      "Add  mcl_cgb\n",
      "Processing group:  rnn\n",
      "Add  pretrained_bigru_cv1d_rnn\n",
      "Add  pretrained_bigru_attention_rnn\n",
      "Add  pretrained_2gru_rnn\n",
      "Add  selftrained_bigru_conv1d_rnn\n",
      "Processing group:  text\n",
      "Add  text_lgb\n",
      "Add  text_cwb_rg\n",
      "Add  text_fm\n",
      "Add  text_rg\n",
      "Processing group:  regression_other\n",
      "Add  mlp\n",
      "Add  alpha_0001\n",
      "Add  alpha_160\n",
      "Add  alpha_10\n",
      "Add  alpha_320\n",
      "Processing group:  classfication_other\n",
      "Add  lr_l1_05\n",
      "Add  lr_l1_1\n",
      "Add  lr_l2_01\n",
      "Add  lr_l2_1\n",
      "Add  cls05_lgb\n",
      "Processing group:  classfication_0\n",
      "Add  cls0_lgb\n",
      "Processing group:  multiclass\n",
      "Add  multiclass_lgbmulticlass_lgb_pred0\n",
      "Add  multiclass_lgbmulticlass_lgb_pred1\n",
      "Add  multiclass_lgbmulticlass_lgb_pred2\n",
      "Add  multiclass_lgbmulticlass_lgb_pred3\n",
      "Add  multiclass_lgbmulticlass_lgb_pred4\n",
      "Add  multiclass_lgbmulticlass_lgb_pred5\n",
      "Add  multiclass_lgbmulticlass_lgb_pred6\n",
      "Add  multiclass_lgbmulticlass_lgb_pred7\n",
      "Add  multiclass_lgbmulticlass_lgb_pred8\n",
      "Add  multiclass_lgbmulticlass_lgbrank\n",
      "Processing group:  multiclass3\n",
      "Add  multiclass3_lgbmulticlass3_lgb_pred0\n",
      "Add  multiclass3_lgbmulticlass3_lgb_pred1\n",
      "Add  multiclass3_lgbmulticlass3_lgb_pred2\n",
      "Add  multiclass3_lgbmulticlass3_lgbrank\n",
      "Add  lgb411_tune_img_meta_xgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_mlp_inter  statistcs\n",
      "Add  lgb411_tune_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  mcl_cgb_mlp_inter  statistcs\n",
      "Add  mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  text_lgb_mlp_inter  statistcs\n",
      "Add  text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  mlp_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_baseline_xgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_mlp_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_mlp_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter  statistcs\n",
      "Add  text_lgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_mlp_inter  statistcs\n",
      "Add  lgb411_tune_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  mcl_cgb_mlp_inter  statistcs\n",
      "Add  mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  text_lgb_mlp_inter  statistcs\n",
      "Add  text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  mlp_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_baseline_xgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_img_meta_xgb_lr_l2_01_inter  statistcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  lgb411_tune_baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_mlp_inter  statistcs\n",
      "Add  lgb411_tune_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  lgb411_tune_mlp_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_mcl_cgb_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_baseline_xgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_mlp_inter  statistcs\n",
      "Add  img_meta_xgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  img_meta_xgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_mcl_cgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_mlp_inter  statistcs\n",
      "Add  baseline_xgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  baseline_xgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter  statistcs\n",
      "Add  mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_mlp_inter  statistcs\n",
      "Add  mcl_cgb_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  mcl_cgb_mlp_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter  statistcs\n",
      "Add  selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter  statistcs\n",
      "Add  text_lgb_mlp_lr_l2_01_inter  statistcs\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import hmean\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "def get_clipped_values(a):\n",
    "    return np.clip(a, 1e-15, 1.)\n",
    "\n",
    "inter_columns = []\n",
    "basic_columns = []\n",
    "for config in configs:\n",
    "    columns = []\n",
    "    print('Processing group: ', config['name'])\n",
    "    for prefix in config['prefixs']:\n",
    "        train_f = folder + prefix + '_oof_val_pred.csv'\n",
    "        test_f = folder + prefix + '_oof_test_pred.csv'\n",
    "        \n",
    "        train_df = pd.read_csv(train_f)\n",
    "        test_df = pd.read_csv(test_f)\n",
    "        \n",
    "        if config['name'] == 'multiclass' or config['name'] == 'multiclass3':\n",
    "            original_cols = train_df.columns.tolist()\n",
    "            \n",
    "            for c in original_cols:\n",
    "                col = prefix+c\n",
    "                print('Add ', col)\n",
    "                \n",
    "                train.loc[:,col] = train_df[c]\n",
    "                test.loc[:,col] = test_df[c]\n",
    "        else:\n",
    "            original_col = train_df.columns.tolist()[0]\n",
    "            col = prefix\n",
    "            print('Add ', col)\n",
    "            columns.append(col)\n",
    "            basic_columns.append(col)\n",
    "            if prefix in best_prefixs:\n",
    "                inter_columns.append(col)\n",
    "\n",
    "            train.loc[:,col] = train_df[original_col]\n",
    "            test.loc[:,col] = test_df[original_col]\n",
    "        \n",
    "        del train_df, test_df; gc.collect()\n",
    "    \n",
    "    # apply feature engineering on intra-group columns\n",
    "    if len(columns) < 2:\n",
    "        continue\n",
    "\n",
    "    for df in [train, test]:    \n",
    "        df.loc[:, config['name']+'_mean'] = df[columns].mean(axis=1)\n",
    "        df.loc[:, config['name']+'_med'] = df[columns].median(axis=1)\n",
    "        df.loc[:, config['name']+'_max'] = df[columns].max(axis=1)\n",
    "        df.loc[:, config['name']+'_min'] = df[columns].min(axis=1)\n",
    "        df.loc[:, config['name']+'_std'] = df[columns].std(axis=1)\n",
    "\n",
    "        '''\n",
    "        col_len = len(columns)\n",
    "        for i in range(col_len-1):\n",
    "            for j in range(i+1, col_len):\n",
    "                cols = [columns[i], columns[j]]\n",
    "                feat_name = cols[0]+'_'+cols[1] \n",
    "                print('Add ', feat_name, ' statistcs')        \n",
    "                df.loc[:, feat_name+'_mean'] = df[cols].mean(axis=1)\n",
    "                df.loc[:, feat_name+'_gmean'] = gmean(get_clipped_values(df[cols].values), axis=1)\n",
    "                df.loc[:, feat_name+'_hmean'] = hmean(get_clipped_values(df[cols].values), axis=1)\n",
    "\n",
    "        if col_len < 3:\n",
    "            continue\n",
    "\n",
    "        for i in range(col_len-2):\n",
    "            for j in range(i+1, col_len-1):\n",
    "                for k in range(j+1, col_len):\n",
    "                    cols = [columns[i], columns[j], columns[k]]\n",
    "                    feat_name = cols[0]+'_'+cols[1]+'_'+cols[2]  \n",
    "                    print('Add ', feat_name, ' statistcs')                    \n",
    "                    df.loc[:, feat_name+'_mean'] = df[cols].mean(axis=1)\n",
    "                    df.loc[:, feat_name+'_gmean'] = gmean(get_clipped_values(df[cols].values), axis=1)\n",
    "                    df.loc[:, feat_name+'_hmean'] = hmean(get_clipped_values(df[cols].values), axis=1)\n",
    "                    df.loc[:, feat_name+'_med'] = df[cols].median(axis=1)\n",
    "                    df.loc[:, feat_name+'_std'] = df[cols].std(axis=1)\n",
    "        '''\n",
    "        #df = df.astype(np.float32)\n",
    "        \n",
    "# apply feature engineering on inter_group columns\n",
    "for df in [train, test]:    \n",
    "    df.loc[:, 'inter_group_mean'] = df[inter_columns].mean(axis=1)\n",
    "    df.loc[:, 'inter_group_med'] = df[inter_columns].median(axis=1)\n",
    "    df.loc[:, 'inter_group_max'] = df[inter_columns].max(axis=1)\n",
    "    df.loc[:, 'inter_group_min'] = df[inter_columns].min(axis=1)\n",
    "    df.loc[:, 'inter_group_std'] = df[inter_columns].std(axis=1)\n",
    "\n",
    "    col_len = len(inter_columns)\n",
    "    for i in range(col_len-1):\n",
    "        for j in range(i+1, col_len):\n",
    "            cols = [inter_columns[i], inter_columns[j]]\n",
    "            feat_name = cols[0]+'_'+cols[1]+'_inter'\n",
    "            print('Add ', feat_name, ' statistcs')        \n",
    "            df.loc[:, feat_name+'_mean'] = df[cols].mean(axis=1)\n",
    "            df.loc[:, feat_name+'_gmean'] = gmean(get_clipped_values(df[cols].values), axis=1)\n",
    "            df.loc[:, feat_name+'_hmean'] = hmean(get_clipped_values(df[cols].values), axis=1)\n",
    "    \n",
    "    for i in range(col_len-2):\n",
    "        for j in range(i+1, col_len-1):\n",
    "            for k in range(j+1, col_len):\n",
    "                cols = [inter_columns[i], inter_columns[j], inter_columns[k]]\n",
    "                feat_name = cols[0]+'_'+cols[1]+'_'+cols[2]+'_inter'  \n",
    "                print('Add ', feat_name, ' statistcs')                    \n",
    "                df.loc[:, feat_name+'_mean'] = df[cols].mean(axis=1)\n",
    "                df.loc[:, feat_name+'_gmean'] = gmean(get_clipped_values(df[cols].values), axis=1)\n",
    "                df.loc[:, feat_name+'_hmean'] = hmean(get_clipped_values(df[cols].values), axis=1)\n",
    "                df.loc[:, feat_name+'_med'] = df[cols].median(axis=1)\n",
    "                df.loc[:, feat_name+'_std'] = df[cols].std(axis=1)  \n",
    "             \n",
    "    #df = df.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb411_tune</th>\n",
       "      <th>plants_lgb</th>\n",
       "      <th>plants_with_img_meta_nima_fm_geo_active_lgb</th>\n",
       "      <th>xentropy_small_lr_cat_lgb</th>\n",
       "      <th>simple_feature_lgb</th>\n",
       "      <th>all_mean_enc_lgb</th>\n",
       "      <th>all_mean_enc_user_feat_lgb</th>\n",
       "      <th>all_mean_enc_user_feat2_lgb</th>\n",
       "      <th>cat_interact_lgb</th>\n",
       "      <th>mean_enc_lgb</th>\n",
       "      <th>...</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_mean</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_gmean</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_hmean</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_med</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_std</th>\n",
       "      <th>text_lgb_mlp_lr_l2_01_inter_mean</th>\n",
       "      <th>text_lgb_mlp_lr_l2_01_inter_gmean</th>\n",
       "      <th>text_lgb_mlp_lr_l2_01_inter_hmean</th>\n",
       "      <th>text_lgb_mlp_lr_l2_01_inter_med</th>\n",
       "      <th>text_lgb_mlp_lr_l2_01_inter_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062362</td>\n",
       "      <td>0.053896</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.047786</td>\n",
       "      <td>0.068429</td>\n",
       "      <td>0.054375</td>\n",
       "      <td>0.054969</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.065756</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.026702</td>\n",
       "      <td>0.026547</td>\n",
       "      <td>0.028362</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>0.032057</td>\n",
       "      <td>0.031740</td>\n",
       "      <td>0.031446</td>\n",
       "      <td>0.029213</td>\n",
       "      <td>0.005679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.057651</td>\n",
       "      <td>0.057721</td>\n",
       "      <td>0.062043</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>0.014746</td>\n",
       "      <td>0.018057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018156</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>0.015426</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.016485</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>0.011623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.040794</td>\n",
       "      <td>0.048324</td>\n",
       "      <td>0.052663</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>0.036656</td>\n",
       "      <td>0.034745</td>\n",
       "      <td>0.035856</td>\n",
       "      <td>0.035003</td>\n",
       "      <td>0.041355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.037182</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.035334</td>\n",
       "      <td>0.012653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lgb411_tune  plants_lgb  plants_with_img_meta_nima_fm_geo_active_lgb  \\\n",
       "0     0.062362    0.053896                                     0.071850   \n",
       "1     0.041640    0.057651                                     0.057721   \n",
       "2     0.031383    0.040794                                     0.048324   \n",
       "\n",
       "   xentropy_small_lr_cat_lgb  simple_feature_lgb  all_mean_enc_lgb  \\\n",
       "0                   0.047786            0.068429          0.054375   \n",
       "1                   0.062043            0.040434          0.015163   \n",
       "2                   0.052663            0.036796          0.036656   \n",
       "\n",
       "   all_mean_enc_user_feat_lgb  all_mean_enc_user_feat2_lgb  cat_interact_lgb  \\\n",
       "0                    0.054969                     0.052336          0.065756   \n",
       "1                    0.015783                     0.017107          0.014746   \n",
       "2                    0.034745                     0.035856          0.035003   \n",
       "\n",
       "   mean_enc_lgb               ...                 \\\n",
       "0      0.048808               ...                  \n",
       "1      0.018057               ...                  \n",
       "2      0.041355               ...                  \n",
       "\n",
       "   selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_mean  \\\n",
       "0                                           0.026851      \n",
       "1                                           0.018156      \n",
       "2                                           0.030409      \n",
       "\n",
       "   selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_gmean  \\\n",
       "0                                           0.026702       \n",
       "1                                           0.016859       \n",
       "2                                           0.028043       \n",
       "\n",
       "   selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_hmean  \\\n",
       "0                                           0.026547       \n",
       "1                                           0.015426       \n",
       "2                                           0.025421       \n",
       "\n",
       "   selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_med  \\\n",
       "0                                           0.028362     \n",
       "1                                           0.021763     \n",
       "2                                           0.037182     \n",
       "\n",
       "   selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_std  \\\n",
       "0                                           0.003379     \n",
       "1                                           0.007547     \n",
       "2                                           0.013095     \n",
       "\n",
       "   text_lgb_mlp_lr_l2_01_inter_mean  text_lgb_mlp_lr_l2_01_inter_gmean  \\\n",
       "0                          0.032057                           0.031740   \n",
       "1                          0.021320                           0.018900   \n",
       "2                          0.029793                           0.027571   \n",
       "\n",
       "   text_lgb_mlp_lr_l2_01_inter_hmean  text_lgb_mlp_lr_l2_01_inter_med  \\\n",
       "0                           0.031446                         0.029213   \n",
       "1                           0.016485                         0.021763   \n",
       "2                           0.025122                         0.035334   \n",
       "\n",
       "   text_lgb_mlp_lr_l2_01_inter_std  \n",
       "0                         0.005679  \n",
       "1                         0.011623  \n",
       "2                         0.012653  \n",
       "\n",
       "[3 rows x 464 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb411_tune</th>\n",
       "      <th>plants_lgb</th>\n",
       "      <th>plants_with_img_meta_nima_fm_geo_active_lgb</th>\n",
       "      <th>xentropy_small_lr_cat_lgb</th>\n",
       "      <th>simple_feature_lgb</th>\n",
       "      <th>all_mean_enc_lgb</th>\n",
       "      <th>all_mean_enc_user_feat_lgb</th>\n",
       "      <th>all_mean_enc_user_feat2_lgb</th>\n",
       "      <th>cat_interact_lgb</th>\n",
       "      <th>mean_enc_lgb</th>\n",
       "      <th>marcus_lgb</th>\n",
       "      <th>fused_text_lgb</th>\n",
       "      <th>mixed_features_text_proprocessing_lgb</th>\n",
       "      <th>select_dense_features_lgb</th>\n",
       "      <th>select_sparse_features_lgb</th>\n",
       "      <th>lgb411_dart_tune</th>\n",
       "      <th>poisson_lgb</th>\n",
       "      <th>small_features_v5_xgb</th>\n",
       "      <th>small_features_v4_xgb</th>\n",
       "      <th>nima_features_xgb</th>\n",
       "      <th>img_meta_xgb</th>\n",
       "      <th>img_meta_nima_xgb</th>\n",
       "      <th>baseline_xgb</th>\n",
       "      <th>ranking_xgb</th>\n",
       "      <th>catboost</th>\n",
       "      <th>catboost1_without_text</th>\n",
       "      <th>mcl_cgb</th>\n",
       "      <th>pretrained_bigru_cv1d_rnn</th>\n",
       "      <th>pretrained_bigru_attention_rnn</th>\n",
       "      <th>pretrained_2gru_rnn</th>\n",
       "      <th>selftrained_bigru_conv1d_rnn</th>\n",
       "      <th>text_lgb</th>\n",
       "      <th>text_cwb_rg</th>\n",
       "      <th>text_fm</th>\n",
       "      <th>text_rg</th>\n",
       "      <th>mlp</th>\n",
       "      <th>alpha_0001</th>\n",
       "      <th>alpha_160</th>\n",
       "      <th>alpha_10</th>\n",
       "      <th>alpha_320</th>\n",
       "      <th>lr_l1_05</th>\n",
       "      <th>lr_l1_1</th>\n",
       "      <th>lr_l2_01</th>\n",
       "      <th>lr_l2_1</th>\n",
       "      <th>cls05_lgb</th>\n",
       "      <th>cls0_lgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgb411_tune</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967159</td>\n",
       "      <td>0.976990</td>\n",
       "      <td>0.967743</td>\n",
       "      <td>0.905456</td>\n",
       "      <td>0.907395</td>\n",
       "      <td>0.907480</td>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.902366</td>\n",
       "      <td>0.906453</td>\n",
       "      <td>0.919712</td>\n",
       "      <td>0.914794</td>\n",
       "      <td>0.915861</td>\n",
       "      <td>0.906655</td>\n",
       "      <td>0.912361</td>\n",
       "      <td>0.982080</td>\n",
       "      <td>0.985545</td>\n",
       "      <td>0.931473</td>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.935678</td>\n",
       "      <td>0.940697</td>\n",
       "      <td>0.943550</td>\n",
       "      <td>0.977573</td>\n",
       "      <td>0.769518</td>\n",
       "      <td>0.896485</td>\n",
       "      <td>0.896485</td>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.895204</td>\n",
       "      <td>0.894450</td>\n",
       "      <td>0.891942</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>0.823135</td>\n",
       "      <td>0.685161</td>\n",
       "      <td>0.821971</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.813002</td>\n",
       "      <td>0.847543</td>\n",
       "      <td>0.828779</td>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.800982</td>\n",
       "      <td>0.822777</td>\n",
       "      <td>0.795435</td>\n",
       "      <td>0.945198</td>\n",
       "      <td>0.788349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plants_lgb</th>\n",
       "      <td>0.967159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968862</td>\n",
       "      <td>0.951730</td>\n",
       "      <td>0.911633</td>\n",
       "      <td>0.912456</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.904745</td>\n",
       "      <td>0.907654</td>\n",
       "      <td>0.912110</td>\n",
       "      <td>0.914469</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>0.921965</td>\n",
       "      <td>0.896150</td>\n",
       "      <td>0.902178</td>\n",
       "      <td>0.966236</td>\n",
       "      <td>0.963319</td>\n",
       "      <td>0.931010</td>\n",
       "      <td>0.926905</td>\n",
       "      <td>0.930084</td>\n",
       "      <td>0.925493</td>\n",
       "      <td>0.924579</td>\n",
       "      <td>0.955797</td>\n",
       "      <td>0.765113</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.969725</td>\n",
       "      <td>0.897465</td>\n",
       "      <td>0.896696</td>\n",
       "      <td>0.894225</td>\n",
       "      <td>0.897245</td>\n",
       "      <td>0.835321</td>\n",
       "      <td>0.695687</td>\n",
       "      <td>0.834781</td>\n",
       "      <td>0.836749</td>\n",
       "      <td>0.874204</td>\n",
       "      <td>0.824736</td>\n",
       "      <td>0.860072</td>\n",
       "      <td>0.840884</td>\n",
       "      <td>0.863767</td>\n",
       "      <td>0.823825</td>\n",
       "      <td>0.811323</td>\n",
       "      <td>0.833793</td>\n",
       "      <td>0.805571</td>\n",
       "      <td>0.919283</td>\n",
       "      <td>0.781393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plants_with_img_meta_nima_fm_geo_active_lgb</th>\n",
       "      <td>0.976990</td>\n",
       "      <td>0.968862</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957623</td>\n",
       "      <td>0.904752</td>\n",
       "      <td>0.906086</td>\n",
       "      <td>0.906017</td>\n",
       "      <td>0.898062</td>\n",
       "      <td>0.900886</td>\n",
       "      <td>0.905318</td>\n",
       "      <td>0.915583</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>0.915626</td>\n",
       "      <td>0.901881</td>\n",
       "      <td>0.907734</td>\n",
       "      <td>0.972245</td>\n",
       "      <td>0.972814</td>\n",
       "      <td>0.924954</td>\n",
       "      <td>0.920816</td>\n",
       "      <td>0.928658</td>\n",
       "      <td>0.931048</td>\n",
       "      <td>0.933453</td>\n",
       "      <td>0.964709</td>\n",
       "      <td>0.768467</td>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>0.891990</td>\n",
       "      <td>0.891134</td>\n",
       "      <td>0.888897</td>\n",
       "      <td>0.891522</td>\n",
       "      <td>0.828427</td>\n",
       "      <td>0.690939</td>\n",
       "      <td>0.827950</td>\n",
       "      <td>0.830028</td>\n",
       "      <td>0.867792</td>\n",
       "      <td>0.818223</td>\n",
       "      <td>0.853142</td>\n",
       "      <td>0.834198</td>\n",
       "      <td>0.856787</td>\n",
       "      <td>0.816884</td>\n",
       "      <td>0.804517</td>\n",
       "      <td>0.826794</td>\n",
       "      <td>0.798866</td>\n",
       "      <td>0.931634</td>\n",
       "      <td>0.782935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xentropy_small_lr_cat_lgb</th>\n",
       "      <td>0.967743</td>\n",
       "      <td>0.951730</td>\n",
       "      <td>0.957623</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914189</td>\n",
       "      <td>0.917104</td>\n",
       "      <td>0.916845</td>\n",
       "      <td>0.907965</td>\n",
       "      <td>0.911571</td>\n",
       "      <td>0.916972</td>\n",
       "      <td>0.932491</td>\n",
       "      <td>0.925647</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.919620</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.963631</td>\n",
       "      <td>0.964006</td>\n",
       "      <td>0.923585</td>\n",
       "      <td>0.919470</td>\n",
       "      <td>0.926771</td>\n",
       "      <td>0.928518</td>\n",
       "      <td>0.930603</td>\n",
       "      <td>0.954971</td>\n",
       "      <td>0.758656</td>\n",
       "      <td>0.888887</td>\n",
       "      <td>0.888887</td>\n",
       "      <td>0.961636</td>\n",
       "      <td>0.901173</td>\n",
       "      <td>0.900185</td>\n",
       "      <td>0.897477</td>\n",
       "      <td>0.899517</td>\n",
       "      <td>0.827092</td>\n",
       "      <td>0.686990</td>\n",
       "      <td>0.827854</td>\n",
       "      <td>0.829751</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.821121</td>\n",
       "      <td>0.856480</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>0.810223</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.804675</td>\n",
       "      <td>0.919523</td>\n",
       "      <td>0.779242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_feature_lgb</th>\n",
       "      <td>0.905456</td>\n",
       "      <td>0.911633</td>\n",
       "      <td>0.904752</td>\n",
       "      <td>0.914189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.968871</td>\n",
       "      <td>0.967921</td>\n",
       "      <td>0.957815</td>\n",
       "      <td>0.966615</td>\n",
       "      <td>0.971213</td>\n",
       "      <td>0.956146</td>\n",
       "      <td>0.936530</td>\n",
       "      <td>0.939173</td>\n",
       "      <td>0.933709</td>\n",
       "      <td>0.945776</td>\n",
       "      <td>0.908589</td>\n",
       "      <td>0.900190</td>\n",
       "      <td>0.922071</td>\n",
       "      <td>0.918064</td>\n",
       "      <td>0.921465</td>\n",
       "      <td>0.916744</td>\n",
       "      <td>0.916079</td>\n",
       "      <td>0.893908</td>\n",
       "      <td>0.738648</td>\n",
       "      <td>0.902593</td>\n",
       "      <td>0.902593</td>\n",
       "      <td>0.918213</td>\n",
       "      <td>0.919504</td>\n",
       "      <td>0.918586</td>\n",
       "      <td>0.916392</td>\n",
       "      <td>0.914868</td>\n",
       "      <td>0.869182</td>\n",
       "      <td>0.716797</td>\n",
       "      <td>0.849560</td>\n",
       "      <td>0.853084</td>\n",
       "      <td>0.893489</td>\n",
       "      <td>0.841548</td>\n",
       "      <td>0.880072</td>\n",
       "      <td>0.858496</td>\n",
       "      <td>0.884830</td>\n",
       "      <td>0.838050</td>\n",
       "      <td>0.822719</td>\n",
       "      <td>0.847758</td>\n",
       "      <td>0.814407</td>\n",
       "      <td>0.849272</td>\n",
       "      <td>0.754601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mean_enc_lgb</th>\n",
       "      <td>0.907395</td>\n",
       "      <td>0.912456</td>\n",
       "      <td>0.906086</td>\n",
       "      <td>0.917104</td>\n",
       "      <td>0.968871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993119</td>\n",
       "      <td>0.979055</td>\n",
       "      <td>0.973705</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>0.939821</td>\n",
       "      <td>0.943974</td>\n",
       "      <td>0.939846</td>\n",
       "      <td>0.953933</td>\n",
       "      <td>0.909640</td>\n",
       "      <td>0.901865</td>\n",
       "      <td>0.925226</td>\n",
       "      <td>0.921064</td>\n",
       "      <td>0.924221</td>\n",
       "      <td>0.919339</td>\n",
       "      <td>0.918438</td>\n",
       "      <td>0.895836</td>\n",
       "      <td>0.737372</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.922844</td>\n",
       "      <td>0.921886</td>\n",
       "      <td>0.919655</td>\n",
       "      <td>0.917843</td>\n",
       "      <td>0.866855</td>\n",
       "      <td>0.710733</td>\n",
       "      <td>0.846129</td>\n",
       "      <td>0.848887</td>\n",
       "      <td>0.896498</td>\n",
       "      <td>0.846315</td>\n",
       "      <td>0.878901</td>\n",
       "      <td>0.860962</td>\n",
       "      <td>0.882902</td>\n",
       "      <td>0.840290</td>\n",
       "      <td>0.826367</td>\n",
       "      <td>0.848981</td>\n",
       "      <td>0.818613</td>\n",
       "      <td>0.851784</td>\n",
       "      <td>0.752769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mean_enc_user_feat_lgb</th>\n",
       "      <td>0.907480</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.906017</td>\n",
       "      <td>0.916845</td>\n",
       "      <td>0.967921</td>\n",
       "      <td>0.993119</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980549</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>0.988447</td>\n",
       "      <td>0.962874</td>\n",
       "      <td>0.940541</td>\n",
       "      <td>0.944687</td>\n",
       "      <td>0.939407</td>\n",
       "      <td>0.953526</td>\n",
       "      <td>0.909536</td>\n",
       "      <td>0.901898</td>\n",
       "      <td>0.925803</td>\n",
       "      <td>0.921642</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.919893</td>\n",
       "      <td>0.919009</td>\n",
       "      <td>0.895842</td>\n",
       "      <td>0.737337</td>\n",
       "      <td>0.902602</td>\n",
       "      <td>0.902602</td>\n",
       "      <td>0.918047</td>\n",
       "      <td>0.921836</td>\n",
       "      <td>0.920802</td>\n",
       "      <td>0.918679</td>\n",
       "      <td>0.916663</td>\n",
       "      <td>0.865516</td>\n",
       "      <td>0.709698</td>\n",
       "      <td>0.844977</td>\n",
       "      <td>0.847646</td>\n",
       "      <td>0.896866</td>\n",
       "      <td>0.846989</td>\n",
       "      <td>0.879539</td>\n",
       "      <td>0.861631</td>\n",
       "      <td>0.883506</td>\n",
       "      <td>0.841827</td>\n",
       "      <td>0.827788</td>\n",
       "      <td>0.850257</td>\n",
       "      <td>0.819961</td>\n",
       "      <td>0.852166</td>\n",
       "      <td>0.752736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mean_enc_user_feat2_lgb</th>\n",
       "      <td>0.899326</td>\n",
       "      <td>0.904745</td>\n",
       "      <td>0.898062</td>\n",
       "      <td>0.907965</td>\n",
       "      <td>0.957815</td>\n",
       "      <td>0.979055</td>\n",
       "      <td>0.980549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>0.974986</td>\n",
       "      <td>0.951025</td>\n",
       "      <td>0.932428</td>\n",
       "      <td>0.936438</td>\n",
       "      <td>0.928221</td>\n",
       "      <td>0.942182</td>\n",
       "      <td>0.902085</td>\n",
       "      <td>0.893906</td>\n",
       "      <td>0.917543</td>\n",
       "      <td>0.913503</td>\n",
       "      <td>0.916518</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.910830</td>\n",
       "      <td>0.887875</td>\n",
       "      <td>0.737129</td>\n",
       "      <td>0.892868</td>\n",
       "      <td>0.892868</td>\n",
       "      <td>0.910381</td>\n",
       "      <td>0.914385</td>\n",
       "      <td>0.913422</td>\n",
       "      <td>0.911270</td>\n",
       "      <td>0.909777</td>\n",
       "      <td>0.871939</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.851349</td>\n",
       "      <td>0.854074</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.877224</td>\n",
       "      <td>0.858859</td>\n",
       "      <td>0.881255</td>\n",
       "      <td>0.834829</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.813033</td>\n",
       "      <td>0.842887</td>\n",
       "      <td>0.752974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_interact_lgb</th>\n",
       "      <td>0.902366</td>\n",
       "      <td>0.907654</td>\n",
       "      <td>0.900886</td>\n",
       "      <td>0.911571</td>\n",
       "      <td>0.966615</td>\n",
       "      <td>0.973705</td>\n",
       "      <td>0.972853</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976066</td>\n",
       "      <td>0.960817</td>\n",
       "      <td>0.933094</td>\n",
       "      <td>0.936761</td>\n",
       "      <td>0.937580</td>\n",
       "      <td>0.950516</td>\n",
       "      <td>0.903797</td>\n",
       "      <td>0.895902</td>\n",
       "      <td>0.920951</td>\n",
       "      <td>0.916776</td>\n",
       "      <td>0.919459</td>\n",
       "      <td>0.914348</td>\n",
       "      <td>0.913160</td>\n",
       "      <td>0.890089</td>\n",
       "      <td>0.734428</td>\n",
       "      <td>0.902092</td>\n",
       "      <td>0.902092</td>\n",
       "      <td>0.913036</td>\n",
       "      <td>0.916719</td>\n",
       "      <td>0.915921</td>\n",
       "      <td>0.913904</td>\n",
       "      <td>0.910945</td>\n",
       "      <td>0.862606</td>\n",
       "      <td>0.710778</td>\n",
       "      <td>0.841915</td>\n",
       "      <td>0.845153</td>\n",
       "      <td>0.891943</td>\n",
       "      <td>0.835859</td>\n",
       "      <td>0.873883</td>\n",
       "      <td>0.852646</td>\n",
       "      <td>0.878525</td>\n",
       "      <td>0.832751</td>\n",
       "      <td>0.817674</td>\n",
       "      <td>0.841905</td>\n",
       "      <td>0.809139</td>\n",
       "      <td>0.845405</td>\n",
       "      <td>0.749832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_enc_lgb</th>\n",
       "      <td>0.906453</td>\n",
       "      <td>0.912110</td>\n",
       "      <td>0.905318</td>\n",
       "      <td>0.916972</td>\n",
       "      <td>0.971213</td>\n",
       "      <td>0.989717</td>\n",
       "      <td>0.988447</td>\n",
       "      <td>0.974986</td>\n",
       "      <td>0.976066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962645</td>\n",
       "      <td>0.938839</td>\n",
       "      <td>0.943108</td>\n",
       "      <td>0.937892</td>\n",
       "      <td>0.952404</td>\n",
       "      <td>0.909121</td>\n",
       "      <td>0.900995</td>\n",
       "      <td>0.923394</td>\n",
       "      <td>0.919352</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.917717</td>\n",
       "      <td>0.916878</td>\n",
       "      <td>0.894925</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.902028</td>\n",
       "      <td>0.902028</td>\n",
       "      <td>0.918151</td>\n",
       "      <td>0.921847</td>\n",
       "      <td>0.921070</td>\n",
       "      <td>0.918656</td>\n",
       "      <td>0.917476</td>\n",
       "      <td>0.869846</td>\n",
       "      <td>0.712374</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.851942</td>\n",
       "      <td>0.897334</td>\n",
       "      <td>0.848833</td>\n",
       "      <td>0.881341</td>\n",
       "      <td>0.863476</td>\n",
       "      <td>0.885262</td>\n",
       "      <td>0.841630</td>\n",
       "      <td>0.827670</td>\n",
       "      <td>0.850187</td>\n",
       "      <td>0.819786</td>\n",
       "      <td>0.850715</td>\n",
       "      <td>0.753346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marcus_lgb</th>\n",
       "      <td>0.919712</td>\n",
       "      <td>0.914469</td>\n",
       "      <td>0.915583</td>\n",
       "      <td>0.932491</td>\n",
       "      <td>0.956146</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>0.962874</td>\n",
       "      <td>0.951025</td>\n",
       "      <td>0.960817</td>\n",
       "      <td>0.962645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942398</td>\n",
       "      <td>0.944862</td>\n",
       "      <td>0.960118</td>\n",
       "      <td>0.975061</td>\n",
       "      <td>0.918962</td>\n",
       "      <td>0.912005</td>\n",
       "      <td>0.933476</td>\n",
       "      <td>0.929105</td>\n",
       "      <td>0.932235</td>\n",
       "      <td>0.934714</td>\n",
       "      <td>0.933102</td>\n",
       "      <td>0.906859</td>\n",
       "      <td>0.736728</td>\n",
       "      <td>0.919560</td>\n",
       "      <td>0.919560</td>\n",
       "      <td>0.925841</td>\n",
       "      <td>0.919778</td>\n",
       "      <td>0.918981</td>\n",
       "      <td>0.916932</td>\n",
       "      <td>0.912860</td>\n",
       "      <td>0.852152</td>\n",
       "      <td>0.701711</td>\n",
       "      <td>0.832332</td>\n",
       "      <td>0.835086</td>\n",
       "      <td>0.887590</td>\n",
       "      <td>0.825604</td>\n",
       "      <td>0.863308</td>\n",
       "      <td>0.842154</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>0.826684</td>\n",
       "      <td>0.811957</td>\n",
       "      <td>0.836491</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>0.865770</td>\n",
       "      <td>0.752856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fused_text_lgb</th>\n",
       "      <td>0.914794</td>\n",
       "      <td>0.921208</td>\n",
       "      <td>0.914908</td>\n",
       "      <td>0.925647</td>\n",
       "      <td>0.936530</td>\n",
       "      <td>0.939821</td>\n",
       "      <td>0.940541</td>\n",
       "      <td>0.932428</td>\n",
       "      <td>0.933094</td>\n",
       "      <td>0.938839</td>\n",
       "      <td>0.942398</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977439</td>\n",
       "      <td>0.925521</td>\n",
       "      <td>0.930711</td>\n",
       "      <td>0.918812</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.950737</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.950493</td>\n",
       "      <td>0.945683</td>\n",
       "      <td>0.945216</td>\n",
       "      <td>0.904365</td>\n",
       "      <td>0.741206</td>\n",
       "      <td>0.904336</td>\n",
       "      <td>0.904336</td>\n",
       "      <td>0.928662</td>\n",
       "      <td>0.917589</td>\n",
       "      <td>0.916410</td>\n",
       "      <td>0.914216</td>\n",
       "      <td>0.915931</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.715492</td>\n",
       "      <td>0.868791</td>\n",
       "      <td>0.870874</td>\n",
       "      <td>0.900485</td>\n",
       "      <td>0.853381</td>\n",
       "      <td>0.891558</td>\n",
       "      <td>0.870849</td>\n",
       "      <td>0.895356</td>\n",
       "      <td>0.851669</td>\n",
       "      <td>0.839089</td>\n",
       "      <td>0.861994</td>\n",
       "      <td>0.832904</td>\n",
       "      <td>0.862263</td>\n",
       "      <td>0.758297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixed_features_text_proprocessing_lgb</th>\n",
       "      <td>0.915861</td>\n",
       "      <td>0.921965</td>\n",
       "      <td>0.915626</td>\n",
       "      <td>0.925489</td>\n",
       "      <td>0.939173</td>\n",
       "      <td>0.943974</td>\n",
       "      <td>0.944687</td>\n",
       "      <td>0.936438</td>\n",
       "      <td>0.936761</td>\n",
       "      <td>0.943108</td>\n",
       "      <td>0.944862</td>\n",
       "      <td>0.977439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921109</td>\n",
       "      <td>0.932118</td>\n",
       "      <td>0.919663</td>\n",
       "      <td>0.911857</td>\n",
       "      <td>0.949450</td>\n",
       "      <td>0.945420</td>\n",
       "      <td>0.949004</td>\n",
       "      <td>0.944127</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>0.905456</td>\n",
       "      <td>0.739742</td>\n",
       "      <td>0.898912</td>\n",
       "      <td>0.898912</td>\n",
       "      <td>0.928638</td>\n",
       "      <td>0.916229</td>\n",
       "      <td>0.914980</td>\n",
       "      <td>0.912850</td>\n",
       "      <td>0.915627</td>\n",
       "      <td>0.868076</td>\n",
       "      <td>0.710091</td>\n",
       "      <td>0.867281</td>\n",
       "      <td>0.868427</td>\n",
       "      <td>0.900056</td>\n",
       "      <td>0.852450</td>\n",
       "      <td>0.889281</td>\n",
       "      <td>0.869725</td>\n",
       "      <td>0.892455</td>\n",
       "      <td>0.851668</td>\n",
       "      <td>0.839892</td>\n",
       "      <td>0.860305</td>\n",
       "      <td>0.833779</td>\n",
       "      <td>0.864107</td>\n",
       "      <td>0.756379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select_dense_features_lgb</th>\n",
       "      <td>0.906655</td>\n",
       "      <td>0.896150</td>\n",
       "      <td>0.901881</td>\n",
       "      <td>0.919620</td>\n",
       "      <td>0.933709</td>\n",
       "      <td>0.939846</td>\n",
       "      <td>0.939407</td>\n",
       "      <td>0.928221</td>\n",
       "      <td>0.937580</td>\n",
       "      <td>0.937892</td>\n",
       "      <td>0.960118</td>\n",
       "      <td>0.925521</td>\n",
       "      <td>0.921109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970594</td>\n",
       "      <td>0.905117</td>\n",
       "      <td>0.898740</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>0.911518</td>\n",
       "      <td>0.918223</td>\n",
       "      <td>0.917351</td>\n",
       "      <td>0.918931</td>\n",
       "      <td>0.893515</td>\n",
       "      <td>0.735165</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.911059</td>\n",
       "      <td>0.913667</td>\n",
       "      <td>0.912747</td>\n",
       "      <td>0.911341</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.823182</td>\n",
       "      <td>0.705327</td>\n",
       "      <td>0.804784</td>\n",
       "      <td>0.809867</td>\n",
       "      <td>0.866792</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.839233</td>\n",
       "      <td>0.814377</td>\n",
       "      <td>0.846704</td>\n",
       "      <td>0.799327</td>\n",
       "      <td>0.781676</td>\n",
       "      <td>0.815745</td>\n",
       "      <td>0.773605</td>\n",
       "      <td>0.849946</td>\n",
       "      <td>0.750064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select_sparse_features_lgb</th>\n",
       "      <td>0.912361</td>\n",
       "      <td>0.902178</td>\n",
       "      <td>0.907734</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.945776</td>\n",
       "      <td>0.953933</td>\n",
       "      <td>0.953526</td>\n",
       "      <td>0.942182</td>\n",
       "      <td>0.950516</td>\n",
       "      <td>0.952404</td>\n",
       "      <td>0.975061</td>\n",
       "      <td>0.930711</td>\n",
       "      <td>0.932118</td>\n",
       "      <td>0.970594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910978</td>\n",
       "      <td>0.904226</td>\n",
       "      <td>0.921414</td>\n",
       "      <td>0.917017</td>\n",
       "      <td>0.923615</td>\n",
       "      <td>0.922866</td>\n",
       "      <td>0.924344</td>\n",
       "      <td>0.899271</td>\n",
       "      <td>0.734971</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.916608</td>\n",
       "      <td>0.914844</td>\n",
       "      <td>0.914060</td>\n",
       "      <td>0.912147</td>\n",
       "      <td>0.907708</td>\n",
       "      <td>0.841794</td>\n",
       "      <td>0.695711</td>\n",
       "      <td>0.821912</td>\n",
       "      <td>0.824401</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>0.813741</td>\n",
       "      <td>0.851746</td>\n",
       "      <td>0.830083</td>\n",
       "      <td>0.856936</td>\n",
       "      <td>0.814837</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.826084</td>\n",
       "      <td>0.791992</td>\n",
       "      <td>0.857522</td>\n",
       "      <td>0.748902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb411_dart_tune</th>\n",
       "      <td>0.982080</td>\n",
       "      <td>0.966236</td>\n",
       "      <td>0.972245</td>\n",
       "      <td>0.963631</td>\n",
       "      <td>0.908589</td>\n",
       "      <td>0.909640</td>\n",
       "      <td>0.909536</td>\n",
       "      <td>0.902085</td>\n",
       "      <td>0.903797</td>\n",
       "      <td>0.909121</td>\n",
       "      <td>0.918962</td>\n",
       "      <td>0.918812</td>\n",
       "      <td>0.919663</td>\n",
       "      <td>0.905117</td>\n",
       "      <td>0.910978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976874</td>\n",
       "      <td>0.932515</td>\n",
       "      <td>0.928168</td>\n",
       "      <td>0.935901</td>\n",
       "      <td>0.938310</td>\n",
       "      <td>0.940572</td>\n",
       "      <td>0.970684</td>\n",
       "      <td>0.771464</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.975858</td>\n",
       "      <td>0.896763</td>\n",
       "      <td>0.896112</td>\n",
       "      <td>0.893587</td>\n",
       "      <td>0.896312</td>\n",
       "      <td>0.834262</td>\n",
       "      <td>0.699198</td>\n",
       "      <td>0.834251</td>\n",
       "      <td>0.836672</td>\n",
       "      <td>0.870983</td>\n",
       "      <td>0.822557</td>\n",
       "      <td>0.858009</td>\n",
       "      <td>0.838689</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.819196</td>\n",
       "      <td>0.806717</td>\n",
       "      <td>0.829294</td>\n",
       "      <td>0.801049</td>\n",
       "      <td>0.932960</td>\n",
       "      <td>0.794906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poisson_lgb</th>\n",
       "      <td>0.985545</td>\n",
       "      <td>0.963319</td>\n",
       "      <td>0.972814</td>\n",
       "      <td>0.964006</td>\n",
       "      <td>0.900190</td>\n",
       "      <td>0.901865</td>\n",
       "      <td>0.901898</td>\n",
       "      <td>0.893906</td>\n",
       "      <td>0.895902</td>\n",
       "      <td>0.900995</td>\n",
       "      <td>0.912005</td>\n",
       "      <td>0.911017</td>\n",
       "      <td>0.911857</td>\n",
       "      <td>0.898740</td>\n",
       "      <td>0.904226</td>\n",
       "      <td>0.976874</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925590</td>\n",
       "      <td>0.921467</td>\n",
       "      <td>0.929679</td>\n",
       "      <td>0.933917</td>\n",
       "      <td>0.936607</td>\n",
       "      <td>0.972942</td>\n",
       "      <td>0.766890</td>\n",
       "      <td>0.891646</td>\n",
       "      <td>0.891646</td>\n",
       "      <td>0.972201</td>\n",
       "      <td>0.890068</td>\n",
       "      <td>0.889364</td>\n",
       "      <td>0.886849</td>\n",
       "      <td>0.889476</td>\n",
       "      <td>0.823106</td>\n",
       "      <td>0.687793</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.824834</td>\n",
       "      <td>0.863948</td>\n",
       "      <td>0.812784</td>\n",
       "      <td>0.847676</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.851360</td>\n",
       "      <td>0.812960</td>\n",
       "      <td>0.800763</td>\n",
       "      <td>0.822877</td>\n",
       "      <td>0.795230</td>\n",
       "      <td>0.939893</td>\n",
       "      <td>0.786246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_features_v5_xgb</th>\n",
       "      <td>0.931473</td>\n",
       "      <td>0.931010</td>\n",
       "      <td>0.924954</td>\n",
       "      <td>0.923585</td>\n",
       "      <td>0.922071</td>\n",
       "      <td>0.925226</td>\n",
       "      <td>0.925803</td>\n",
       "      <td>0.917543</td>\n",
       "      <td>0.920951</td>\n",
       "      <td>0.923394</td>\n",
       "      <td>0.933476</td>\n",
       "      <td>0.950737</td>\n",
       "      <td>0.949450</td>\n",
       "      <td>0.915873</td>\n",
       "      <td>0.921414</td>\n",
       "      <td>0.932515</td>\n",
       "      <td>0.925590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987775</td>\n",
       "      <td>0.987994</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.977972</td>\n",
       "      <td>0.919868</td>\n",
       "      <td>0.742374</td>\n",
       "      <td>0.911971</td>\n",
       "      <td>0.911971</td>\n",
       "      <td>0.935648</td>\n",
       "      <td>0.911734</td>\n",
       "      <td>0.911048</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.910415</td>\n",
       "      <td>0.850251</td>\n",
       "      <td>0.702433</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.849526</td>\n",
       "      <td>0.890831</td>\n",
       "      <td>0.832539</td>\n",
       "      <td>0.868728</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>0.872565</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.820057</td>\n",
       "      <td>0.842153</td>\n",
       "      <td>0.814219</td>\n",
       "      <td>0.878056</td>\n",
       "      <td>0.766178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_features_v4_xgb</th>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.926905</td>\n",
       "      <td>0.920816</td>\n",
       "      <td>0.919470</td>\n",
       "      <td>0.918064</td>\n",
       "      <td>0.921064</td>\n",
       "      <td>0.921642</td>\n",
       "      <td>0.913503</td>\n",
       "      <td>0.916776</td>\n",
       "      <td>0.919352</td>\n",
       "      <td>0.929105</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>0.945420</td>\n",
       "      <td>0.911518</td>\n",
       "      <td>0.917017</td>\n",
       "      <td>0.928168</td>\n",
       "      <td>0.921467</td>\n",
       "      <td>0.987775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983083</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.973208</td>\n",
       "      <td>0.915654</td>\n",
       "      <td>0.740242</td>\n",
       "      <td>0.908011</td>\n",
       "      <td>0.908011</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.907881</td>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.904934</td>\n",
       "      <td>0.906435</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.699813</td>\n",
       "      <td>0.844865</td>\n",
       "      <td>0.846549</td>\n",
       "      <td>0.887099</td>\n",
       "      <td>0.829478</td>\n",
       "      <td>0.865605</td>\n",
       "      <td>0.845877</td>\n",
       "      <td>0.869439</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.817291</td>\n",
       "      <td>0.839354</td>\n",
       "      <td>0.811488</td>\n",
       "      <td>0.874278</td>\n",
       "      <td>0.762278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nima_features_xgb</th>\n",
       "      <td>0.935678</td>\n",
       "      <td>0.930084</td>\n",
       "      <td>0.928658</td>\n",
       "      <td>0.926771</td>\n",
       "      <td>0.921465</td>\n",
       "      <td>0.924221</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.916518</td>\n",
       "      <td>0.919459</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.932235</td>\n",
       "      <td>0.950493</td>\n",
       "      <td>0.949004</td>\n",
       "      <td>0.918223</td>\n",
       "      <td>0.923615</td>\n",
       "      <td>0.935901</td>\n",
       "      <td>0.929679</td>\n",
       "      <td>0.987994</td>\n",
       "      <td>0.983083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978996</td>\n",
       "      <td>0.982510</td>\n",
       "      <td>0.923812</td>\n",
       "      <td>0.745941</td>\n",
       "      <td>0.912601</td>\n",
       "      <td>0.912601</td>\n",
       "      <td>0.939565</td>\n",
       "      <td>0.911240</td>\n",
       "      <td>0.910508</td>\n",
       "      <td>0.908305</td>\n",
       "      <td>0.910069</td>\n",
       "      <td>0.851777</td>\n",
       "      <td>0.704198</td>\n",
       "      <td>0.849669</td>\n",
       "      <td>0.851503</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.833880</td>\n",
       "      <td>0.870432</td>\n",
       "      <td>0.850466</td>\n",
       "      <td>0.874324</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.821330</td>\n",
       "      <td>0.843655</td>\n",
       "      <td>0.815432</td>\n",
       "      <td>0.883037</td>\n",
       "      <td>0.768238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_meta_xgb</th>\n",
       "      <td>0.940697</td>\n",
       "      <td>0.925493</td>\n",
       "      <td>0.931048</td>\n",
       "      <td>0.928518</td>\n",
       "      <td>0.916744</td>\n",
       "      <td>0.919339</td>\n",
       "      <td>0.919893</td>\n",
       "      <td>0.911642</td>\n",
       "      <td>0.914348</td>\n",
       "      <td>0.917717</td>\n",
       "      <td>0.934714</td>\n",
       "      <td>0.945683</td>\n",
       "      <td>0.944127</td>\n",
       "      <td>0.917351</td>\n",
       "      <td>0.922866</td>\n",
       "      <td>0.938310</td>\n",
       "      <td>0.933917</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.978996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989161</td>\n",
       "      <td>0.928109</td>\n",
       "      <td>0.745525</td>\n",
       "      <td>0.919371</td>\n",
       "      <td>0.919371</td>\n",
       "      <td>0.940027</td>\n",
       "      <td>0.907005</td>\n",
       "      <td>0.906356</td>\n",
       "      <td>0.904119</td>\n",
       "      <td>0.905936</td>\n",
       "      <td>0.848667</td>\n",
       "      <td>0.701815</td>\n",
       "      <td>0.846402</td>\n",
       "      <td>0.848316</td>\n",
       "      <td>0.887415</td>\n",
       "      <td>0.830441</td>\n",
       "      <td>0.866846</td>\n",
       "      <td>0.846931</td>\n",
       "      <td>0.870755</td>\n",
       "      <td>0.830167</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.839933</td>\n",
       "      <td>0.811707</td>\n",
       "      <td>0.889163</td>\n",
       "      <td>0.768521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>img_meta_nima_xgb</th>\n",
       "      <td>0.943550</td>\n",
       "      <td>0.924579</td>\n",
       "      <td>0.933453</td>\n",
       "      <td>0.930603</td>\n",
       "      <td>0.916079</td>\n",
       "      <td>0.918438</td>\n",
       "      <td>0.919009</td>\n",
       "      <td>0.910830</td>\n",
       "      <td>0.913160</td>\n",
       "      <td>0.916878</td>\n",
       "      <td>0.933102</td>\n",
       "      <td>0.945216</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>0.918931</td>\n",
       "      <td>0.924344</td>\n",
       "      <td>0.940572</td>\n",
       "      <td>0.936607</td>\n",
       "      <td>0.977972</td>\n",
       "      <td>0.973208</td>\n",
       "      <td>0.982510</td>\n",
       "      <td>0.989161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930752</td>\n",
       "      <td>0.746903</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>0.906398</td>\n",
       "      <td>0.905686</td>\n",
       "      <td>0.903453</td>\n",
       "      <td>0.905387</td>\n",
       "      <td>0.849234</td>\n",
       "      <td>0.702643</td>\n",
       "      <td>0.847204</td>\n",
       "      <td>0.849195</td>\n",
       "      <td>0.887269</td>\n",
       "      <td>0.830877</td>\n",
       "      <td>0.867503</td>\n",
       "      <td>0.847454</td>\n",
       "      <td>0.871452</td>\n",
       "      <td>0.830699</td>\n",
       "      <td>0.818032</td>\n",
       "      <td>0.840524</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.892597</td>\n",
       "      <td>0.769703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline_xgb</th>\n",
       "      <td>0.977573</td>\n",
       "      <td>0.955797</td>\n",
       "      <td>0.964709</td>\n",
       "      <td>0.954971</td>\n",
       "      <td>0.893908</td>\n",
       "      <td>0.895836</td>\n",
       "      <td>0.895842</td>\n",
       "      <td>0.887875</td>\n",
       "      <td>0.890089</td>\n",
       "      <td>0.894925</td>\n",
       "      <td>0.906859</td>\n",
       "      <td>0.904365</td>\n",
       "      <td>0.905456</td>\n",
       "      <td>0.893515</td>\n",
       "      <td>0.899271</td>\n",
       "      <td>0.970684</td>\n",
       "      <td>0.972942</td>\n",
       "      <td>0.919868</td>\n",
       "      <td>0.915654</td>\n",
       "      <td>0.923812</td>\n",
       "      <td>0.928109</td>\n",
       "      <td>0.930752</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.885101</td>\n",
       "      <td>0.885101</td>\n",
       "      <td>0.964860</td>\n",
       "      <td>0.884244</td>\n",
       "      <td>0.883541</td>\n",
       "      <td>0.880995</td>\n",
       "      <td>0.883780</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.680675</td>\n",
       "      <td>0.815321</td>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.857438</td>\n",
       "      <td>0.805885</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.821631</td>\n",
       "      <td>0.843861</td>\n",
       "      <td>0.805410</td>\n",
       "      <td>0.793460</td>\n",
       "      <td>0.815044</td>\n",
       "      <td>0.787995</td>\n",
       "      <td>0.931141</td>\n",
       "      <td>0.780367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ranking_xgb</th>\n",
       "      <td>0.769518</td>\n",
       "      <td>0.765113</td>\n",
       "      <td>0.768467</td>\n",
       "      <td>0.758656</td>\n",
       "      <td>0.738648</td>\n",
       "      <td>0.737372</td>\n",
       "      <td>0.737337</td>\n",
       "      <td>0.737129</td>\n",
       "      <td>0.734428</td>\n",
       "      <td>0.737832</td>\n",
       "      <td>0.736728</td>\n",
       "      <td>0.741206</td>\n",
       "      <td>0.739742</td>\n",
       "      <td>0.735165</td>\n",
       "      <td>0.734971</td>\n",
       "      <td>0.771464</td>\n",
       "      <td>0.766890</td>\n",
       "      <td>0.742374</td>\n",
       "      <td>0.740242</td>\n",
       "      <td>0.745941</td>\n",
       "      <td>0.745525</td>\n",
       "      <td>0.746903</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>0.774139</td>\n",
       "      <td>0.723515</td>\n",
       "      <td>0.721852</td>\n",
       "      <td>0.722929</td>\n",
       "      <td>0.717453</td>\n",
       "      <td>0.689818</td>\n",
       "      <td>0.628618</td>\n",
       "      <td>0.696715</td>\n",
       "      <td>0.703082</td>\n",
       "      <td>0.709817</td>\n",
       "      <td>0.697815</td>\n",
       "      <td>0.731386</td>\n",
       "      <td>0.711790</td>\n",
       "      <td>0.736516</td>\n",
       "      <td>0.602618</td>\n",
       "      <td>0.591424</td>\n",
       "      <td>0.612974</td>\n",
       "      <td>0.586523</td>\n",
       "      <td>0.643513</td>\n",
       "      <td>0.880291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost</th>\n",
       "      <td>0.896485</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.888887</td>\n",
       "      <td>0.902593</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.902602</td>\n",
       "      <td>0.892868</td>\n",
       "      <td>0.902092</td>\n",
       "      <td>0.902028</td>\n",
       "      <td>0.919560</td>\n",
       "      <td>0.904336</td>\n",
       "      <td>0.898912</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.891646</td>\n",
       "      <td>0.911971</td>\n",
       "      <td>0.908011</td>\n",
       "      <td>0.912601</td>\n",
       "      <td>0.919371</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.885101</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911169</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.887385</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.872626</td>\n",
       "      <td>0.811322</td>\n",
       "      <td>0.729226</td>\n",
       "      <td>0.794808</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.846260</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.803459</td>\n",
       "      <td>0.839353</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>0.757783</td>\n",
       "      <td>0.797388</td>\n",
       "      <td>0.749147</td>\n",
       "      <td>0.837270</td>\n",
       "      <td>0.760855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost1_without_text</th>\n",
       "      <td>0.896485</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.894999</td>\n",
       "      <td>0.888887</td>\n",
       "      <td>0.902593</td>\n",
       "      <td>0.903455</td>\n",
       "      <td>0.902602</td>\n",
       "      <td>0.892868</td>\n",
       "      <td>0.902092</td>\n",
       "      <td>0.902028</td>\n",
       "      <td>0.919560</td>\n",
       "      <td>0.904336</td>\n",
       "      <td>0.898912</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.907917</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.891646</td>\n",
       "      <td>0.911971</td>\n",
       "      <td>0.908011</td>\n",
       "      <td>0.912601</td>\n",
       "      <td>0.919371</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.885101</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911169</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.887385</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.872626</td>\n",
       "      <td>0.811322</td>\n",
       "      <td>0.729226</td>\n",
       "      <td>0.794808</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.846260</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.803459</td>\n",
       "      <td>0.839353</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>0.757783</td>\n",
       "      <td>0.797388</td>\n",
       "      <td>0.749147</td>\n",
       "      <td>0.837270</td>\n",
       "      <td>0.760855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcl_cgb</th>\n",
       "      <td>0.976032</td>\n",
       "      <td>0.969725</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>0.961636</td>\n",
       "      <td>0.918213</td>\n",
       "      <td>0.918551</td>\n",
       "      <td>0.918047</td>\n",
       "      <td>0.910381</td>\n",
       "      <td>0.913036</td>\n",
       "      <td>0.918151</td>\n",
       "      <td>0.925841</td>\n",
       "      <td>0.928662</td>\n",
       "      <td>0.928638</td>\n",
       "      <td>0.911059</td>\n",
       "      <td>0.916608</td>\n",
       "      <td>0.975858</td>\n",
       "      <td>0.972201</td>\n",
       "      <td>0.935648</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.939565</td>\n",
       "      <td>0.940027</td>\n",
       "      <td>0.942517</td>\n",
       "      <td>0.964860</td>\n",
       "      <td>0.774139</td>\n",
       "      <td>0.911169</td>\n",
       "      <td>0.911169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906421</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.903111</td>\n",
       "      <td>0.906813</td>\n",
       "      <td>0.848970</td>\n",
       "      <td>0.710759</td>\n",
       "      <td>0.848899</td>\n",
       "      <td>0.851632</td>\n",
       "      <td>0.884223</td>\n",
       "      <td>0.836093</td>\n",
       "      <td>0.872388</td>\n",
       "      <td>0.852646</td>\n",
       "      <td>0.876225</td>\n",
       "      <td>0.833304</td>\n",
       "      <td>0.820399</td>\n",
       "      <td>0.843665</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.928724</td>\n",
       "      <td>0.791433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_bigru_cv1d_rnn</th>\n",
       "      <td>0.895204</td>\n",
       "      <td>0.897465</td>\n",
       "      <td>0.891990</td>\n",
       "      <td>0.901173</td>\n",
       "      <td>0.919504</td>\n",
       "      <td>0.922844</td>\n",
       "      <td>0.921836</td>\n",
       "      <td>0.914385</td>\n",
       "      <td>0.916719</td>\n",
       "      <td>0.921847</td>\n",
       "      <td>0.919778</td>\n",
       "      <td>0.917589</td>\n",
       "      <td>0.916229</td>\n",
       "      <td>0.913667</td>\n",
       "      <td>0.914844</td>\n",
       "      <td>0.896763</td>\n",
       "      <td>0.890068</td>\n",
       "      <td>0.911734</td>\n",
       "      <td>0.907881</td>\n",
       "      <td>0.911240</td>\n",
       "      <td>0.907005</td>\n",
       "      <td>0.906398</td>\n",
       "      <td>0.884244</td>\n",
       "      <td>0.723515</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.906421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970911</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.953796</td>\n",
       "      <td>0.849023</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.834455</td>\n",
       "      <td>0.838957</td>\n",
       "      <td>0.897684</td>\n",
       "      <td>0.824145</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>0.840836</td>\n",
       "      <td>0.870583</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>0.815955</td>\n",
       "      <td>0.845514</td>\n",
       "      <td>0.808175</td>\n",
       "      <td>0.842373</td>\n",
       "      <td>0.748887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_bigru_attention_rnn</th>\n",
       "      <td>0.894450</td>\n",
       "      <td>0.896696</td>\n",
       "      <td>0.891134</td>\n",
       "      <td>0.900185</td>\n",
       "      <td>0.918586</td>\n",
       "      <td>0.921886</td>\n",
       "      <td>0.920802</td>\n",
       "      <td>0.913422</td>\n",
       "      <td>0.915921</td>\n",
       "      <td>0.921070</td>\n",
       "      <td>0.918981</td>\n",
       "      <td>0.916410</td>\n",
       "      <td>0.914980</td>\n",
       "      <td>0.912747</td>\n",
       "      <td>0.914060</td>\n",
       "      <td>0.896112</td>\n",
       "      <td>0.889364</td>\n",
       "      <td>0.911048</td>\n",
       "      <td>0.907101</td>\n",
       "      <td>0.910508</td>\n",
       "      <td>0.906356</td>\n",
       "      <td>0.905686</td>\n",
       "      <td>0.883541</td>\n",
       "      <td>0.721852</td>\n",
       "      <td>0.887385</td>\n",
       "      <td>0.887385</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.970911</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966881</td>\n",
       "      <td>0.952309</td>\n",
       "      <td>0.848229</td>\n",
       "      <td>0.715935</td>\n",
       "      <td>0.834031</td>\n",
       "      <td>0.838447</td>\n",
       "      <td>0.897399</td>\n",
       "      <td>0.823576</td>\n",
       "      <td>0.863698</td>\n",
       "      <td>0.840236</td>\n",
       "      <td>0.869998</td>\n",
       "      <td>0.831351</td>\n",
       "      <td>0.815478</td>\n",
       "      <td>0.845075</td>\n",
       "      <td>0.807718</td>\n",
       "      <td>0.842070</td>\n",
       "      <td>0.747406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pretrained_2gru_rnn</th>\n",
       "      <td>0.891942</td>\n",
       "      <td>0.894225</td>\n",
       "      <td>0.888897</td>\n",
       "      <td>0.897477</td>\n",
       "      <td>0.916392</td>\n",
       "      <td>0.919655</td>\n",
       "      <td>0.918679</td>\n",
       "      <td>0.911270</td>\n",
       "      <td>0.913904</td>\n",
       "      <td>0.918656</td>\n",
       "      <td>0.916932</td>\n",
       "      <td>0.914216</td>\n",
       "      <td>0.912850</td>\n",
       "      <td>0.911341</td>\n",
       "      <td>0.912147</td>\n",
       "      <td>0.893587</td>\n",
       "      <td>0.886849</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.904934</td>\n",
       "      <td>0.908305</td>\n",
       "      <td>0.904119</td>\n",
       "      <td>0.903453</td>\n",
       "      <td>0.880995</td>\n",
       "      <td>0.722929</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.885382</td>\n",
       "      <td>0.903111</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.966881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.949535</td>\n",
       "      <td>0.845857</td>\n",
       "      <td>0.712184</td>\n",
       "      <td>0.831046</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.894728</td>\n",
       "      <td>0.820945</td>\n",
       "      <td>0.860862</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.867146</td>\n",
       "      <td>0.828514</td>\n",
       "      <td>0.812761</td>\n",
       "      <td>0.842175</td>\n",
       "      <td>0.805075</td>\n",
       "      <td>0.838464</td>\n",
       "      <td>0.748642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selftrained_bigru_conv1d_rnn</th>\n",
       "      <td>0.894094</td>\n",
       "      <td>0.897245</td>\n",
       "      <td>0.891522</td>\n",
       "      <td>0.899517</td>\n",
       "      <td>0.914868</td>\n",
       "      <td>0.917843</td>\n",
       "      <td>0.916663</td>\n",
       "      <td>0.909777</td>\n",
       "      <td>0.910945</td>\n",
       "      <td>0.917476</td>\n",
       "      <td>0.912860</td>\n",
       "      <td>0.915931</td>\n",
       "      <td>0.915627</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.907708</td>\n",
       "      <td>0.896312</td>\n",
       "      <td>0.889476</td>\n",
       "      <td>0.910415</td>\n",
       "      <td>0.906435</td>\n",
       "      <td>0.910069</td>\n",
       "      <td>0.905936</td>\n",
       "      <td>0.905387</td>\n",
       "      <td>0.883780</td>\n",
       "      <td>0.717453</td>\n",
       "      <td>0.872626</td>\n",
       "      <td>0.872626</td>\n",
       "      <td>0.906813</td>\n",
       "      <td>0.953796</td>\n",
       "      <td>0.952309</td>\n",
       "      <td>0.949535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859459</td>\n",
       "      <td>0.708776</td>\n",
       "      <td>0.847166</td>\n",
       "      <td>0.850304</td>\n",
       "      <td>0.906238</td>\n",
       "      <td>0.833640</td>\n",
       "      <td>0.872537</td>\n",
       "      <td>0.850473</td>\n",
       "      <td>0.877578</td>\n",
       "      <td>0.843599</td>\n",
       "      <td>0.829074</td>\n",
       "      <td>0.854190</td>\n",
       "      <td>0.821526</td>\n",
       "      <td>0.842529</td>\n",
       "      <td>0.743855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_lgb</th>\n",
       "      <td>0.823135</td>\n",
       "      <td>0.835321</td>\n",
       "      <td>0.828427</td>\n",
       "      <td>0.827092</td>\n",
       "      <td>0.869182</td>\n",
       "      <td>0.866855</td>\n",
       "      <td>0.865516</td>\n",
       "      <td>0.871939</td>\n",
       "      <td>0.862606</td>\n",
       "      <td>0.869846</td>\n",
       "      <td>0.852152</td>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.868076</td>\n",
       "      <td>0.823182</td>\n",
       "      <td>0.841794</td>\n",
       "      <td>0.834262</td>\n",
       "      <td>0.823106</td>\n",
       "      <td>0.850251</td>\n",
       "      <td>0.847049</td>\n",
       "      <td>0.851777</td>\n",
       "      <td>0.848667</td>\n",
       "      <td>0.849234</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.689818</td>\n",
       "      <td>0.811322</td>\n",
       "      <td>0.811322</td>\n",
       "      <td>0.848970</td>\n",
       "      <td>0.849023</td>\n",
       "      <td>0.848229</td>\n",
       "      <td>0.845857</td>\n",
       "      <td>0.859459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788162</td>\n",
       "      <td>0.940994</td>\n",
       "      <td>0.947320</td>\n",
       "      <td>0.886745</td>\n",
       "      <td>0.872291</td>\n",
       "      <td>0.914040</td>\n",
       "      <td>0.891084</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.859582</td>\n",
       "      <td>0.843389</td>\n",
       "      <td>0.866778</td>\n",
       "      <td>0.834438</td>\n",
       "      <td>0.773886</td>\n",
       "      <td>0.712138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_cwb_rg</th>\n",
       "      <td>0.685161</td>\n",
       "      <td>0.695687</td>\n",
       "      <td>0.690939</td>\n",
       "      <td>0.686990</td>\n",
       "      <td>0.716797</td>\n",
       "      <td>0.710733</td>\n",
       "      <td>0.709698</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.710778</td>\n",
       "      <td>0.712374</td>\n",
       "      <td>0.701711</td>\n",
       "      <td>0.715492</td>\n",
       "      <td>0.710091</td>\n",
       "      <td>0.705327</td>\n",
       "      <td>0.695711</td>\n",
       "      <td>0.699198</td>\n",
       "      <td>0.687793</td>\n",
       "      <td>0.702433</td>\n",
       "      <td>0.699813</td>\n",
       "      <td>0.704198</td>\n",
       "      <td>0.701815</td>\n",
       "      <td>0.702643</td>\n",
       "      <td>0.680675</td>\n",
       "      <td>0.628618</td>\n",
       "      <td>0.729226</td>\n",
       "      <td>0.729226</td>\n",
       "      <td>0.710759</td>\n",
       "      <td>0.714940</td>\n",
       "      <td>0.715935</td>\n",
       "      <td>0.712184</td>\n",
       "      <td>0.708776</td>\n",
       "      <td>0.788162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.768838</td>\n",
       "      <td>0.789577</td>\n",
       "      <td>0.716294</td>\n",
       "      <td>0.717921</td>\n",
       "      <td>0.766546</td>\n",
       "      <td>0.734738</td>\n",
       "      <td>0.778527</td>\n",
       "      <td>0.672195</td>\n",
       "      <td>0.651171</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>0.642389</td>\n",
       "      <td>0.617066</td>\n",
       "      <td>0.661407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_fm</th>\n",
       "      <td>0.821971</td>\n",
       "      <td>0.834781</td>\n",
       "      <td>0.827950</td>\n",
       "      <td>0.827854</td>\n",
       "      <td>0.849560</td>\n",
       "      <td>0.846129</td>\n",
       "      <td>0.844977</td>\n",
       "      <td>0.851349</td>\n",
       "      <td>0.841915</td>\n",
       "      <td>0.849202</td>\n",
       "      <td>0.832332</td>\n",
       "      <td>0.868791</td>\n",
       "      <td>0.867281</td>\n",
       "      <td>0.804784</td>\n",
       "      <td>0.821912</td>\n",
       "      <td>0.834251</td>\n",
       "      <td>0.822702</td>\n",
       "      <td>0.847868</td>\n",
       "      <td>0.844865</td>\n",
       "      <td>0.849669</td>\n",
       "      <td>0.846402</td>\n",
       "      <td>0.847204</td>\n",
       "      <td>0.815321</td>\n",
       "      <td>0.696715</td>\n",
       "      <td>0.794808</td>\n",
       "      <td>0.794808</td>\n",
       "      <td>0.848899</td>\n",
       "      <td>0.834455</td>\n",
       "      <td>0.834031</td>\n",
       "      <td>0.831046</td>\n",
       "      <td>0.847166</td>\n",
       "      <td>0.940994</td>\n",
       "      <td>0.768838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992530</td>\n",
       "      <td>0.895938</td>\n",
       "      <td>0.923028</td>\n",
       "      <td>0.952428</td>\n",
       "      <td>0.941819</td>\n",
       "      <td>0.949769</td>\n",
       "      <td>0.868349</td>\n",
       "      <td>0.865318</td>\n",
       "      <td>0.875062</td>\n",
       "      <td>0.863841</td>\n",
       "      <td>0.769777</td>\n",
       "      <td>0.713722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_rg</th>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.836749</td>\n",
       "      <td>0.830028</td>\n",
       "      <td>0.829751</td>\n",
       "      <td>0.853084</td>\n",
       "      <td>0.848887</td>\n",
       "      <td>0.847646</td>\n",
       "      <td>0.854074</td>\n",
       "      <td>0.845153</td>\n",
       "      <td>0.851942</td>\n",
       "      <td>0.835086</td>\n",
       "      <td>0.870874</td>\n",
       "      <td>0.868427</td>\n",
       "      <td>0.809867</td>\n",
       "      <td>0.824401</td>\n",
       "      <td>0.836672</td>\n",
       "      <td>0.824834</td>\n",
       "      <td>0.849526</td>\n",
       "      <td>0.846549</td>\n",
       "      <td>0.851503</td>\n",
       "      <td>0.848316</td>\n",
       "      <td>0.849195</td>\n",
       "      <td>0.817191</td>\n",
       "      <td>0.703082</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.803529</td>\n",
       "      <td>0.851632</td>\n",
       "      <td>0.838957</td>\n",
       "      <td>0.838447</td>\n",
       "      <td>0.835543</td>\n",
       "      <td>0.850304</td>\n",
       "      <td>0.947320</td>\n",
       "      <td>0.789577</td>\n",
       "      <td>0.992530</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890810</td>\n",
       "      <td>0.918645</td>\n",
       "      <td>0.956040</td>\n",
       "      <td>0.938341</td>\n",
       "      <td>0.956755</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.863262</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>0.859124</td>\n",
       "      <td>0.770324</td>\n",
       "      <td>0.722905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>0.866779</td>\n",
       "      <td>0.874204</td>\n",
       "      <td>0.867792</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.893489</td>\n",
       "      <td>0.896498</td>\n",
       "      <td>0.896866</td>\n",
       "      <td>0.891481</td>\n",
       "      <td>0.891943</td>\n",
       "      <td>0.897334</td>\n",
       "      <td>0.887590</td>\n",
       "      <td>0.900485</td>\n",
       "      <td>0.900056</td>\n",
       "      <td>0.866792</td>\n",
       "      <td>0.877851</td>\n",
       "      <td>0.870983</td>\n",
       "      <td>0.863948</td>\n",
       "      <td>0.890831</td>\n",
       "      <td>0.887099</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.887415</td>\n",
       "      <td>0.887269</td>\n",
       "      <td>0.857438</td>\n",
       "      <td>0.709817</td>\n",
       "      <td>0.846260</td>\n",
       "      <td>0.846260</td>\n",
       "      <td>0.884223</td>\n",
       "      <td>0.897684</td>\n",
       "      <td>0.897399</td>\n",
       "      <td>0.894728</td>\n",
       "      <td>0.906238</td>\n",
       "      <td>0.886745</td>\n",
       "      <td>0.716294</td>\n",
       "      <td>0.895938</td>\n",
       "      <td>0.890810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.879176</td>\n",
       "      <td>0.914453</td>\n",
       "      <td>0.897046</td>\n",
       "      <td>0.916009</td>\n",
       "      <td>0.885489</td>\n",
       "      <td>0.878420</td>\n",
       "      <td>0.892664</td>\n",
       "      <td>0.874183</td>\n",
       "      <td>0.815301</td>\n",
       "      <td>0.728305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_0001</th>\n",
       "      <td>0.813002</td>\n",
       "      <td>0.824736</td>\n",
       "      <td>0.818223</td>\n",
       "      <td>0.821121</td>\n",
       "      <td>0.841548</td>\n",
       "      <td>0.846315</td>\n",
       "      <td>0.846989</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.835859</td>\n",
       "      <td>0.848833</td>\n",
       "      <td>0.825604</td>\n",
       "      <td>0.853381</td>\n",
       "      <td>0.852450</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.813741</td>\n",
       "      <td>0.822557</td>\n",
       "      <td>0.812784</td>\n",
       "      <td>0.832539</td>\n",
       "      <td>0.829478</td>\n",
       "      <td>0.833880</td>\n",
       "      <td>0.830441</td>\n",
       "      <td>0.830877</td>\n",
       "      <td>0.805885</td>\n",
       "      <td>0.697815</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.836093</td>\n",
       "      <td>0.824145</td>\n",
       "      <td>0.823576</td>\n",
       "      <td>0.820945</td>\n",
       "      <td>0.833640</td>\n",
       "      <td>0.872291</td>\n",
       "      <td>0.717921</td>\n",
       "      <td>0.923028</td>\n",
       "      <td>0.918645</td>\n",
       "      <td>0.879176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967830</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.957903</td>\n",
       "      <td>0.871821</td>\n",
       "      <td>0.879716</td>\n",
       "      <td>0.874106</td>\n",
       "      <td>0.883194</td>\n",
       "      <td>0.758149</td>\n",
       "      <td>0.705350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_160</th>\n",
       "      <td>0.847543</td>\n",
       "      <td>0.860072</td>\n",
       "      <td>0.853142</td>\n",
       "      <td>0.856480</td>\n",
       "      <td>0.880072</td>\n",
       "      <td>0.878901</td>\n",
       "      <td>0.879539</td>\n",
       "      <td>0.877224</td>\n",
       "      <td>0.873883</td>\n",
       "      <td>0.881341</td>\n",
       "      <td>0.863308</td>\n",
       "      <td>0.891558</td>\n",
       "      <td>0.889281</td>\n",
       "      <td>0.839233</td>\n",
       "      <td>0.851746</td>\n",
       "      <td>0.858009</td>\n",
       "      <td>0.847676</td>\n",
       "      <td>0.868728</td>\n",
       "      <td>0.865605</td>\n",
       "      <td>0.870432</td>\n",
       "      <td>0.866846</td>\n",
       "      <td>0.867503</td>\n",
       "      <td>0.840286</td>\n",
       "      <td>0.731386</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.830252</td>\n",
       "      <td>0.872388</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>0.863698</td>\n",
       "      <td>0.860862</td>\n",
       "      <td>0.872537</td>\n",
       "      <td>0.914040</td>\n",
       "      <td>0.766546</td>\n",
       "      <td>0.952428</td>\n",
       "      <td>0.956040</td>\n",
       "      <td>0.914453</td>\n",
       "      <td>0.967830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.987128</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.908484</td>\n",
       "      <td>0.903209</td>\n",
       "      <td>0.915742</td>\n",
       "      <td>0.900263</td>\n",
       "      <td>0.788835</td>\n",
       "      <td>0.740193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_10</th>\n",
       "      <td>0.828779</td>\n",
       "      <td>0.840884</td>\n",
       "      <td>0.834198</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.858496</td>\n",
       "      <td>0.860962</td>\n",
       "      <td>0.861631</td>\n",
       "      <td>0.858859</td>\n",
       "      <td>0.852646</td>\n",
       "      <td>0.863476</td>\n",
       "      <td>0.842154</td>\n",
       "      <td>0.870849</td>\n",
       "      <td>0.869725</td>\n",
       "      <td>0.814377</td>\n",
       "      <td>0.830083</td>\n",
       "      <td>0.838689</td>\n",
       "      <td>0.828704</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>0.845877</td>\n",
       "      <td>0.850466</td>\n",
       "      <td>0.846931</td>\n",
       "      <td>0.847454</td>\n",
       "      <td>0.821631</td>\n",
       "      <td>0.711790</td>\n",
       "      <td>0.803459</td>\n",
       "      <td>0.803459</td>\n",
       "      <td>0.852646</td>\n",
       "      <td>0.840836</td>\n",
       "      <td>0.840236</td>\n",
       "      <td>0.837553</td>\n",
       "      <td>0.850473</td>\n",
       "      <td>0.891084</td>\n",
       "      <td>0.734738</td>\n",
       "      <td>0.941819</td>\n",
       "      <td>0.938341</td>\n",
       "      <td>0.897046</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>0.987128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978444</td>\n",
       "      <td>0.890701</td>\n",
       "      <td>0.896439</td>\n",
       "      <td>0.893548</td>\n",
       "      <td>0.898729</td>\n",
       "      <td>0.772767</td>\n",
       "      <td>0.719560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha_320</th>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.863767</td>\n",
       "      <td>0.856787</td>\n",
       "      <td>0.859861</td>\n",
       "      <td>0.884830</td>\n",
       "      <td>0.882902</td>\n",
       "      <td>0.883506</td>\n",
       "      <td>0.881255</td>\n",
       "      <td>0.878525</td>\n",
       "      <td>0.885262</td>\n",
       "      <td>0.867993</td>\n",
       "      <td>0.895356</td>\n",
       "      <td>0.892455</td>\n",
       "      <td>0.846704</td>\n",
       "      <td>0.856936</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.851360</td>\n",
       "      <td>0.872565</td>\n",
       "      <td>0.869439</td>\n",
       "      <td>0.874324</td>\n",
       "      <td>0.870755</td>\n",
       "      <td>0.871452</td>\n",
       "      <td>0.843861</td>\n",
       "      <td>0.736516</td>\n",
       "      <td>0.839353</td>\n",
       "      <td>0.839353</td>\n",
       "      <td>0.876225</td>\n",
       "      <td>0.870583</td>\n",
       "      <td>0.869998</td>\n",
       "      <td>0.867146</td>\n",
       "      <td>0.877578</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.778527</td>\n",
       "      <td>0.949769</td>\n",
       "      <td>0.956755</td>\n",
       "      <td>0.916009</td>\n",
       "      <td>0.957903</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.978444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909061</td>\n",
       "      <td>0.898929</td>\n",
       "      <td>0.919034</td>\n",
       "      <td>0.893928</td>\n",
       "      <td>0.791272</td>\n",
       "      <td>0.746015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_l1_05</th>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.823825</td>\n",
       "      <td>0.816884</td>\n",
       "      <td>0.821350</td>\n",
       "      <td>0.838050</td>\n",
       "      <td>0.840290</td>\n",
       "      <td>0.841827</td>\n",
       "      <td>0.834829</td>\n",
       "      <td>0.832751</td>\n",
       "      <td>0.841630</td>\n",
       "      <td>0.826684</td>\n",
       "      <td>0.851669</td>\n",
       "      <td>0.851668</td>\n",
       "      <td>0.799327</td>\n",
       "      <td>0.814837</td>\n",
       "      <td>0.819196</td>\n",
       "      <td>0.812960</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.830167</td>\n",
       "      <td>0.830699</td>\n",
       "      <td>0.805410</td>\n",
       "      <td>0.602618</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>0.778251</td>\n",
       "      <td>0.833304</td>\n",
       "      <td>0.831889</td>\n",
       "      <td>0.831351</td>\n",
       "      <td>0.828514</td>\n",
       "      <td>0.843599</td>\n",
       "      <td>0.859582</td>\n",
       "      <td>0.672195</td>\n",
       "      <td>0.868349</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.885489</td>\n",
       "      <td>0.871821</td>\n",
       "      <td>0.908484</td>\n",
       "      <td>0.890701</td>\n",
       "      <td>0.909061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989319</td>\n",
       "      <td>0.990881</td>\n",
       "      <td>0.977235</td>\n",
       "      <td>0.819547</td>\n",
       "      <td>0.591173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_l1_1</th>\n",
       "      <td>0.800982</td>\n",
       "      <td>0.811323</td>\n",
       "      <td>0.804517</td>\n",
       "      <td>0.810223</td>\n",
       "      <td>0.822719</td>\n",
       "      <td>0.826367</td>\n",
       "      <td>0.827788</td>\n",
       "      <td>0.820828</td>\n",
       "      <td>0.817674</td>\n",
       "      <td>0.827670</td>\n",
       "      <td>0.811957</td>\n",
       "      <td>0.839089</td>\n",
       "      <td>0.839892</td>\n",
       "      <td>0.781676</td>\n",
       "      <td>0.799672</td>\n",
       "      <td>0.806717</td>\n",
       "      <td>0.800763</td>\n",
       "      <td>0.820057</td>\n",
       "      <td>0.817291</td>\n",
       "      <td>0.821330</td>\n",
       "      <td>0.817575</td>\n",
       "      <td>0.818032</td>\n",
       "      <td>0.793460</td>\n",
       "      <td>0.591424</td>\n",
       "      <td>0.757783</td>\n",
       "      <td>0.757783</td>\n",
       "      <td>0.820399</td>\n",
       "      <td>0.815955</td>\n",
       "      <td>0.815478</td>\n",
       "      <td>0.812761</td>\n",
       "      <td>0.829074</td>\n",
       "      <td>0.843389</td>\n",
       "      <td>0.651171</td>\n",
       "      <td>0.865318</td>\n",
       "      <td>0.863262</td>\n",
       "      <td>0.878420</td>\n",
       "      <td>0.879716</td>\n",
       "      <td>0.903209</td>\n",
       "      <td>0.896439</td>\n",
       "      <td>0.898929</td>\n",
       "      <td>0.989319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978720</td>\n",
       "      <td>0.993113</td>\n",
       "      <td>0.808260</td>\n",
       "      <td>0.579615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_l2_01</th>\n",
       "      <td>0.822777</td>\n",
       "      <td>0.833793</td>\n",
       "      <td>0.826794</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.847758</td>\n",
       "      <td>0.848981</td>\n",
       "      <td>0.850257</td>\n",
       "      <td>0.843080</td>\n",
       "      <td>0.841905</td>\n",
       "      <td>0.850187</td>\n",
       "      <td>0.836491</td>\n",
       "      <td>0.861994</td>\n",
       "      <td>0.860305</td>\n",
       "      <td>0.815745</td>\n",
       "      <td>0.826084</td>\n",
       "      <td>0.829294</td>\n",
       "      <td>0.822877</td>\n",
       "      <td>0.842153</td>\n",
       "      <td>0.839354</td>\n",
       "      <td>0.843655</td>\n",
       "      <td>0.839933</td>\n",
       "      <td>0.840524</td>\n",
       "      <td>0.815044</td>\n",
       "      <td>0.612974</td>\n",
       "      <td>0.797388</td>\n",
       "      <td>0.797388</td>\n",
       "      <td>0.843665</td>\n",
       "      <td>0.845514</td>\n",
       "      <td>0.845075</td>\n",
       "      <td>0.842175</td>\n",
       "      <td>0.854190</td>\n",
       "      <td>0.866778</td>\n",
       "      <td>0.692946</td>\n",
       "      <td>0.875062</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>0.892664</td>\n",
       "      <td>0.874106</td>\n",
       "      <td>0.915742</td>\n",
       "      <td>0.893548</td>\n",
       "      <td>0.919034</td>\n",
       "      <td>0.990881</td>\n",
       "      <td>0.978720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972749</td>\n",
       "      <td>0.827777</td>\n",
       "      <td>0.602285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_l2_1</th>\n",
       "      <td>0.795435</td>\n",
       "      <td>0.805571</td>\n",
       "      <td>0.798866</td>\n",
       "      <td>0.804675</td>\n",
       "      <td>0.814407</td>\n",
       "      <td>0.818613</td>\n",
       "      <td>0.819961</td>\n",
       "      <td>0.813033</td>\n",
       "      <td>0.809139</td>\n",
       "      <td>0.819786</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>0.832904</td>\n",
       "      <td>0.833779</td>\n",
       "      <td>0.773605</td>\n",
       "      <td>0.791992</td>\n",
       "      <td>0.801049</td>\n",
       "      <td>0.795230</td>\n",
       "      <td>0.814219</td>\n",
       "      <td>0.811488</td>\n",
       "      <td>0.815432</td>\n",
       "      <td>0.811707</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.787995</td>\n",
       "      <td>0.586523</td>\n",
       "      <td>0.749147</td>\n",
       "      <td>0.749147</td>\n",
       "      <td>0.814558</td>\n",
       "      <td>0.808175</td>\n",
       "      <td>0.807718</td>\n",
       "      <td>0.805075</td>\n",
       "      <td>0.821526</td>\n",
       "      <td>0.834438</td>\n",
       "      <td>0.642389</td>\n",
       "      <td>0.863841</td>\n",
       "      <td>0.859124</td>\n",
       "      <td>0.874183</td>\n",
       "      <td>0.883194</td>\n",
       "      <td>0.900263</td>\n",
       "      <td>0.898729</td>\n",
       "      <td>0.893928</td>\n",
       "      <td>0.977235</td>\n",
       "      <td>0.993113</td>\n",
       "      <td>0.972749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.802999</td>\n",
       "      <td>0.574573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cls05_lgb</th>\n",
       "      <td>0.945198</td>\n",
       "      <td>0.919283</td>\n",
       "      <td>0.931634</td>\n",
       "      <td>0.919523</td>\n",
       "      <td>0.849272</td>\n",
       "      <td>0.851784</td>\n",
       "      <td>0.852166</td>\n",
       "      <td>0.842887</td>\n",
       "      <td>0.845405</td>\n",
       "      <td>0.850715</td>\n",
       "      <td>0.865770</td>\n",
       "      <td>0.862263</td>\n",
       "      <td>0.864107</td>\n",
       "      <td>0.849946</td>\n",
       "      <td>0.857522</td>\n",
       "      <td>0.932960</td>\n",
       "      <td>0.939893</td>\n",
       "      <td>0.878056</td>\n",
       "      <td>0.874278</td>\n",
       "      <td>0.883037</td>\n",
       "      <td>0.889163</td>\n",
       "      <td>0.892597</td>\n",
       "      <td>0.931141</td>\n",
       "      <td>0.643513</td>\n",
       "      <td>0.837270</td>\n",
       "      <td>0.837270</td>\n",
       "      <td>0.928724</td>\n",
       "      <td>0.842373</td>\n",
       "      <td>0.842070</td>\n",
       "      <td>0.838464</td>\n",
       "      <td>0.842529</td>\n",
       "      <td>0.773886</td>\n",
       "      <td>0.617066</td>\n",
       "      <td>0.769777</td>\n",
       "      <td>0.770324</td>\n",
       "      <td>0.815301</td>\n",
       "      <td>0.758149</td>\n",
       "      <td>0.788835</td>\n",
       "      <td>0.772767</td>\n",
       "      <td>0.791272</td>\n",
       "      <td>0.819547</td>\n",
       "      <td>0.808260</td>\n",
       "      <td>0.827777</td>\n",
       "      <td>0.802999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.634385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cls0_lgb</th>\n",
       "      <td>0.788349</td>\n",
       "      <td>0.781393</td>\n",
       "      <td>0.782935</td>\n",
       "      <td>0.779242</td>\n",
       "      <td>0.754601</td>\n",
       "      <td>0.752769</td>\n",
       "      <td>0.752736</td>\n",
       "      <td>0.752974</td>\n",
       "      <td>0.749832</td>\n",
       "      <td>0.753346</td>\n",
       "      <td>0.752856</td>\n",
       "      <td>0.758297</td>\n",
       "      <td>0.756379</td>\n",
       "      <td>0.750064</td>\n",
       "      <td>0.748902</td>\n",
       "      <td>0.794906</td>\n",
       "      <td>0.786246</td>\n",
       "      <td>0.766178</td>\n",
       "      <td>0.762278</td>\n",
       "      <td>0.768238</td>\n",
       "      <td>0.768521</td>\n",
       "      <td>0.769703</td>\n",
       "      <td>0.780367</td>\n",
       "      <td>0.880291</td>\n",
       "      <td>0.760855</td>\n",
       "      <td>0.760855</td>\n",
       "      <td>0.791433</td>\n",
       "      <td>0.748887</td>\n",
       "      <td>0.747406</td>\n",
       "      <td>0.748642</td>\n",
       "      <td>0.743855</td>\n",
       "      <td>0.712138</td>\n",
       "      <td>0.661407</td>\n",
       "      <td>0.713722</td>\n",
       "      <td>0.722905</td>\n",
       "      <td>0.728305</td>\n",
       "      <td>0.705350</td>\n",
       "      <td>0.740193</td>\n",
       "      <td>0.719560</td>\n",
       "      <td>0.746015</td>\n",
       "      <td>0.591173</td>\n",
       "      <td>0.579615</td>\n",
       "      <td>0.602285</td>\n",
       "      <td>0.574573</td>\n",
       "      <td>0.634385</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lgb411_tune  plants_lgb  \\\n",
       "lgb411_tune                                     1.000000    0.967159   \n",
       "plants_lgb                                      0.967159    1.000000   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb     0.976990    0.968862   \n",
       "xentropy_small_lr_cat_lgb                       0.967743    0.951730   \n",
       "simple_feature_lgb                              0.905456    0.911633   \n",
       "all_mean_enc_lgb                                0.907395    0.912456   \n",
       "all_mean_enc_user_feat_lgb                      0.907480    0.912490   \n",
       "all_mean_enc_user_feat2_lgb                     0.899326    0.904745   \n",
       "cat_interact_lgb                                0.902366    0.907654   \n",
       "mean_enc_lgb                                    0.906453    0.912110   \n",
       "marcus_lgb                                      0.919712    0.914469   \n",
       "fused_text_lgb                                  0.914794    0.921208   \n",
       "mixed_features_text_proprocessing_lgb           0.915861    0.921965   \n",
       "select_dense_features_lgb                       0.906655    0.896150   \n",
       "select_sparse_features_lgb                      0.912361    0.902178   \n",
       "lgb411_dart_tune                                0.982080    0.966236   \n",
       "poisson_lgb                                     0.985545    0.963319   \n",
       "small_features_v5_xgb                           0.931473    0.931010   \n",
       "small_features_v4_xgb                           0.927126    0.926905   \n",
       "nima_features_xgb                               0.935678    0.930084   \n",
       "img_meta_xgb                                    0.940697    0.925493   \n",
       "img_meta_nima_xgb                               0.943550    0.924579   \n",
       "baseline_xgb                                    0.977573    0.955797   \n",
       "ranking_xgb                                     0.769518    0.765113   \n",
       "catboost                                        0.896485    0.890247   \n",
       "catboost1_without_text                          0.896485    0.890247   \n",
       "mcl_cgb                                         0.976032    0.969725   \n",
       "pretrained_bigru_cv1d_rnn                       0.895204    0.897465   \n",
       "pretrained_bigru_attention_rnn                  0.894450    0.896696   \n",
       "pretrained_2gru_rnn                             0.891942    0.894225   \n",
       "selftrained_bigru_conv1d_rnn                    0.894094    0.897245   \n",
       "text_lgb                                        0.823135    0.835321   \n",
       "text_cwb_rg                                     0.685161    0.695687   \n",
       "text_fm                                         0.821971    0.834781   \n",
       "text_rg                                         0.823708    0.836749   \n",
       "mlp                                             0.866779    0.874204   \n",
       "alpha_0001                                      0.813002    0.824736   \n",
       "alpha_160                                       0.847543    0.860072   \n",
       "alpha_10                                        0.828779    0.840884   \n",
       "alpha_320                                       0.851161    0.863767   \n",
       "lr_l1_05                                        0.813043    0.823825   \n",
       "lr_l1_1                                         0.800982    0.811323   \n",
       "lr_l2_01                                        0.822777    0.833793   \n",
       "lr_l2_1                                         0.795435    0.805571   \n",
       "cls05_lgb                                       0.945198    0.919283   \n",
       "cls0_lgb                                        0.788349    0.781393   \n",
       "\n",
       "                                             plants_with_img_meta_nima_fm_geo_active_lgb  \\\n",
       "lgb411_tune                                                                     0.976990   \n",
       "plants_lgb                                                                      0.968862   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                                     1.000000   \n",
       "xentropy_small_lr_cat_lgb                                                       0.957623   \n",
       "simple_feature_lgb                                                              0.904752   \n",
       "all_mean_enc_lgb                                                                0.906086   \n",
       "all_mean_enc_user_feat_lgb                                                      0.906017   \n",
       "all_mean_enc_user_feat2_lgb                                                     0.898062   \n",
       "cat_interact_lgb                                                                0.900886   \n",
       "mean_enc_lgb                                                                    0.905318   \n",
       "marcus_lgb                                                                      0.915583   \n",
       "fused_text_lgb                                                                  0.914908   \n",
       "mixed_features_text_proprocessing_lgb                                           0.915626   \n",
       "select_dense_features_lgb                                                       0.901881   \n",
       "select_sparse_features_lgb                                                      0.907734   \n",
       "lgb411_dart_tune                                                                0.972245   \n",
       "poisson_lgb                                                                     0.972814   \n",
       "small_features_v5_xgb                                                           0.924954   \n",
       "small_features_v4_xgb                                                           0.920816   \n",
       "nima_features_xgb                                                               0.928658   \n",
       "img_meta_xgb                                                                    0.931048   \n",
       "img_meta_nima_xgb                                                               0.933453   \n",
       "baseline_xgb                                                                    0.964709   \n",
       "ranking_xgb                                                                     0.768467   \n",
       "catboost                                                                        0.894999   \n",
       "catboost1_without_text                                                          0.894999   \n",
       "mcl_cgb                                                                         0.976016   \n",
       "pretrained_bigru_cv1d_rnn                                                       0.891990   \n",
       "pretrained_bigru_attention_rnn                                                  0.891134   \n",
       "pretrained_2gru_rnn                                                             0.888897   \n",
       "selftrained_bigru_conv1d_rnn                                                    0.891522   \n",
       "text_lgb                                                                        0.828427   \n",
       "text_cwb_rg                                                                     0.690939   \n",
       "text_fm                                                                         0.827950   \n",
       "text_rg                                                                         0.830028   \n",
       "mlp                                                                             0.867792   \n",
       "alpha_0001                                                                      0.818223   \n",
       "alpha_160                                                                       0.853142   \n",
       "alpha_10                                                                        0.834198   \n",
       "alpha_320                                                                       0.856787   \n",
       "lr_l1_05                                                                        0.816884   \n",
       "lr_l1_1                                                                         0.804517   \n",
       "lr_l2_01                                                                        0.826794   \n",
       "lr_l2_1                                                                         0.798866   \n",
       "cls05_lgb                                                                       0.931634   \n",
       "cls0_lgb                                                                        0.782935   \n",
       "\n",
       "                                             xentropy_small_lr_cat_lgb  \\\n",
       "lgb411_tune                                                   0.967743   \n",
       "plants_lgb                                                    0.951730   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                   0.957623   \n",
       "xentropy_small_lr_cat_lgb                                     1.000000   \n",
       "simple_feature_lgb                                            0.914189   \n",
       "all_mean_enc_lgb                                              0.917104   \n",
       "all_mean_enc_user_feat_lgb                                    0.916845   \n",
       "all_mean_enc_user_feat2_lgb                                   0.907965   \n",
       "cat_interact_lgb                                              0.911571   \n",
       "mean_enc_lgb                                                  0.916972   \n",
       "marcus_lgb                                                    0.932491   \n",
       "fused_text_lgb                                                0.925647   \n",
       "mixed_features_text_proprocessing_lgb                         0.925489   \n",
       "select_dense_features_lgb                                     0.919620   \n",
       "select_sparse_features_lgb                                    0.924598   \n",
       "lgb411_dart_tune                                              0.963631   \n",
       "poisson_lgb                                                   0.964006   \n",
       "small_features_v5_xgb                                         0.923585   \n",
       "small_features_v4_xgb                                         0.919470   \n",
       "nima_features_xgb                                             0.926771   \n",
       "img_meta_xgb                                                  0.928518   \n",
       "img_meta_nima_xgb                                             0.930603   \n",
       "baseline_xgb                                                  0.954971   \n",
       "ranking_xgb                                                   0.758656   \n",
       "catboost                                                      0.888887   \n",
       "catboost1_without_text                                        0.888887   \n",
       "mcl_cgb                                                       0.961636   \n",
       "pretrained_bigru_cv1d_rnn                                     0.901173   \n",
       "pretrained_bigru_attention_rnn                                0.900185   \n",
       "pretrained_2gru_rnn                                           0.897477   \n",
       "selftrained_bigru_conv1d_rnn                                  0.899517   \n",
       "text_lgb                                                      0.827092   \n",
       "text_cwb_rg                                                   0.686990   \n",
       "text_fm                                                       0.827854   \n",
       "text_rg                                                       0.829751   \n",
       "mlp                                                           0.872978   \n",
       "alpha_0001                                                    0.821121   \n",
       "alpha_160                                                     0.856480   \n",
       "alpha_10                                                      0.837466   \n",
       "alpha_320                                                     0.859861   \n",
       "lr_l1_05                                                      0.821350   \n",
       "lr_l1_1                                                       0.810223   \n",
       "lr_l2_01                                                      0.830989   \n",
       "lr_l2_1                                                       0.804675   \n",
       "cls05_lgb                                                     0.919523   \n",
       "cls0_lgb                                                      0.779242   \n",
       "\n",
       "                                             simple_feature_lgb  \\\n",
       "lgb411_tune                                            0.905456   \n",
       "plants_lgb                                             0.911633   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb            0.904752   \n",
       "xentropy_small_lr_cat_lgb                              0.914189   \n",
       "simple_feature_lgb                                     1.000000   \n",
       "all_mean_enc_lgb                                       0.968871   \n",
       "all_mean_enc_user_feat_lgb                             0.967921   \n",
       "all_mean_enc_user_feat2_lgb                            0.957815   \n",
       "cat_interact_lgb                                       0.966615   \n",
       "mean_enc_lgb                                           0.971213   \n",
       "marcus_lgb                                             0.956146   \n",
       "fused_text_lgb                                         0.936530   \n",
       "mixed_features_text_proprocessing_lgb                  0.939173   \n",
       "select_dense_features_lgb                              0.933709   \n",
       "select_sparse_features_lgb                             0.945776   \n",
       "lgb411_dart_tune                                       0.908589   \n",
       "poisson_lgb                                            0.900190   \n",
       "small_features_v5_xgb                                  0.922071   \n",
       "small_features_v4_xgb                                  0.918064   \n",
       "nima_features_xgb                                      0.921465   \n",
       "img_meta_xgb                                           0.916744   \n",
       "img_meta_nima_xgb                                      0.916079   \n",
       "baseline_xgb                                           0.893908   \n",
       "ranking_xgb                                            0.738648   \n",
       "catboost                                               0.902593   \n",
       "catboost1_without_text                                 0.902593   \n",
       "mcl_cgb                                                0.918213   \n",
       "pretrained_bigru_cv1d_rnn                              0.919504   \n",
       "pretrained_bigru_attention_rnn                         0.918586   \n",
       "pretrained_2gru_rnn                                    0.916392   \n",
       "selftrained_bigru_conv1d_rnn                           0.914868   \n",
       "text_lgb                                               0.869182   \n",
       "text_cwb_rg                                            0.716797   \n",
       "text_fm                                                0.849560   \n",
       "text_rg                                                0.853084   \n",
       "mlp                                                    0.893489   \n",
       "alpha_0001                                             0.841548   \n",
       "alpha_160                                              0.880072   \n",
       "alpha_10                                               0.858496   \n",
       "alpha_320                                              0.884830   \n",
       "lr_l1_05                                               0.838050   \n",
       "lr_l1_1                                                0.822719   \n",
       "lr_l2_01                                               0.847758   \n",
       "lr_l2_1                                                0.814407   \n",
       "cls05_lgb                                              0.849272   \n",
       "cls0_lgb                                               0.754601   \n",
       "\n",
       "                                             all_mean_enc_lgb  \\\n",
       "lgb411_tune                                          0.907395   \n",
       "plants_lgb                                           0.912456   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb          0.906086   \n",
       "xentropy_small_lr_cat_lgb                            0.917104   \n",
       "simple_feature_lgb                                   0.968871   \n",
       "all_mean_enc_lgb                                     1.000000   \n",
       "all_mean_enc_user_feat_lgb                           0.993119   \n",
       "all_mean_enc_user_feat2_lgb                          0.979055   \n",
       "cat_interact_lgb                                     0.973705   \n",
       "mean_enc_lgb                                         0.989717   \n",
       "marcus_lgb                                           0.963624   \n",
       "fused_text_lgb                                       0.939821   \n",
       "mixed_features_text_proprocessing_lgb                0.943974   \n",
       "select_dense_features_lgb                            0.939846   \n",
       "select_sparse_features_lgb                           0.953933   \n",
       "lgb411_dart_tune                                     0.909640   \n",
       "poisson_lgb                                          0.901865   \n",
       "small_features_v5_xgb                                0.925226   \n",
       "small_features_v4_xgb                                0.921064   \n",
       "nima_features_xgb                                    0.924221   \n",
       "img_meta_xgb                                         0.919339   \n",
       "img_meta_nima_xgb                                    0.918438   \n",
       "baseline_xgb                                         0.895836   \n",
       "ranking_xgb                                          0.737372   \n",
       "catboost                                             0.903455   \n",
       "catboost1_without_text                               0.903455   \n",
       "mcl_cgb                                              0.918551   \n",
       "pretrained_bigru_cv1d_rnn                            0.922844   \n",
       "pretrained_bigru_attention_rnn                       0.921886   \n",
       "pretrained_2gru_rnn                                  0.919655   \n",
       "selftrained_bigru_conv1d_rnn                         0.917843   \n",
       "text_lgb                                             0.866855   \n",
       "text_cwb_rg                                          0.710733   \n",
       "text_fm                                              0.846129   \n",
       "text_rg                                              0.848887   \n",
       "mlp                                                  0.896498   \n",
       "alpha_0001                                           0.846315   \n",
       "alpha_160                                            0.878901   \n",
       "alpha_10                                             0.860962   \n",
       "alpha_320                                            0.882902   \n",
       "lr_l1_05                                             0.840290   \n",
       "lr_l1_1                                              0.826367   \n",
       "lr_l2_01                                             0.848981   \n",
       "lr_l2_1                                              0.818613   \n",
       "cls05_lgb                                            0.851784   \n",
       "cls0_lgb                                             0.752769   \n",
       "\n",
       "                                             all_mean_enc_user_feat_lgb  \\\n",
       "lgb411_tune                                                    0.907480   \n",
       "plants_lgb                                                     0.912490   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                    0.906017   \n",
       "xentropy_small_lr_cat_lgb                                      0.916845   \n",
       "simple_feature_lgb                                             0.967921   \n",
       "all_mean_enc_lgb                                               0.993119   \n",
       "all_mean_enc_user_feat_lgb                                     1.000000   \n",
       "all_mean_enc_user_feat2_lgb                                    0.980549   \n",
       "cat_interact_lgb                                               0.972853   \n",
       "mean_enc_lgb                                                   0.988447   \n",
       "marcus_lgb                                                     0.962874   \n",
       "fused_text_lgb                                                 0.940541   \n",
       "mixed_features_text_proprocessing_lgb                          0.944687   \n",
       "select_dense_features_lgb                                      0.939407   \n",
       "select_sparse_features_lgb                                     0.953526   \n",
       "lgb411_dart_tune                                               0.909536   \n",
       "poisson_lgb                                                    0.901898   \n",
       "small_features_v5_xgb                                          0.925803   \n",
       "small_features_v4_xgb                                          0.921642   \n",
       "nima_features_xgb                                              0.924791   \n",
       "img_meta_xgb                                                   0.919893   \n",
       "img_meta_nima_xgb                                              0.919009   \n",
       "baseline_xgb                                                   0.895842   \n",
       "ranking_xgb                                                    0.737337   \n",
       "catboost                                                       0.902602   \n",
       "catboost1_without_text                                         0.902602   \n",
       "mcl_cgb                                                        0.918047   \n",
       "pretrained_bigru_cv1d_rnn                                      0.921836   \n",
       "pretrained_bigru_attention_rnn                                 0.920802   \n",
       "pretrained_2gru_rnn                                            0.918679   \n",
       "selftrained_bigru_conv1d_rnn                                   0.916663   \n",
       "text_lgb                                                       0.865516   \n",
       "text_cwb_rg                                                    0.709698   \n",
       "text_fm                                                        0.844977   \n",
       "text_rg                                                        0.847646   \n",
       "mlp                                                            0.896866   \n",
       "alpha_0001                                                     0.846989   \n",
       "alpha_160                                                      0.879539   \n",
       "alpha_10                                                       0.861631   \n",
       "alpha_320                                                      0.883506   \n",
       "lr_l1_05                                                       0.841827   \n",
       "lr_l1_1                                                        0.827788   \n",
       "lr_l2_01                                                       0.850257   \n",
       "lr_l2_1                                                        0.819961   \n",
       "cls05_lgb                                                      0.852166   \n",
       "cls0_lgb                                                       0.752736   \n",
       "\n",
       "                                             all_mean_enc_user_feat2_lgb  \\\n",
       "lgb411_tune                                                     0.899326   \n",
       "plants_lgb                                                      0.904745   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                     0.898062   \n",
       "xentropy_small_lr_cat_lgb                                       0.907965   \n",
       "simple_feature_lgb                                              0.957815   \n",
       "all_mean_enc_lgb                                                0.979055   \n",
       "all_mean_enc_user_feat_lgb                                      0.980549   \n",
       "all_mean_enc_user_feat2_lgb                                     1.000000   \n",
       "cat_interact_lgb                                                0.962162   \n",
       "mean_enc_lgb                                                    0.974986   \n",
       "marcus_lgb                                                      0.951025   \n",
       "fused_text_lgb                                                  0.932428   \n",
       "mixed_features_text_proprocessing_lgb                           0.936438   \n",
       "select_dense_features_lgb                                       0.928221   \n",
       "select_sparse_features_lgb                                      0.942182   \n",
       "lgb411_dart_tune                                                0.902085   \n",
       "poisson_lgb                                                     0.893906   \n",
       "small_features_v5_xgb                                           0.917543   \n",
       "small_features_v4_xgb                                           0.913503   \n",
       "nima_features_xgb                                               0.916518   \n",
       "img_meta_xgb                                                    0.911642   \n",
       "img_meta_nima_xgb                                               0.910830   \n",
       "baseline_xgb                                                    0.887875   \n",
       "ranking_xgb                                                     0.737129   \n",
       "catboost                                                        0.892868   \n",
       "catboost1_without_text                                          0.892868   \n",
       "mcl_cgb                                                         0.910381   \n",
       "pretrained_bigru_cv1d_rnn                                       0.914385   \n",
       "pretrained_bigru_attention_rnn                                  0.913422   \n",
       "pretrained_2gru_rnn                                             0.911270   \n",
       "selftrained_bigru_conv1d_rnn                                    0.909777   \n",
       "text_lgb                                                        0.871939   \n",
       "text_cwb_rg                                                     0.715363   \n",
       "text_fm                                                         0.851349   \n",
       "text_rg                                                         0.854074   \n",
       "mlp                                                             0.891481   \n",
       "alpha_0001                                                      0.843902   \n",
       "alpha_160                                                       0.877224   \n",
       "alpha_10                                                        0.858859   \n",
       "alpha_320                                                       0.881255   \n",
       "lr_l1_05                                                        0.834829   \n",
       "lr_l1_1                                                         0.820828   \n",
       "lr_l2_01                                                        0.843080   \n",
       "lr_l2_1                                                         0.813033   \n",
       "cls05_lgb                                                       0.842887   \n",
       "cls0_lgb                                                        0.752974   \n",
       "\n",
       "                                             cat_interact_lgb  mean_enc_lgb  \\\n",
       "lgb411_tune                                          0.902366      0.906453   \n",
       "plants_lgb                                           0.907654      0.912110   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb          0.900886      0.905318   \n",
       "xentropy_small_lr_cat_lgb                            0.911571      0.916972   \n",
       "simple_feature_lgb                                   0.966615      0.971213   \n",
       "all_mean_enc_lgb                                     0.973705      0.989717   \n",
       "all_mean_enc_user_feat_lgb                           0.972853      0.988447   \n",
       "all_mean_enc_user_feat2_lgb                          0.962162      0.974986   \n",
       "cat_interact_lgb                                     1.000000      0.976066   \n",
       "mean_enc_lgb                                         0.976066      1.000000   \n",
       "marcus_lgb                                           0.960817      0.962645   \n",
       "fused_text_lgb                                       0.933094      0.938839   \n",
       "mixed_features_text_proprocessing_lgb                0.936761      0.943108   \n",
       "select_dense_features_lgb                            0.937580      0.937892   \n",
       "select_sparse_features_lgb                           0.950516      0.952404   \n",
       "lgb411_dart_tune                                     0.903797      0.909121   \n",
       "poisson_lgb                                          0.895902      0.900995   \n",
       "small_features_v5_xgb                                0.920951      0.923394   \n",
       "small_features_v4_xgb                                0.916776      0.919352   \n",
       "nima_features_xgb                                    0.919459      0.922500   \n",
       "img_meta_xgb                                         0.914348      0.917717   \n",
       "img_meta_nima_xgb                                    0.913160      0.916878   \n",
       "baseline_xgb                                         0.890089      0.894925   \n",
       "ranking_xgb                                          0.734428      0.737832   \n",
       "catboost                                             0.902092      0.902028   \n",
       "catboost1_without_text                               0.902092      0.902028   \n",
       "mcl_cgb                                              0.913036      0.918151   \n",
       "pretrained_bigru_cv1d_rnn                            0.916719      0.921847   \n",
       "pretrained_bigru_attention_rnn                       0.915921      0.921070   \n",
       "pretrained_2gru_rnn                                  0.913904      0.918656   \n",
       "selftrained_bigru_conv1d_rnn                         0.910945      0.917476   \n",
       "text_lgb                                             0.862606      0.869846   \n",
       "text_cwb_rg                                          0.710778      0.712374   \n",
       "text_fm                                              0.841915      0.849202   \n",
       "text_rg                                              0.845153      0.851942   \n",
       "mlp                                                  0.891943      0.897334   \n",
       "alpha_0001                                           0.835859      0.848833   \n",
       "alpha_160                                            0.873883      0.881341   \n",
       "alpha_10                                             0.852646      0.863476   \n",
       "alpha_320                                            0.878525      0.885262   \n",
       "lr_l1_05                                             0.832751      0.841630   \n",
       "lr_l1_1                                              0.817674      0.827670   \n",
       "lr_l2_01                                             0.841905      0.850187   \n",
       "lr_l2_1                                              0.809139      0.819786   \n",
       "cls05_lgb                                            0.845405      0.850715   \n",
       "cls0_lgb                                             0.749832      0.753346   \n",
       "\n",
       "                                             marcus_lgb  fused_text_lgb  \\\n",
       "lgb411_tune                                    0.919712        0.914794   \n",
       "plants_lgb                                     0.914469        0.921208   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb    0.915583        0.914908   \n",
       "xentropy_small_lr_cat_lgb                      0.932491        0.925647   \n",
       "simple_feature_lgb                             0.956146        0.936530   \n",
       "all_mean_enc_lgb                               0.963624        0.939821   \n",
       "all_mean_enc_user_feat_lgb                     0.962874        0.940541   \n",
       "all_mean_enc_user_feat2_lgb                    0.951025        0.932428   \n",
       "cat_interact_lgb                               0.960817        0.933094   \n",
       "mean_enc_lgb                                   0.962645        0.938839   \n",
       "marcus_lgb                                     1.000000        0.942398   \n",
       "fused_text_lgb                                 0.942398        1.000000   \n",
       "mixed_features_text_proprocessing_lgb          0.944862        0.977439   \n",
       "select_dense_features_lgb                      0.960118        0.925521   \n",
       "select_sparse_features_lgb                     0.975061        0.930711   \n",
       "lgb411_dart_tune                               0.918962        0.918812   \n",
       "poisson_lgb                                    0.912005        0.911017   \n",
       "small_features_v5_xgb                          0.933476        0.950737   \n",
       "small_features_v4_xgb                          0.929105        0.946809   \n",
       "nima_features_xgb                              0.932235        0.950493   \n",
       "img_meta_xgb                                   0.934714        0.945683   \n",
       "img_meta_nima_xgb                              0.933102        0.945216   \n",
       "baseline_xgb                                   0.906859        0.904365   \n",
       "ranking_xgb                                    0.736728        0.741206   \n",
       "catboost                                       0.919560        0.904336   \n",
       "catboost1_without_text                         0.919560        0.904336   \n",
       "mcl_cgb                                        0.925841        0.928662   \n",
       "pretrained_bigru_cv1d_rnn                      0.919778        0.917589   \n",
       "pretrained_bigru_attention_rnn                 0.918981        0.916410   \n",
       "pretrained_2gru_rnn                            0.916932        0.914216   \n",
       "selftrained_bigru_conv1d_rnn                   0.912860        0.915931   \n",
       "text_lgb                                       0.852152        0.866779   \n",
       "text_cwb_rg                                    0.701711        0.715492   \n",
       "text_fm                                        0.832332        0.868791   \n",
       "text_rg                                        0.835086        0.870874   \n",
       "mlp                                            0.887590        0.900485   \n",
       "alpha_0001                                     0.825604        0.853381   \n",
       "alpha_160                                      0.863308        0.891558   \n",
       "alpha_10                                       0.842154        0.870849   \n",
       "alpha_320                                      0.867993        0.895356   \n",
       "lr_l1_05                                       0.826684        0.851669   \n",
       "lr_l1_1                                        0.811957        0.839089   \n",
       "lr_l2_01                                       0.836491        0.861994   \n",
       "lr_l2_1                                        0.804208        0.832904   \n",
       "cls05_lgb                                      0.865770        0.862263   \n",
       "cls0_lgb                                       0.752856        0.758297   \n",
       "\n",
       "                                             mixed_features_text_proprocessing_lgb  \\\n",
       "lgb411_tune                                                               0.915861   \n",
       "plants_lgb                                                                0.921965   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                               0.915626   \n",
       "xentropy_small_lr_cat_lgb                                                 0.925489   \n",
       "simple_feature_lgb                                                        0.939173   \n",
       "all_mean_enc_lgb                                                          0.943974   \n",
       "all_mean_enc_user_feat_lgb                                                0.944687   \n",
       "all_mean_enc_user_feat2_lgb                                               0.936438   \n",
       "cat_interact_lgb                                                          0.936761   \n",
       "mean_enc_lgb                                                              0.943108   \n",
       "marcus_lgb                                                                0.944862   \n",
       "fused_text_lgb                                                            0.977439   \n",
       "mixed_features_text_proprocessing_lgb                                     1.000000   \n",
       "select_dense_features_lgb                                                 0.921109   \n",
       "select_sparse_features_lgb                                                0.932118   \n",
       "lgb411_dart_tune                                                          0.919663   \n",
       "poisson_lgb                                                               0.911857   \n",
       "small_features_v5_xgb                                                     0.949450   \n",
       "small_features_v4_xgb                                                     0.945420   \n",
       "nima_features_xgb                                                         0.949004   \n",
       "img_meta_xgb                                                              0.944127   \n",
       "img_meta_nima_xgb                                                         0.943598   \n",
       "baseline_xgb                                                              0.905456   \n",
       "ranking_xgb                                                               0.739742   \n",
       "catboost                                                                  0.898912   \n",
       "catboost1_without_text                                                    0.898912   \n",
       "mcl_cgb                                                                   0.928638   \n",
       "pretrained_bigru_cv1d_rnn                                                 0.916229   \n",
       "pretrained_bigru_attention_rnn                                            0.914980   \n",
       "pretrained_2gru_rnn                                                       0.912850   \n",
       "selftrained_bigru_conv1d_rnn                                              0.915627   \n",
       "text_lgb                                                                  0.868076   \n",
       "text_cwb_rg                                                               0.710091   \n",
       "text_fm                                                                   0.867281   \n",
       "text_rg                                                                   0.868427   \n",
       "mlp                                                                       0.900056   \n",
       "alpha_0001                                                                0.852450   \n",
       "alpha_160                                                                 0.889281   \n",
       "alpha_10                                                                  0.869725   \n",
       "alpha_320                                                                 0.892455   \n",
       "lr_l1_05                                                                  0.851668   \n",
       "lr_l1_1                                                                   0.839892   \n",
       "lr_l2_01                                                                  0.860305   \n",
       "lr_l2_1                                                                   0.833779   \n",
       "cls05_lgb                                                                 0.864107   \n",
       "cls0_lgb                                                                  0.756379   \n",
       "\n",
       "                                             select_dense_features_lgb  \\\n",
       "lgb411_tune                                                   0.906655   \n",
       "plants_lgb                                                    0.896150   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                   0.901881   \n",
       "xentropy_small_lr_cat_lgb                                     0.919620   \n",
       "simple_feature_lgb                                            0.933709   \n",
       "all_mean_enc_lgb                                              0.939846   \n",
       "all_mean_enc_user_feat_lgb                                    0.939407   \n",
       "all_mean_enc_user_feat2_lgb                                   0.928221   \n",
       "cat_interact_lgb                                              0.937580   \n",
       "mean_enc_lgb                                                  0.937892   \n",
       "marcus_lgb                                                    0.960118   \n",
       "fused_text_lgb                                                0.925521   \n",
       "mixed_features_text_proprocessing_lgb                         0.921109   \n",
       "select_dense_features_lgb                                     1.000000   \n",
       "select_sparse_features_lgb                                    0.970594   \n",
       "lgb411_dart_tune                                              0.905117   \n",
       "poisson_lgb                                                   0.898740   \n",
       "small_features_v5_xgb                                         0.915873   \n",
       "small_features_v4_xgb                                         0.911518   \n",
       "nima_features_xgb                                             0.918223   \n",
       "img_meta_xgb                                                  0.917351   \n",
       "img_meta_nima_xgb                                             0.918931   \n",
       "baseline_xgb                                                  0.893515   \n",
       "ranking_xgb                                                   0.735165   \n",
       "catboost                                                      0.918677   \n",
       "catboost1_without_text                                        0.918677   \n",
       "mcl_cgb                                                       0.911059   \n",
       "pretrained_bigru_cv1d_rnn                                     0.913667   \n",
       "pretrained_bigru_attention_rnn                                0.912747   \n",
       "pretrained_2gru_rnn                                           0.911341   \n",
       "selftrained_bigru_conv1d_rnn                                  0.900438   \n",
       "text_lgb                                                      0.823182   \n",
       "text_cwb_rg                                                   0.705327   \n",
       "text_fm                                                       0.804784   \n",
       "text_rg                                                       0.809867   \n",
       "mlp                                                           0.866792   \n",
       "alpha_0001                                                    0.797980   \n",
       "alpha_160                                                     0.839233   \n",
       "alpha_10                                                      0.814377   \n",
       "alpha_320                                                     0.846704   \n",
       "lr_l1_05                                                      0.799327   \n",
       "lr_l1_1                                                       0.781676   \n",
       "lr_l2_01                                                      0.815745   \n",
       "lr_l2_1                                                       0.773605   \n",
       "cls05_lgb                                                     0.849946   \n",
       "cls0_lgb                                                      0.750064   \n",
       "\n",
       "                                             select_sparse_features_lgb  \\\n",
       "lgb411_tune                                                    0.912361   \n",
       "plants_lgb                                                     0.902178   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                    0.907734   \n",
       "xentropy_small_lr_cat_lgb                                      0.924598   \n",
       "simple_feature_lgb                                             0.945776   \n",
       "all_mean_enc_lgb                                               0.953933   \n",
       "all_mean_enc_user_feat_lgb                                     0.953526   \n",
       "all_mean_enc_user_feat2_lgb                                    0.942182   \n",
       "cat_interact_lgb                                               0.950516   \n",
       "mean_enc_lgb                                                   0.952404   \n",
       "marcus_lgb                                                     0.975061   \n",
       "fused_text_lgb                                                 0.930711   \n",
       "mixed_features_text_proprocessing_lgb                          0.932118   \n",
       "select_dense_features_lgb                                      0.970594   \n",
       "select_sparse_features_lgb                                     1.000000   \n",
       "lgb411_dart_tune                                               0.910978   \n",
       "poisson_lgb                                                    0.904226   \n",
       "small_features_v5_xgb                                          0.921414   \n",
       "small_features_v4_xgb                                          0.917017   \n",
       "nima_features_xgb                                              0.923615   \n",
       "img_meta_xgb                                                   0.922866   \n",
       "img_meta_nima_xgb                                              0.924344   \n",
       "baseline_xgb                                                   0.899271   \n",
       "ranking_xgb                                                    0.734971   \n",
       "catboost                                                       0.907917   \n",
       "catboost1_without_text                                         0.907917   \n",
       "mcl_cgb                                                        0.916608   \n",
       "pretrained_bigru_cv1d_rnn                                      0.914844   \n",
       "pretrained_bigru_attention_rnn                                 0.914060   \n",
       "pretrained_2gru_rnn                                            0.912147   \n",
       "selftrained_bigru_conv1d_rnn                                   0.907708   \n",
       "text_lgb                                                       0.841794   \n",
       "text_cwb_rg                                                    0.695711   \n",
       "text_fm                                                        0.821912   \n",
       "text_rg                                                        0.824401   \n",
       "mlp                                                            0.877851   \n",
       "alpha_0001                                                     0.813741   \n",
       "alpha_160                                                      0.851746   \n",
       "alpha_10                                                       0.830083   \n",
       "alpha_320                                                      0.856936   \n",
       "lr_l1_05                                                       0.814837   \n",
       "lr_l1_1                                                        0.799672   \n",
       "lr_l2_01                                                       0.826084   \n",
       "lr_l2_1                                                        0.791992   \n",
       "cls05_lgb                                                      0.857522   \n",
       "cls0_lgb                                                       0.748902   \n",
       "\n",
       "                                             lgb411_dart_tune  poisson_lgb  \\\n",
       "lgb411_tune                                          0.982080     0.985545   \n",
       "plants_lgb                                           0.966236     0.963319   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb          0.972245     0.972814   \n",
       "xentropy_small_lr_cat_lgb                            0.963631     0.964006   \n",
       "simple_feature_lgb                                   0.908589     0.900190   \n",
       "all_mean_enc_lgb                                     0.909640     0.901865   \n",
       "all_mean_enc_user_feat_lgb                           0.909536     0.901898   \n",
       "all_mean_enc_user_feat2_lgb                          0.902085     0.893906   \n",
       "cat_interact_lgb                                     0.903797     0.895902   \n",
       "mean_enc_lgb                                         0.909121     0.900995   \n",
       "marcus_lgb                                           0.918962     0.912005   \n",
       "fused_text_lgb                                       0.918812     0.911017   \n",
       "mixed_features_text_proprocessing_lgb                0.919663     0.911857   \n",
       "select_dense_features_lgb                            0.905117     0.898740   \n",
       "select_sparse_features_lgb                           0.910978     0.904226   \n",
       "lgb411_dart_tune                                     1.000000     0.976874   \n",
       "poisson_lgb                                          0.976874     1.000000   \n",
       "small_features_v5_xgb                                0.932515     0.925590   \n",
       "small_features_v4_xgb                                0.928168     0.921467   \n",
       "nima_features_xgb                                    0.935901     0.929679   \n",
       "img_meta_xgb                                         0.938310     0.933917   \n",
       "img_meta_nima_xgb                                    0.940572     0.936607   \n",
       "baseline_xgb                                         0.970684     0.972942   \n",
       "ranking_xgb                                          0.771464     0.766890   \n",
       "catboost                                             0.897617     0.891646   \n",
       "catboost1_without_text                               0.897617     0.891646   \n",
       "mcl_cgb                                              0.975858     0.972201   \n",
       "pretrained_bigru_cv1d_rnn                            0.896763     0.890068   \n",
       "pretrained_bigru_attention_rnn                       0.896112     0.889364   \n",
       "pretrained_2gru_rnn                                  0.893587     0.886849   \n",
       "selftrained_bigru_conv1d_rnn                         0.896312     0.889476   \n",
       "text_lgb                                             0.834262     0.823106   \n",
       "text_cwb_rg                                          0.699198     0.687793   \n",
       "text_fm                                              0.834251     0.822702   \n",
       "text_rg                                              0.836672     0.824834   \n",
       "mlp                                                  0.870983     0.863948   \n",
       "alpha_0001                                           0.822557     0.812784   \n",
       "alpha_160                                            0.858009     0.847676   \n",
       "alpha_10                                             0.838689     0.828704   \n",
       "alpha_320                                            0.861811     0.851360   \n",
       "lr_l1_05                                             0.819196     0.812960   \n",
       "lr_l1_1                                              0.806717     0.800763   \n",
       "lr_l2_01                                             0.829294     0.822877   \n",
       "lr_l2_1                                              0.801049     0.795230   \n",
       "cls05_lgb                                            0.932960     0.939893   \n",
       "cls0_lgb                                             0.794906     0.786246   \n",
       "\n",
       "                                             small_features_v5_xgb  \\\n",
       "lgb411_tune                                               0.931473   \n",
       "plants_lgb                                                0.931010   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb               0.924954   \n",
       "xentropy_small_lr_cat_lgb                                 0.923585   \n",
       "simple_feature_lgb                                        0.922071   \n",
       "all_mean_enc_lgb                                          0.925226   \n",
       "all_mean_enc_user_feat_lgb                                0.925803   \n",
       "all_mean_enc_user_feat2_lgb                               0.917543   \n",
       "cat_interact_lgb                                          0.920951   \n",
       "mean_enc_lgb                                              0.923394   \n",
       "marcus_lgb                                                0.933476   \n",
       "fused_text_lgb                                            0.950737   \n",
       "mixed_features_text_proprocessing_lgb                     0.949450   \n",
       "select_dense_features_lgb                                 0.915873   \n",
       "select_sparse_features_lgb                                0.921414   \n",
       "lgb411_dart_tune                                          0.932515   \n",
       "poisson_lgb                                               0.925590   \n",
       "small_features_v5_xgb                                     1.000000   \n",
       "small_features_v4_xgb                                     0.987775   \n",
       "nima_features_xgb                                         0.987994   \n",
       "img_meta_xgb                                              0.980873   \n",
       "img_meta_nima_xgb                                         0.977972   \n",
       "baseline_xgb                                              0.919868   \n",
       "ranking_xgb                                               0.742374   \n",
       "catboost                                                  0.911971   \n",
       "catboost1_without_text                                    0.911971   \n",
       "mcl_cgb                                                   0.935648   \n",
       "pretrained_bigru_cv1d_rnn                                 0.911734   \n",
       "pretrained_bigru_attention_rnn                            0.911048   \n",
       "pretrained_2gru_rnn                                       0.908847   \n",
       "selftrained_bigru_conv1d_rnn                              0.910415   \n",
       "text_lgb                                                  0.850251   \n",
       "text_cwb_rg                                               0.702433   \n",
       "text_fm                                                   0.847868   \n",
       "text_rg                                                   0.849526   \n",
       "mlp                                                       0.890831   \n",
       "alpha_0001                                                0.832539   \n",
       "alpha_160                                                 0.868728   \n",
       "alpha_10                                                  0.848975   \n",
       "alpha_320                                                 0.872565   \n",
       "lr_l1_05                                                  0.832500   \n",
       "lr_l1_1                                                   0.820057   \n",
       "lr_l2_01                                                  0.842153   \n",
       "lr_l2_1                                                   0.814219   \n",
       "cls05_lgb                                                 0.878056   \n",
       "cls0_lgb                                                  0.766178   \n",
       "\n",
       "                                             small_features_v4_xgb  \\\n",
       "lgb411_tune                                               0.927126   \n",
       "plants_lgb                                                0.926905   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb               0.920816   \n",
       "xentropy_small_lr_cat_lgb                                 0.919470   \n",
       "simple_feature_lgb                                        0.918064   \n",
       "all_mean_enc_lgb                                          0.921064   \n",
       "all_mean_enc_user_feat_lgb                                0.921642   \n",
       "all_mean_enc_user_feat2_lgb                               0.913503   \n",
       "cat_interact_lgb                                          0.916776   \n",
       "mean_enc_lgb                                              0.919352   \n",
       "marcus_lgb                                                0.929105   \n",
       "fused_text_lgb                                            0.946809   \n",
       "mixed_features_text_proprocessing_lgb                     0.945420   \n",
       "select_dense_features_lgb                                 0.911518   \n",
       "select_sparse_features_lgb                                0.917017   \n",
       "lgb411_dart_tune                                          0.928168   \n",
       "poisson_lgb                                               0.921467   \n",
       "small_features_v5_xgb                                     0.987775   \n",
       "small_features_v4_xgb                                     1.000000   \n",
       "nima_features_xgb                                         0.983083   \n",
       "img_meta_xgb                                              0.976000   \n",
       "img_meta_nima_xgb                                         0.973208   \n",
       "baseline_xgb                                              0.915654   \n",
       "ranking_xgb                                               0.740242   \n",
       "catboost                                                  0.908011   \n",
       "catboost1_without_text                                    0.908011   \n",
       "mcl_cgb                                                   0.931624   \n",
       "pretrained_bigru_cv1d_rnn                                 0.907881   \n",
       "pretrained_bigru_attention_rnn                            0.907101   \n",
       "pretrained_2gru_rnn                                       0.904934   \n",
       "selftrained_bigru_conv1d_rnn                              0.906435   \n",
       "text_lgb                                                  0.847049   \n",
       "text_cwb_rg                                               0.699813   \n",
       "text_fm                                                   0.844865   \n",
       "text_rg                                                   0.846549   \n",
       "mlp                                                       0.887099   \n",
       "alpha_0001                                                0.829478   \n",
       "alpha_160                                                 0.865605   \n",
       "alpha_10                                                  0.845877   \n",
       "alpha_320                                                 0.869439   \n",
       "lr_l1_05                                                  0.829700   \n",
       "lr_l1_1                                                   0.817291   \n",
       "lr_l2_01                                                  0.839354   \n",
       "lr_l2_1                                                   0.811488   \n",
       "cls05_lgb                                                 0.874278   \n",
       "cls0_lgb                                                  0.762278   \n",
       "\n",
       "                                             nima_features_xgb  img_meta_xgb  \\\n",
       "lgb411_tune                                           0.935678      0.940697   \n",
       "plants_lgb                                            0.930084      0.925493   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb           0.928658      0.931048   \n",
       "xentropy_small_lr_cat_lgb                             0.926771      0.928518   \n",
       "simple_feature_lgb                                    0.921465      0.916744   \n",
       "all_mean_enc_lgb                                      0.924221      0.919339   \n",
       "all_mean_enc_user_feat_lgb                            0.924791      0.919893   \n",
       "all_mean_enc_user_feat2_lgb                           0.916518      0.911642   \n",
       "cat_interact_lgb                                      0.919459      0.914348   \n",
       "mean_enc_lgb                                          0.922500      0.917717   \n",
       "marcus_lgb                                            0.932235      0.934714   \n",
       "fused_text_lgb                                        0.950493      0.945683   \n",
       "mixed_features_text_proprocessing_lgb                 0.949004      0.944127   \n",
       "select_dense_features_lgb                             0.918223      0.917351   \n",
       "select_sparse_features_lgb                            0.923615      0.922866   \n",
       "lgb411_dart_tune                                      0.935901      0.938310   \n",
       "poisson_lgb                                           0.929679      0.933917   \n",
       "small_features_v5_xgb                                 0.987994      0.980873   \n",
       "small_features_v4_xgb                                 0.983083      0.976000   \n",
       "nima_features_xgb                                     1.000000      0.978996   \n",
       "img_meta_xgb                                          0.978996      1.000000   \n",
       "img_meta_nima_xgb                                     0.982510      0.989161   \n",
       "baseline_xgb                                          0.923812      0.928109   \n",
       "ranking_xgb                                           0.745941      0.745525   \n",
       "catboost                                              0.912601      0.919371   \n",
       "catboost1_without_text                                0.912601      0.919371   \n",
       "mcl_cgb                                               0.939565      0.940027   \n",
       "pretrained_bigru_cv1d_rnn                             0.911240      0.907005   \n",
       "pretrained_bigru_attention_rnn                        0.910508      0.906356   \n",
       "pretrained_2gru_rnn                                   0.908305      0.904119   \n",
       "selftrained_bigru_conv1d_rnn                          0.910069      0.905936   \n",
       "text_lgb                                              0.851777      0.848667   \n",
       "text_cwb_rg                                           0.704198      0.701815   \n",
       "text_fm                                               0.849669      0.846402   \n",
       "text_rg                                               0.851503      0.848316   \n",
       "mlp                                                   0.891333      0.887415   \n",
       "alpha_0001                                            0.833880      0.830441   \n",
       "alpha_160                                             0.870432      0.866846   \n",
       "alpha_10                                              0.850466      0.846931   \n",
       "alpha_320                                             0.874324      0.870755   \n",
       "lr_l1_05                                              0.833920      0.830167   \n",
       "lr_l1_1                                               0.821330      0.817575   \n",
       "lr_l2_01                                              0.843655      0.839933   \n",
       "lr_l2_1                                               0.815432      0.811707   \n",
       "cls05_lgb                                             0.883037      0.889163   \n",
       "cls0_lgb                                              0.768238      0.768521   \n",
       "\n",
       "                                             img_meta_nima_xgb  baseline_xgb  \\\n",
       "lgb411_tune                                           0.943550      0.977573   \n",
       "plants_lgb                                            0.924579      0.955797   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb           0.933453      0.964709   \n",
       "xentropy_small_lr_cat_lgb                             0.930603      0.954971   \n",
       "simple_feature_lgb                                    0.916079      0.893908   \n",
       "all_mean_enc_lgb                                      0.918438      0.895836   \n",
       "all_mean_enc_user_feat_lgb                            0.919009      0.895842   \n",
       "all_mean_enc_user_feat2_lgb                           0.910830      0.887875   \n",
       "cat_interact_lgb                                      0.913160      0.890089   \n",
       "mean_enc_lgb                                          0.916878      0.894925   \n",
       "marcus_lgb                                            0.933102      0.906859   \n",
       "fused_text_lgb                                        0.945216      0.904365   \n",
       "mixed_features_text_proprocessing_lgb                 0.943598      0.905456   \n",
       "select_dense_features_lgb                             0.918931      0.893515   \n",
       "select_sparse_features_lgb                            0.924344      0.899271   \n",
       "lgb411_dart_tune                                      0.940572      0.970684   \n",
       "poisson_lgb                                           0.936607      0.972942   \n",
       "small_features_v5_xgb                                 0.977972      0.919868   \n",
       "small_features_v4_xgb                                 0.973208      0.915654   \n",
       "nima_features_xgb                                     0.982510      0.923812   \n",
       "img_meta_xgb                                          0.989161      0.928109   \n",
       "img_meta_nima_xgb                                     1.000000      0.930752   \n",
       "baseline_xgb                                          0.930752      1.000000   \n",
       "ranking_xgb                                           0.746903      0.762846   \n",
       "catboost                                              0.918528      0.885101   \n",
       "catboost1_without_text                                0.918528      0.885101   \n",
       "mcl_cgb                                               0.942517      0.964860   \n",
       "pretrained_bigru_cv1d_rnn                             0.906398      0.884244   \n",
       "pretrained_bigru_attention_rnn                        0.905686      0.883541   \n",
       "pretrained_2gru_rnn                                   0.903453      0.880995   \n",
       "selftrained_bigru_conv1d_rnn                          0.905387      0.883780   \n",
       "text_lgb                                              0.849234      0.816482   \n",
       "text_cwb_rg                                           0.702643      0.680675   \n",
       "text_fm                                               0.847204      0.815321   \n",
       "text_rg                                               0.849195      0.817191   \n",
       "mlp                                                   0.887269      0.857438   \n",
       "alpha_0001                                            0.830877      0.805885   \n",
       "alpha_160                                             0.867503      0.840286   \n",
       "alpha_10                                              0.847454      0.821631   \n",
       "alpha_320                                             0.871452      0.843861   \n",
       "lr_l1_05                                              0.830699      0.805410   \n",
       "lr_l1_1                                               0.818032      0.793460   \n",
       "lr_l2_01                                              0.840524      0.815044   \n",
       "lr_l2_1                                               0.812143      0.787995   \n",
       "cls05_lgb                                             0.892597      0.931141   \n",
       "cls0_lgb                                              0.769703      0.780367   \n",
       "\n",
       "                                             ranking_xgb  catboost  \\\n",
       "lgb411_tune                                     0.769518  0.896485   \n",
       "plants_lgb                                      0.765113  0.890247   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb     0.768467  0.894999   \n",
       "xentropy_small_lr_cat_lgb                       0.758656  0.888887   \n",
       "simple_feature_lgb                              0.738648  0.902593   \n",
       "all_mean_enc_lgb                                0.737372  0.903455   \n",
       "all_mean_enc_user_feat_lgb                      0.737337  0.902602   \n",
       "all_mean_enc_user_feat2_lgb                     0.737129  0.892868   \n",
       "cat_interact_lgb                                0.734428  0.902092   \n",
       "mean_enc_lgb                                    0.737832  0.902028   \n",
       "marcus_lgb                                      0.736728  0.919560   \n",
       "fused_text_lgb                                  0.741206  0.904336   \n",
       "mixed_features_text_proprocessing_lgb           0.739742  0.898912   \n",
       "select_dense_features_lgb                       0.735165  0.918677   \n",
       "select_sparse_features_lgb                      0.734971  0.907917   \n",
       "lgb411_dart_tune                                0.771464  0.897617   \n",
       "poisson_lgb                                     0.766890  0.891646   \n",
       "small_features_v5_xgb                           0.742374  0.911971   \n",
       "small_features_v4_xgb                           0.740242  0.908011   \n",
       "nima_features_xgb                               0.745941  0.912601   \n",
       "img_meta_xgb                                    0.745525  0.919371   \n",
       "img_meta_nima_xgb                               0.746903  0.918528   \n",
       "baseline_xgb                                    0.762846  0.885101   \n",
       "ranking_xgb                                     1.000000  0.741137   \n",
       "catboost                                        0.741137  1.000000   \n",
       "catboost1_without_text                          0.741137  1.000000   \n",
       "mcl_cgb                                         0.774139  0.911169   \n",
       "pretrained_bigru_cv1d_rnn                       0.723515  0.888036   \n",
       "pretrained_bigru_attention_rnn                  0.721852  0.887385   \n",
       "pretrained_2gru_rnn                             0.722929  0.885382   \n",
       "selftrained_bigru_conv1d_rnn                    0.717453  0.872626   \n",
       "text_lgb                                        0.689818  0.811322   \n",
       "text_cwb_rg                                     0.628618  0.729226   \n",
       "text_fm                                         0.696715  0.794808   \n",
       "text_rg                                         0.703082  0.803529   \n",
       "mlp                                             0.709817  0.846260   \n",
       "alpha_0001                                      0.697815  0.787240   \n",
       "alpha_160                                       0.731386  0.830252   \n",
       "alpha_10                                        0.711790  0.803459   \n",
       "alpha_320                                       0.736516  0.839353   \n",
       "lr_l1_05                                        0.602618  0.778251   \n",
       "lr_l1_1                                         0.591424  0.757783   \n",
       "lr_l2_01                                        0.612974  0.797388   \n",
       "lr_l2_1                                         0.586523  0.749147   \n",
       "cls05_lgb                                       0.643513  0.837270   \n",
       "cls0_lgb                                        0.880291  0.760855   \n",
       "\n",
       "                                             catboost1_without_text   mcl_cgb  \\\n",
       "lgb411_tune                                                0.896485  0.976032   \n",
       "plants_lgb                                                 0.890247  0.969725   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                0.894999  0.976016   \n",
       "xentropy_small_lr_cat_lgb                                  0.888887  0.961636   \n",
       "simple_feature_lgb                                         0.902593  0.918213   \n",
       "all_mean_enc_lgb                                           0.903455  0.918551   \n",
       "all_mean_enc_user_feat_lgb                                 0.902602  0.918047   \n",
       "all_mean_enc_user_feat2_lgb                                0.892868  0.910381   \n",
       "cat_interact_lgb                                           0.902092  0.913036   \n",
       "mean_enc_lgb                                               0.902028  0.918151   \n",
       "marcus_lgb                                                 0.919560  0.925841   \n",
       "fused_text_lgb                                             0.904336  0.928662   \n",
       "mixed_features_text_proprocessing_lgb                      0.898912  0.928638   \n",
       "select_dense_features_lgb                                  0.918677  0.911059   \n",
       "select_sparse_features_lgb                                 0.907917  0.916608   \n",
       "lgb411_dart_tune                                           0.897617  0.975858   \n",
       "poisson_lgb                                                0.891646  0.972201   \n",
       "small_features_v5_xgb                                      0.911971  0.935648   \n",
       "small_features_v4_xgb                                      0.908011  0.931624   \n",
       "nima_features_xgb                                          0.912601  0.939565   \n",
       "img_meta_xgb                                               0.919371  0.940027   \n",
       "img_meta_nima_xgb                                          0.918528  0.942517   \n",
       "baseline_xgb                                               0.885101  0.964860   \n",
       "ranking_xgb                                                0.741137  0.774139   \n",
       "catboost                                                   1.000000  0.911169   \n",
       "catboost1_without_text                                     1.000000  0.911169   \n",
       "mcl_cgb                                                    0.911169  1.000000   \n",
       "pretrained_bigru_cv1d_rnn                                  0.888036  0.906421   \n",
       "pretrained_bigru_attention_rnn                             0.887385  0.905660   \n",
       "pretrained_2gru_rnn                                        0.885382  0.903111   \n",
       "selftrained_bigru_conv1d_rnn                               0.872626  0.906813   \n",
       "text_lgb                                                   0.811322  0.848970   \n",
       "text_cwb_rg                                                0.729226  0.710759   \n",
       "text_fm                                                    0.794808  0.848899   \n",
       "text_rg                                                    0.803529  0.851632   \n",
       "mlp                                                        0.846260  0.884223   \n",
       "alpha_0001                                                 0.787240  0.836093   \n",
       "alpha_160                                                  0.830252  0.872388   \n",
       "alpha_10                                                   0.803459  0.852646   \n",
       "alpha_320                                                  0.839353  0.876225   \n",
       "lr_l1_05                                                   0.778251  0.833304   \n",
       "lr_l1_1                                                    0.757783  0.820399   \n",
       "lr_l2_01                                                   0.797388  0.843665   \n",
       "lr_l2_1                                                    0.749147  0.814558   \n",
       "cls05_lgb                                                  0.837270  0.928724   \n",
       "cls0_lgb                                                   0.760855  0.791433   \n",
       "\n",
       "                                             pretrained_bigru_cv1d_rnn  \\\n",
       "lgb411_tune                                                   0.895204   \n",
       "plants_lgb                                                    0.897465   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                   0.891990   \n",
       "xentropy_small_lr_cat_lgb                                     0.901173   \n",
       "simple_feature_lgb                                            0.919504   \n",
       "all_mean_enc_lgb                                              0.922844   \n",
       "all_mean_enc_user_feat_lgb                                    0.921836   \n",
       "all_mean_enc_user_feat2_lgb                                   0.914385   \n",
       "cat_interact_lgb                                              0.916719   \n",
       "mean_enc_lgb                                                  0.921847   \n",
       "marcus_lgb                                                    0.919778   \n",
       "fused_text_lgb                                                0.917589   \n",
       "mixed_features_text_proprocessing_lgb                         0.916229   \n",
       "select_dense_features_lgb                                     0.913667   \n",
       "select_sparse_features_lgb                                    0.914844   \n",
       "lgb411_dart_tune                                              0.896763   \n",
       "poisson_lgb                                                   0.890068   \n",
       "small_features_v5_xgb                                         0.911734   \n",
       "small_features_v4_xgb                                         0.907881   \n",
       "nima_features_xgb                                             0.911240   \n",
       "img_meta_xgb                                                  0.907005   \n",
       "img_meta_nima_xgb                                             0.906398   \n",
       "baseline_xgb                                                  0.884244   \n",
       "ranking_xgb                                                   0.723515   \n",
       "catboost                                                      0.888036   \n",
       "catboost1_without_text                                        0.888036   \n",
       "mcl_cgb                                                       0.906421   \n",
       "pretrained_bigru_cv1d_rnn                                     1.000000   \n",
       "pretrained_bigru_attention_rnn                                0.970911   \n",
       "pretrained_2gru_rnn                                           0.968960   \n",
       "selftrained_bigru_conv1d_rnn                                  0.953796   \n",
       "text_lgb                                                      0.849023   \n",
       "text_cwb_rg                                                   0.714940   \n",
       "text_fm                                                       0.834455   \n",
       "text_rg                                                       0.838957   \n",
       "mlp                                                           0.897684   \n",
       "alpha_0001                                                    0.824145   \n",
       "alpha_160                                                     0.864294   \n",
       "alpha_10                                                      0.840836   \n",
       "alpha_320                                                     0.870583   \n",
       "lr_l1_05                                                      0.831889   \n",
       "lr_l1_1                                                       0.815955   \n",
       "lr_l2_01                                                      0.845514   \n",
       "lr_l2_1                                                       0.808175   \n",
       "cls05_lgb                                                     0.842373   \n",
       "cls0_lgb                                                      0.748887   \n",
       "\n",
       "                                             pretrained_bigru_attention_rnn  \\\n",
       "lgb411_tune                                                        0.894450   \n",
       "plants_lgb                                                         0.896696   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                        0.891134   \n",
       "xentropy_small_lr_cat_lgb                                          0.900185   \n",
       "simple_feature_lgb                                                 0.918586   \n",
       "all_mean_enc_lgb                                                   0.921886   \n",
       "all_mean_enc_user_feat_lgb                                         0.920802   \n",
       "all_mean_enc_user_feat2_lgb                                        0.913422   \n",
       "cat_interact_lgb                                                   0.915921   \n",
       "mean_enc_lgb                                                       0.921070   \n",
       "marcus_lgb                                                         0.918981   \n",
       "fused_text_lgb                                                     0.916410   \n",
       "mixed_features_text_proprocessing_lgb                              0.914980   \n",
       "select_dense_features_lgb                                          0.912747   \n",
       "select_sparse_features_lgb                                         0.914060   \n",
       "lgb411_dart_tune                                                   0.896112   \n",
       "poisson_lgb                                                        0.889364   \n",
       "small_features_v5_xgb                                              0.911048   \n",
       "small_features_v4_xgb                                              0.907101   \n",
       "nima_features_xgb                                                  0.910508   \n",
       "img_meta_xgb                                                       0.906356   \n",
       "img_meta_nima_xgb                                                  0.905686   \n",
       "baseline_xgb                                                       0.883541   \n",
       "ranking_xgb                                                        0.721852   \n",
       "catboost                                                           0.887385   \n",
       "catboost1_without_text                                             0.887385   \n",
       "mcl_cgb                                                            0.905660   \n",
       "pretrained_bigru_cv1d_rnn                                          0.970911   \n",
       "pretrained_bigru_attention_rnn                                     1.000000   \n",
       "pretrained_2gru_rnn                                                0.966881   \n",
       "selftrained_bigru_conv1d_rnn                                       0.952309   \n",
       "text_lgb                                                           0.848229   \n",
       "text_cwb_rg                                                        0.715935   \n",
       "text_fm                                                            0.834031   \n",
       "text_rg                                                            0.838447   \n",
       "mlp                                                                0.897399   \n",
       "alpha_0001                                                         0.823576   \n",
       "alpha_160                                                          0.863698   \n",
       "alpha_10                                                           0.840236   \n",
       "alpha_320                                                          0.869998   \n",
       "lr_l1_05                                                           0.831351   \n",
       "lr_l1_1                                                            0.815478   \n",
       "lr_l2_01                                                           0.845075   \n",
       "lr_l2_1                                                            0.807718   \n",
       "cls05_lgb                                                          0.842070   \n",
       "cls0_lgb                                                           0.747406   \n",
       "\n",
       "                                             pretrained_2gru_rnn  \\\n",
       "lgb411_tune                                             0.891942   \n",
       "plants_lgb                                              0.894225   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb             0.888897   \n",
       "xentropy_small_lr_cat_lgb                               0.897477   \n",
       "simple_feature_lgb                                      0.916392   \n",
       "all_mean_enc_lgb                                        0.919655   \n",
       "all_mean_enc_user_feat_lgb                              0.918679   \n",
       "all_mean_enc_user_feat2_lgb                             0.911270   \n",
       "cat_interact_lgb                                        0.913904   \n",
       "mean_enc_lgb                                            0.918656   \n",
       "marcus_lgb                                              0.916932   \n",
       "fused_text_lgb                                          0.914216   \n",
       "mixed_features_text_proprocessing_lgb                   0.912850   \n",
       "select_dense_features_lgb                               0.911341   \n",
       "select_sparse_features_lgb                              0.912147   \n",
       "lgb411_dart_tune                                        0.893587   \n",
       "poisson_lgb                                             0.886849   \n",
       "small_features_v5_xgb                                   0.908847   \n",
       "small_features_v4_xgb                                   0.904934   \n",
       "nima_features_xgb                                       0.908305   \n",
       "img_meta_xgb                                            0.904119   \n",
       "img_meta_nima_xgb                                       0.903453   \n",
       "baseline_xgb                                            0.880995   \n",
       "ranking_xgb                                             0.722929   \n",
       "catboost                                                0.885382   \n",
       "catboost1_without_text                                  0.885382   \n",
       "mcl_cgb                                                 0.903111   \n",
       "pretrained_bigru_cv1d_rnn                               0.968960   \n",
       "pretrained_bigru_attention_rnn                          0.966881   \n",
       "pretrained_2gru_rnn                                     1.000000   \n",
       "selftrained_bigru_conv1d_rnn                            0.949535   \n",
       "text_lgb                                                0.845857   \n",
       "text_cwb_rg                                             0.712184   \n",
       "text_fm                                                 0.831046   \n",
       "text_rg                                                 0.835543   \n",
       "mlp                                                     0.894728   \n",
       "alpha_0001                                              0.820945   \n",
       "alpha_160                                               0.860862   \n",
       "alpha_10                                                0.837553   \n",
       "alpha_320                                               0.867146   \n",
       "lr_l1_05                                                0.828514   \n",
       "lr_l1_1                                                 0.812761   \n",
       "lr_l2_01                                                0.842175   \n",
       "lr_l2_1                                                 0.805075   \n",
       "cls05_lgb                                               0.838464   \n",
       "cls0_lgb                                                0.748642   \n",
       "\n",
       "                                             selftrained_bigru_conv1d_rnn  \\\n",
       "lgb411_tune                                                      0.894094   \n",
       "plants_lgb                                                       0.897245   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb                      0.891522   \n",
       "xentropy_small_lr_cat_lgb                                        0.899517   \n",
       "simple_feature_lgb                                               0.914868   \n",
       "all_mean_enc_lgb                                                 0.917843   \n",
       "all_mean_enc_user_feat_lgb                                       0.916663   \n",
       "all_mean_enc_user_feat2_lgb                                      0.909777   \n",
       "cat_interact_lgb                                                 0.910945   \n",
       "mean_enc_lgb                                                     0.917476   \n",
       "marcus_lgb                                                       0.912860   \n",
       "fused_text_lgb                                                   0.915931   \n",
       "mixed_features_text_proprocessing_lgb                            0.915627   \n",
       "select_dense_features_lgb                                        0.900438   \n",
       "select_sparse_features_lgb                                       0.907708   \n",
       "lgb411_dart_tune                                                 0.896312   \n",
       "poisson_lgb                                                      0.889476   \n",
       "small_features_v5_xgb                                            0.910415   \n",
       "small_features_v4_xgb                                            0.906435   \n",
       "nima_features_xgb                                                0.910069   \n",
       "img_meta_xgb                                                     0.905936   \n",
       "img_meta_nima_xgb                                                0.905387   \n",
       "baseline_xgb                                                     0.883780   \n",
       "ranking_xgb                                                      0.717453   \n",
       "catboost                                                         0.872626   \n",
       "catboost1_without_text                                           0.872626   \n",
       "mcl_cgb                                                          0.906813   \n",
       "pretrained_bigru_cv1d_rnn                                        0.953796   \n",
       "pretrained_bigru_attention_rnn                                   0.952309   \n",
       "pretrained_2gru_rnn                                              0.949535   \n",
       "selftrained_bigru_conv1d_rnn                                     1.000000   \n",
       "text_lgb                                                         0.859459   \n",
       "text_cwb_rg                                                      0.708776   \n",
       "text_fm                                                          0.847166   \n",
       "text_rg                                                          0.850304   \n",
       "mlp                                                              0.906238   \n",
       "alpha_0001                                                       0.833640   \n",
       "alpha_160                                                        0.872537   \n",
       "alpha_10                                                         0.850473   \n",
       "alpha_320                                                        0.877578   \n",
       "lr_l1_05                                                         0.843599   \n",
       "lr_l1_1                                                          0.829074   \n",
       "lr_l2_01                                                         0.854190   \n",
       "lr_l2_1                                                          0.821526   \n",
       "cls05_lgb                                                        0.842529   \n",
       "cls0_lgb                                                         0.743855   \n",
       "\n",
       "                                             text_lgb  text_cwb_rg   text_fm  \\\n",
       "lgb411_tune                                  0.823135     0.685161  0.821971   \n",
       "plants_lgb                                   0.835321     0.695687  0.834781   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb  0.828427     0.690939  0.827950   \n",
       "xentropy_small_lr_cat_lgb                    0.827092     0.686990  0.827854   \n",
       "simple_feature_lgb                           0.869182     0.716797  0.849560   \n",
       "all_mean_enc_lgb                             0.866855     0.710733  0.846129   \n",
       "all_mean_enc_user_feat_lgb                   0.865516     0.709698  0.844977   \n",
       "all_mean_enc_user_feat2_lgb                  0.871939     0.715363  0.851349   \n",
       "cat_interact_lgb                             0.862606     0.710778  0.841915   \n",
       "mean_enc_lgb                                 0.869846     0.712374  0.849202   \n",
       "marcus_lgb                                   0.852152     0.701711  0.832332   \n",
       "fused_text_lgb                               0.866779     0.715492  0.868791   \n",
       "mixed_features_text_proprocessing_lgb        0.868076     0.710091  0.867281   \n",
       "select_dense_features_lgb                    0.823182     0.705327  0.804784   \n",
       "select_sparse_features_lgb                   0.841794     0.695711  0.821912   \n",
       "lgb411_dart_tune                             0.834262     0.699198  0.834251   \n",
       "poisson_lgb                                  0.823106     0.687793  0.822702   \n",
       "small_features_v5_xgb                        0.850251     0.702433  0.847868   \n",
       "small_features_v4_xgb                        0.847049     0.699813  0.844865   \n",
       "nima_features_xgb                            0.851777     0.704198  0.849669   \n",
       "img_meta_xgb                                 0.848667     0.701815  0.846402   \n",
       "img_meta_nima_xgb                            0.849234     0.702643  0.847204   \n",
       "baseline_xgb                                 0.816482     0.680675  0.815321   \n",
       "ranking_xgb                                  0.689818     0.628618  0.696715   \n",
       "catboost                                     0.811322     0.729226  0.794808   \n",
       "catboost1_without_text                       0.811322     0.729226  0.794808   \n",
       "mcl_cgb                                      0.848970     0.710759  0.848899   \n",
       "pretrained_bigru_cv1d_rnn                    0.849023     0.714940  0.834455   \n",
       "pretrained_bigru_attention_rnn               0.848229     0.715935  0.834031   \n",
       "pretrained_2gru_rnn                          0.845857     0.712184  0.831046   \n",
       "selftrained_bigru_conv1d_rnn                 0.859459     0.708776  0.847166   \n",
       "text_lgb                                     1.000000     0.788162  0.940994   \n",
       "text_cwb_rg                                  0.788162     1.000000  0.768838   \n",
       "text_fm                                      0.940994     0.768838  1.000000   \n",
       "text_rg                                      0.947320     0.789577  0.992530   \n",
       "mlp                                          0.886745     0.716294  0.895938   \n",
       "alpha_0001                                   0.872291     0.717921  0.923028   \n",
       "alpha_160                                    0.914040     0.766546  0.952428   \n",
       "alpha_10                                     0.891084     0.734738  0.941819   \n",
       "alpha_320                                    0.918478     0.778527  0.949769   \n",
       "lr_l1_05                                     0.859582     0.672195  0.868349   \n",
       "lr_l1_1                                      0.843389     0.651171  0.865318   \n",
       "lr_l2_01                                     0.866778     0.692946  0.875062   \n",
       "lr_l2_1                                      0.834438     0.642389  0.863841   \n",
       "cls05_lgb                                    0.773886     0.617066  0.769777   \n",
       "cls0_lgb                                     0.712138     0.661407  0.713722   \n",
       "\n",
       "                                              text_rg       mlp  alpha_0001  \\\n",
       "lgb411_tune                                  0.823708  0.866779    0.813002   \n",
       "plants_lgb                                   0.836749  0.874204    0.824736   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb  0.830028  0.867792    0.818223   \n",
       "xentropy_small_lr_cat_lgb                    0.829751  0.872978    0.821121   \n",
       "simple_feature_lgb                           0.853084  0.893489    0.841548   \n",
       "all_mean_enc_lgb                             0.848887  0.896498    0.846315   \n",
       "all_mean_enc_user_feat_lgb                   0.847646  0.896866    0.846989   \n",
       "all_mean_enc_user_feat2_lgb                  0.854074  0.891481    0.843902   \n",
       "cat_interact_lgb                             0.845153  0.891943    0.835859   \n",
       "mean_enc_lgb                                 0.851942  0.897334    0.848833   \n",
       "marcus_lgb                                   0.835086  0.887590    0.825604   \n",
       "fused_text_lgb                               0.870874  0.900485    0.853381   \n",
       "mixed_features_text_proprocessing_lgb        0.868427  0.900056    0.852450   \n",
       "select_dense_features_lgb                    0.809867  0.866792    0.797980   \n",
       "select_sparse_features_lgb                   0.824401  0.877851    0.813741   \n",
       "lgb411_dart_tune                             0.836672  0.870983    0.822557   \n",
       "poisson_lgb                                  0.824834  0.863948    0.812784   \n",
       "small_features_v5_xgb                        0.849526  0.890831    0.832539   \n",
       "small_features_v4_xgb                        0.846549  0.887099    0.829478   \n",
       "nima_features_xgb                            0.851503  0.891333    0.833880   \n",
       "img_meta_xgb                                 0.848316  0.887415    0.830441   \n",
       "img_meta_nima_xgb                            0.849195  0.887269    0.830877   \n",
       "baseline_xgb                                 0.817191  0.857438    0.805885   \n",
       "ranking_xgb                                  0.703082  0.709817    0.697815   \n",
       "catboost                                     0.803529  0.846260    0.787240   \n",
       "catboost1_without_text                       0.803529  0.846260    0.787240   \n",
       "mcl_cgb                                      0.851632  0.884223    0.836093   \n",
       "pretrained_bigru_cv1d_rnn                    0.838957  0.897684    0.824145   \n",
       "pretrained_bigru_attention_rnn               0.838447  0.897399    0.823576   \n",
       "pretrained_2gru_rnn                          0.835543  0.894728    0.820945   \n",
       "selftrained_bigru_conv1d_rnn                 0.850304  0.906238    0.833640   \n",
       "text_lgb                                     0.947320  0.886745    0.872291   \n",
       "text_cwb_rg                                  0.789577  0.716294    0.717921   \n",
       "text_fm                                      0.992530  0.895938    0.923028   \n",
       "text_rg                                      1.000000  0.890810    0.918645   \n",
       "mlp                                          0.890810  1.000000    0.879176   \n",
       "alpha_0001                                   0.918645  0.879176    1.000000   \n",
       "alpha_160                                    0.956040  0.914453    0.967830   \n",
       "alpha_10                                     0.938341  0.897046    0.993234   \n",
       "alpha_320                                    0.956755  0.916009    0.957903   \n",
       "lr_l1_05                                     0.871884  0.885489    0.871821   \n",
       "lr_l1_1                                      0.863262  0.878420    0.879716   \n",
       "lr_l2_01                                     0.880309  0.892664    0.874106   \n",
       "lr_l2_1                                      0.859124  0.874183    0.883194   \n",
       "cls05_lgb                                    0.770324  0.815301    0.758149   \n",
       "cls0_lgb                                     0.722905  0.728305    0.705350   \n",
       "\n",
       "                                             alpha_160  alpha_10  alpha_320  \\\n",
       "lgb411_tune                                   0.847543  0.828779   0.851161   \n",
       "plants_lgb                                    0.860072  0.840884   0.863767   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb   0.853142  0.834198   0.856787   \n",
       "xentropy_small_lr_cat_lgb                     0.856480  0.837466   0.859861   \n",
       "simple_feature_lgb                            0.880072  0.858496   0.884830   \n",
       "all_mean_enc_lgb                              0.878901  0.860962   0.882902   \n",
       "all_mean_enc_user_feat_lgb                    0.879539  0.861631   0.883506   \n",
       "all_mean_enc_user_feat2_lgb                   0.877224  0.858859   0.881255   \n",
       "cat_interact_lgb                              0.873883  0.852646   0.878525   \n",
       "mean_enc_lgb                                  0.881341  0.863476   0.885262   \n",
       "marcus_lgb                                    0.863308  0.842154   0.867993   \n",
       "fused_text_lgb                                0.891558  0.870849   0.895356   \n",
       "mixed_features_text_proprocessing_lgb         0.889281  0.869725   0.892455   \n",
       "select_dense_features_lgb                     0.839233  0.814377   0.846704   \n",
       "select_sparse_features_lgb                    0.851746  0.830083   0.856936   \n",
       "lgb411_dart_tune                              0.858009  0.838689   0.861811   \n",
       "poisson_lgb                                   0.847676  0.828704   0.851360   \n",
       "small_features_v5_xgb                         0.868728  0.848975   0.872565   \n",
       "small_features_v4_xgb                         0.865605  0.845877   0.869439   \n",
       "nima_features_xgb                             0.870432  0.850466   0.874324   \n",
       "img_meta_xgb                                  0.866846  0.846931   0.870755   \n",
       "img_meta_nima_xgb                             0.867503  0.847454   0.871452   \n",
       "baseline_xgb                                  0.840286  0.821631   0.843861   \n",
       "ranking_xgb                                   0.731386  0.711790   0.736516   \n",
       "catboost                                      0.830252  0.803459   0.839353   \n",
       "catboost1_without_text                        0.830252  0.803459   0.839353   \n",
       "mcl_cgb                                       0.872388  0.852646   0.876225   \n",
       "pretrained_bigru_cv1d_rnn                     0.864294  0.840836   0.870583   \n",
       "pretrained_bigru_attention_rnn                0.863698  0.840236   0.869998   \n",
       "pretrained_2gru_rnn                           0.860862  0.837553   0.867146   \n",
       "selftrained_bigru_conv1d_rnn                  0.872537  0.850473   0.877578   \n",
       "text_lgb                                      0.914040  0.891084   0.918478   \n",
       "text_cwb_rg                                   0.766546  0.734738   0.778527   \n",
       "text_fm                                       0.952428  0.941819   0.949769   \n",
       "text_rg                                       0.956040  0.938341   0.956755   \n",
       "mlp                                           0.914453  0.897046   0.916009   \n",
       "alpha_0001                                    0.967830  0.993234   0.957903   \n",
       "alpha_160                                     1.000000  0.987128   0.998369   \n",
       "alpha_10                                      0.987128  1.000000   0.978444   \n",
       "alpha_320                                     0.998369  0.978444   1.000000   \n",
       "lr_l1_05                                      0.908484  0.890701   0.909061   \n",
       "lr_l1_1                                       0.903209  0.896439   0.898929   \n",
       "lr_l2_01                                      0.915742  0.893548   0.919034   \n",
       "lr_l2_1                                       0.900263  0.898729   0.893928   \n",
       "cls05_lgb                                     0.788835  0.772767   0.791272   \n",
       "cls0_lgb                                      0.740193  0.719560   0.746015   \n",
       "\n",
       "                                             lr_l1_05   lr_l1_1  lr_l2_01  \\\n",
       "lgb411_tune                                  0.813043  0.800982  0.822777   \n",
       "plants_lgb                                   0.823825  0.811323  0.833793   \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb  0.816884  0.804517  0.826794   \n",
       "xentropy_small_lr_cat_lgb                    0.821350  0.810223  0.830989   \n",
       "simple_feature_lgb                           0.838050  0.822719  0.847758   \n",
       "all_mean_enc_lgb                             0.840290  0.826367  0.848981   \n",
       "all_mean_enc_user_feat_lgb                   0.841827  0.827788  0.850257   \n",
       "all_mean_enc_user_feat2_lgb                  0.834829  0.820828  0.843080   \n",
       "cat_interact_lgb                             0.832751  0.817674  0.841905   \n",
       "mean_enc_lgb                                 0.841630  0.827670  0.850187   \n",
       "marcus_lgb                                   0.826684  0.811957  0.836491   \n",
       "fused_text_lgb                               0.851669  0.839089  0.861994   \n",
       "mixed_features_text_proprocessing_lgb        0.851668  0.839892  0.860305   \n",
       "select_dense_features_lgb                    0.799327  0.781676  0.815745   \n",
       "select_sparse_features_lgb                   0.814837  0.799672  0.826084   \n",
       "lgb411_dart_tune                             0.819196  0.806717  0.829294   \n",
       "poisson_lgb                                  0.812960  0.800763  0.822877   \n",
       "small_features_v5_xgb                        0.832500  0.820057  0.842153   \n",
       "small_features_v4_xgb                        0.829700  0.817291  0.839354   \n",
       "nima_features_xgb                            0.833920  0.821330  0.843655   \n",
       "img_meta_xgb                                 0.830167  0.817575  0.839933   \n",
       "img_meta_nima_xgb                            0.830699  0.818032  0.840524   \n",
       "baseline_xgb                                 0.805410  0.793460  0.815044   \n",
       "ranking_xgb                                  0.602618  0.591424  0.612974   \n",
       "catboost                                     0.778251  0.757783  0.797388   \n",
       "catboost1_without_text                       0.778251  0.757783  0.797388   \n",
       "mcl_cgb                                      0.833304  0.820399  0.843665   \n",
       "pretrained_bigru_cv1d_rnn                    0.831889  0.815955  0.845514   \n",
       "pretrained_bigru_attention_rnn               0.831351  0.815478  0.845075   \n",
       "pretrained_2gru_rnn                          0.828514  0.812761  0.842175   \n",
       "selftrained_bigru_conv1d_rnn                 0.843599  0.829074  0.854190   \n",
       "text_lgb                                     0.859582  0.843389  0.866778   \n",
       "text_cwb_rg                                  0.672195  0.651171  0.692946   \n",
       "text_fm                                      0.868349  0.865318  0.875062   \n",
       "text_rg                                      0.871884  0.863262  0.880309   \n",
       "mlp                                          0.885489  0.878420  0.892664   \n",
       "alpha_0001                                   0.871821  0.879716  0.874106   \n",
       "alpha_160                                    0.908484  0.903209  0.915742   \n",
       "alpha_10                                     0.890701  0.896439  0.893548   \n",
       "alpha_320                                    0.909061  0.898929  0.919034   \n",
       "lr_l1_05                                     1.000000  0.989319  0.990881   \n",
       "lr_l1_1                                      0.989319  1.000000  0.978720   \n",
       "lr_l2_01                                     0.990881  0.978720  1.000000   \n",
       "lr_l2_1                                      0.977235  0.993113  0.972749   \n",
       "cls05_lgb                                    0.819547  0.808260  0.827777   \n",
       "cls0_lgb                                     0.591173  0.579615  0.602285   \n",
       "\n",
       "                                              lr_l2_1  cls05_lgb  cls0_lgb  \n",
       "lgb411_tune                                  0.795435   0.945198  0.788349  \n",
       "plants_lgb                                   0.805571   0.919283  0.781393  \n",
       "plants_with_img_meta_nima_fm_geo_active_lgb  0.798866   0.931634  0.782935  \n",
       "xentropy_small_lr_cat_lgb                    0.804675   0.919523  0.779242  \n",
       "simple_feature_lgb                           0.814407   0.849272  0.754601  \n",
       "all_mean_enc_lgb                             0.818613   0.851784  0.752769  \n",
       "all_mean_enc_user_feat_lgb                   0.819961   0.852166  0.752736  \n",
       "all_mean_enc_user_feat2_lgb                  0.813033   0.842887  0.752974  \n",
       "cat_interact_lgb                             0.809139   0.845405  0.749832  \n",
       "mean_enc_lgb                                 0.819786   0.850715  0.753346  \n",
       "marcus_lgb                                   0.804208   0.865770  0.752856  \n",
       "fused_text_lgb                               0.832904   0.862263  0.758297  \n",
       "mixed_features_text_proprocessing_lgb        0.833779   0.864107  0.756379  \n",
       "select_dense_features_lgb                    0.773605   0.849946  0.750064  \n",
       "select_sparse_features_lgb                   0.791992   0.857522  0.748902  \n",
       "lgb411_dart_tune                             0.801049   0.932960  0.794906  \n",
       "poisson_lgb                                  0.795230   0.939893  0.786246  \n",
       "small_features_v5_xgb                        0.814219   0.878056  0.766178  \n",
       "small_features_v4_xgb                        0.811488   0.874278  0.762278  \n",
       "nima_features_xgb                            0.815432   0.883037  0.768238  \n",
       "img_meta_xgb                                 0.811707   0.889163  0.768521  \n",
       "img_meta_nima_xgb                            0.812143   0.892597  0.769703  \n",
       "baseline_xgb                                 0.787995   0.931141  0.780367  \n",
       "ranking_xgb                                  0.586523   0.643513  0.880291  \n",
       "catboost                                     0.749147   0.837270  0.760855  \n",
       "catboost1_without_text                       0.749147   0.837270  0.760855  \n",
       "mcl_cgb                                      0.814558   0.928724  0.791433  \n",
       "pretrained_bigru_cv1d_rnn                    0.808175   0.842373  0.748887  \n",
       "pretrained_bigru_attention_rnn               0.807718   0.842070  0.747406  \n",
       "pretrained_2gru_rnn                          0.805075   0.838464  0.748642  \n",
       "selftrained_bigru_conv1d_rnn                 0.821526   0.842529  0.743855  \n",
       "text_lgb                                     0.834438   0.773886  0.712138  \n",
       "text_cwb_rg                                  0.642389   0.617066  0.661407  \n",
       "text_fm                                      0.863841   0.769777  0.713722  \n",
       "text_rg                                      0.859124   0.770324  0.722905  \n",
       "mlp                                          0.874183   0.815301  0.728305  \n",
       "alpha_0001                                   0.883194   0.758149  0.705350  \n",
       "alpha_160                                    0.900263   0.788835  0.740193  \n",
       "alpha_10                                     0.898729   0.772767  0.719560  \n",
       "alpha_320                                    0.893928   0.791272  0.746015  \n",
       "lr_l1_05                                     0.977235   0.819547  0.591173  \n",
       "lr_l1_1                                      0.993113   0.808260  0.579615  \n",
       "lr_l2_01                                     0.972749   0.827777  0.602285  \n",
       "lr_l2_1                                      1.000000   0.802999  0.574573  \n",
       "cls05_lgb                                    0.802999   1.000000  0.634385  \n",
       "cls0_lgb                                     0.574573   0.634385  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows=100\n",
    "pd.options.display.max_columns=100\n",
    "train[basic_columns].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503424 entries, 0 to 1503423\n",
      "Data columns (total 464 columns):\n",
      "lgb411_tune                                                           float64\n",
      "plants_lgb                                                            float64\n",
      "plants_with_img_meta_nima_fm_geo_active_lgb                           float64\n",
      "xentropy_small_lr_cat_lgb                                             float64\n",
      "simple_feature_lgb                                                    float64\n",
      "all_mean_enc_lgb                                                      float64\n",
      "all_mean_enc_user_feat_lgb                                            float64\n",
      "all_mean_enc_user_feat2_lgb                                           float64\n",
      "cat_interact_lgb                                                      float64\n",
      "mean_enc_lgb                                                          float64\n",
      "marcus_lgb                                                            float64\n",
      "fused_text_lgb                                                        float64\n",
      "mixed_features_text_proprocessing_lgb                                 float64\n",
      "select_dense_features_lgb                                             float64\n",
      "select_sparse_features_lgb                                            float64\n",
      "lgb_mean                                                              float64\n",
      "lgb_med                                                               float64\n",
      "lgb_max                                                               float64\n",
      "lgb_min                                                               float64\n",
      "lgb_std                                                               float64\n",
      "lgb411_dart_tune                                                      float64\n",
      "poisson_lgb                                                           float64\n",
      "small_features_v5_xgb                                                 float64\n",
      "small_features_v4_xgb                                                 float64\n",
      "nima_features_xgb                                                     float64\n",
      "img_meta_xgb                                                          float64\n",
      "img_meta_nima_xgb                                                     float64\n",
      "xgb_lg_mean                                                           float64\n",
      "xgb_lg_med                                                            float64\n",
      "xgb_lg_max                                                            float64\n",
      "xgb_lg_min                                                            float64\n",
      "xgb_lg_std                                                            float64\n",
      "baseline_xgb                                                          float64\n",
      "ranking_xgb                                                           float64\n",
      "catboost                                                              float64\n",
      "catboost1_without_text                                                float64\n",
      "mcl_cgb                                                               float64\n",
      "catboost_mean                                                         float64\n",
      "catboost_med                                                          float64\n",
      "catboost_max                                                          float64\n",
      "catboost_min                                                          float64\n",
      "catboost_std                                                          float64\n",
      "pretrained_bigru_cv1d_rnn                                             float64\n",
      "pretrained_bigru_attention_rnn                                        float64\n",
      "pretrained_2gru_rnn                                                   float64\n",
      "selftrained_bigru_conv1d_rnn                                          float64\n",
      "rnn_mean                                                              float64\n",
      "rnn_med                                                               float64\n",
      "rnn_max                                                               float64\n",
      "rnn_min                                                               float64\n",
      "rnn_std                                                               float64\n",
      "text_lgb                                                              float64\n",
      "text_cwb_rg                                                           float64\n",
      "text_fm                                                               float64\n",
      "text_rg                                                               float64\n",
      "text_mean                                                             float64\n",
      "text_med                                                              float64\n",
      "text_max                                                              float64\n",
      "text_min                                                              float64\n",
      "text_std                                                              float64\n",
      "mlp                                                                   float64\n",
      "alpha_0001                                                            float64\n",
      "alpha_160                                                             float64\n",
      "alpha_10                                                              float64\n",
      "alpha_320                                                             float64\n",
      "regression_other_mean                                                 float64\n",
      "regression_other_med                                                  float64\n",
      "regression_other_max                                                  float64\n",
      "regression_other_min                                                  float64\n",
      "regression_other_std                                                  float64\n",
      "lr_l1_05                                                              float64\n",
      "lr_l1_1                                                               float64\n",
      "lr_l2_01                                                              float64\n",
      "lr_l2_1                                                               float64\n",
      "cls05_lgb                                                             float64\n",
      "classfication_other_mean                                              float64\n",
      "classfication_other_med                                               float64\n",
      "classfication_other_max                                               float64\n",
      "classfication_other_min                                               float64\n",
      "classfication_other_std                                               float64\n",
      "cls0_lgb                                                              float64\n",
      "multiclass_lgbmulticlass_lgb_pred0                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred1                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred2                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred3                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred4                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred5                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred6                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred7                                    float64\n",
      "multiclass_lgbmulticlass_lgb_pred8                                    float64\n",
      "multiclass_lgbmulticlass_lgbrank                                      int64\n",
      "multiclass3_lgbmulticlass3_lgb_pred0                                  float64\n",
      "multiclass3_lgbmulticlass3_lgb_pred1                                  float64\n",
      "multiclass3_lgbmulticlass3_lgb_pred2                                  float64\n",
      "multiclass3_lgbmulticlass3_lgbrank                                    int64\n",
      "inter_group_mean                                                      float64\n",
      "inter_group_med                                                       float64\n",
      "inter_group_max                                                       float64\n",
      "inter_group_min                                                       float64\n",
      "inter_group_std                                                       float64\n",
      "lgb411_tune_img_meta_xgb_inter_mean                                   float64\n",
      "lgb411_tune_img_meta_xgb_inter_gmean                                  float64\n",
      "lgb411_tune_img_meta_xgb_inter_hmean                                  float64\n",
      "lgb411_tune_baseline_xgb_inter_mean                                   float64\n",
      "lgb411_tune_baseline_xgb_inter_gmean                                  float64\n",
      "lgb411_tune_baseline_xgb_inter_hmean                                  float64\n",
      "lgb411_tune_mcl_cgb_inter_mean                                        float64\n",
      "lgb411_tune_mcl_cgb_inter_gmean                                       float64\n",
      "lgb411_tune_mcl_cgb_inter_hmean                                       float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_mean                   float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_gmean                  float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_hmean                  float64\n",
      "lgb411_tune_text_lgb_inter_mean                                       float64\n",
      "lgb411_tune_text_lgb_inter_gmean                                      float64\n",
      "lgb411_tune_text_lgb_inter_hmean                                      float64\n",
      "lgb411_tune_mlp_inter_mean                                            float64\n",
      "lgb411_tune_mlp_inter_gmean                                           float64\n",
      "lgb411_tune_mlp_inter_hmean                                           float64\n",
      "lgb411_tune_lr_l2_01_inter_mean                                       float64\n",
      "lgb411_tune_lr_l2_01_inter_gmean                                      float64\n",
      "lgb411_tune_lr_l2_01_inter_hmean                                      float64\n",
      "img_meta_xgb_baseline_xgb_inter_mean                                  float64\n",
      "img_meta_xgb_baseline_xgb_inter_gmean                                 float64\n",
      "img_meta_xgb_baseline_xgb_inter_hmean                                 float64\n",
      "img_meta_xgb_mcl_cgb_inter_mean                                       float64\n",
      "img_meta_xgb_mcl_cgb_inter_gmean                                      float64\n",
      "img_meta_xgb_mcl_cgb_inter_hmean                                      float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_mean                  float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_gmean                 float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_hmean                 float64\n",
      "img_meta_xgb_text_lgb_inter_mean                                      float64\n",
      "img_meta_xgb_text_lgb_inter_gmean                                     float64\n",
      "img_meta_xgb_text_lgb_inter_hmean                                     float64\n",
      "img_meta_xgb_mlp_inter_mean                                           float64\n",
      "img_meta_xgb_mlp_inter_gmean                                          float64\n",
      "img_meta_xgb_mlp_inter_hmean                                          float64\n",
      "img_meta_xgb_lr_l2_01_inter_mean                                      float64\n",
      "img_meta_xgb_lr_l2_01_inter_gmean                                     float64\n",
      "img_meta_xgb_lr_l2_01_inter_hmean                                     float64\n",
      "baseline_xgb_mcl_cgb_inter_mean                                       float64\n",
      "baseline_xgb_mcl_cgb_inter_gmean                                      float64\n",
      "baseline_xgb_mcl_cgb_inter_hmean                                      float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean                  float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean                 float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean                 float64\n",
      "baseline_xgb_text_lgb_inter_mean                                      float64\n",
      "baseline_xgb_text_lgb_inter_gmean                                     float64\n",
      "baseline_xgb_text_lgb_inter_hmean                                     float64\n",
      "baseline_xgb_mlp_inter_mean                                           float64\n",
      "baseline_xgb_mlp_inter_gmean                                          float64\n",
      "baseline_xgb_mlp_inter_hmean                                          float64\n",
      "baseline_xgb_lr_l2_01_inter_mean                                      float64\n",
      "baseline_xgb_lr_l2_01_inter_gmean                                     float64\n",
      "baseline_xgb_lr_l2_01_inter_hmean                                     float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean                       float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean                      float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean                      float64\n",
      "mcl_cgb_text_lgb_inter_mean                                           float64\n",
      "mcl_cgb_text_lgb_inter_gmean                                          float64\n",
      "mcl_cgb_text_lgb_inter_hmean                                          float64\n",
      "mcl_cgb_mlp_inter_mean                                                float64\n",
      "mcl_cgb_mlp_inter_gmean                                               float64\n",
      "mcl_cgb_mlp_inter_hmean                                               float64\n",
      "mcl_cgb_lr_l2_01_inter_mean                                           float64\n",
      "mcl_cgb_lr_l2_01_inter_gmean                                          float64\n",
      "mcl_cgb_lr_l2_01_inter_hmean                                          float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_mean                      float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean                     float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean                     float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_mean                           float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_gmean                          float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_hmean                          float64\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean                      float64\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean                     float64\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean                     float64\n",
      "text_lgb_mlp_inter_mean                                               float64\n",
      "text_lgb_mlp_inter_gmean                                              float64\n",
      "text_lgb_mlp_inter_hmean                                              float64\n",
      "text_lgb_lr_l2_01_inter_mean                                          float64\n",
      "text_lgb_lr_l2_01_inter_gmean                                         float64\n",
      "text_lgb_lr_l2_01_inter_hmean                                         float64\n",
      "mlp_lr_l2_01_inter_mean                                               float64\n",
      "mlp_lr_l2_01_inter_gmean                                              float64\n",
      "mlp_lr_l2_01_inter_hmean                                              float64\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_mean                      float64\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_gmean                     float64\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_hmean                     float64\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_med                       float64\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_std                       float64\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_mean                           float64\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_gmean                          float64\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_hmean                          float64\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_med                            float64\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_std                            float64\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_mean      float64\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_gmean     float64\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_hmean     float64\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_med       float64\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_std       float64\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_mean                          float64\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_gmean                         float64\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_hmean                         float64\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_med                           float64\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_std                           float64\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_mean                               float64\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_gmean                              float64\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_hmean                              float64\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_med                                float64\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_std                                float64\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_mean                          float64\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_gmean                         float64\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_hmean                         float64\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_med                           float64\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_std                           float64\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_mean                           float64\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_gmean                          float64\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_hmean                          float64\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_med                            float64\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_std                            float64\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean      float64\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean     float64\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean     float64\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_med       float64\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_std       float64\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_mean                          float64\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_gmean                         float64\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_hmean                         float64\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_med                           float64\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_std                           float64\n",
      "lgb411_tune_baseline_xgb_mlp_inter_mean                               float64\n",
      "lgb411_tune_baseline_xgb_mlp_inter_gmean                              float64\n",
      "lgb411_tune_baseline_xgb_mlp_inter_hmean                              float64\n",
      "lgb411_tune_baseline_xgb_mlp_inter_med                                float64\n",
      "lgb411_tune_baseline_xgb_mlp_inter_std                                float64\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_mean                          float64\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_gmean                         float64\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_hmean                         float64\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_med                           float64\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_std                           float64\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean           float64\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean          float64\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean          float64\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med            float64\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std            float64\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_mean                               float64\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_gmean                              float64\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_hmean                              float64\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_med                                float64\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_std                                float64\n",
      "lgb411_tune_mcl_cgb_mlp_inter_mean                                    float64\n",
      "lgb411_tune_mcl_cgb_mlp_inter_gmean                                   float64\n",
      "lgb411_tune_mcl_cgb_mlp_inter_hmean                                   float64\n",
      "lgb411_tune_mcl_cgb_mlp_inter_med                                     float64\n",
      "lgb411_tune_mcl_cgb_mlp_inter_std                                     float64\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_mean                               float64\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_gmean                              float64\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_hmean                              float64\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_med                                float64\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_std                                float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean          float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean         float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean         float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_med           float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_std           float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_mean               float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_gmean              float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_hmean              float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_med                float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_std                float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean          float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean         float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean         float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med           float64\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std           float64\n",
      "lgb411_tune_text_lgb_mlp_inter_mean                                   float64\n",
      "lgb411_tune_text_lgb_mlp_inter_gmean                                  float64\n",
      "lgb411_tune_text_lgb_mlp_inter_hmean                                  float64\n",
      "lgb411_tune_text_lgb_mlp_inter_med                                    float64\n",
      "lgb411_tune_text_lgb_mlp_inter_std                                    float64\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_mean                              float64\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_gmean                             float64\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_hmean                             float64\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_med                               float64\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_std                               float64\n",
      "lgb411_tune_mlp_lr_l2_01_inter_mean                                   float64\n",
      "lgb411_tune_mlp_lr_l2_01_inter_gmean                                  float64\n",
      "lgb411_tune_mlp_lr_l2_01_inter_hmean                                  float64\n",
      "lgb411_tune_mlp_lr_l2_01_inter_med                                    float64\n",
      "lgb411_tune_mlp_lr_l2_01_inter_std                                    float64\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_mean                          float64\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_gmean                         float64\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_hmean                         float64\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_med                           float64\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_std                           float64\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean     float64\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean    float64\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean    float64\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_med      float64\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_std      float64\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_mean                         float64\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_gmean                        float64\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_hmean                        float64\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_med                          float64\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_std                          float64\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_mean                              float64\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_gmean                             float64\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_hmean                             float64\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_med                               float64\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_std                               float64\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_mean                         float64\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_gmean                        float64\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_hmean                        float64\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_med                          float64\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_std                          float64\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean          float64\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean         float64\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean         float64\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med           float64\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std           float64\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_mean                              float64\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_gmean                             float64\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_hmean                             float64\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_med                               float64\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_std                               float64\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_mean                                   float64\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_gmean                                  float64\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_hmean                                  float64\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_med                                    float64\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_std                                    float64\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_mean                              float64\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_gmean                             float64\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_hmean                             float64\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_med                               float64\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_std                               float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean         float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean        float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean        float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med          float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std          float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean              float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean             float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean             float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_med               float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_std               float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean         float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean        float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean        float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med          float64\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std          float64\n",
      "img_meta_xgb_text_lgb_mlp_inter_mean                                  float64\n",
      "img_meta_xgb_text_lgb_mlp_inter_gmean                                 float64\n",
      "img_meta_xgb_text_lgb_mlp_inter_hmean                                 float64\n",
      "img_meta_xgb_text_lgb_mlp_inter_med                                   float64\n",
      "img_meta_xgb_text_lgb_mlp_inter_std                                   float64\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_mean                             float64\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_gmean                            float64\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_hmean                            float64\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_med                              float64\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_std                              float64\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_mean                                  float64\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_gmean                                 float64\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_hmean                                 float64\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_med                                   float64\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_std                                   float64\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean          float64\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean         float64\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean         float64\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med           float64\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std           float64\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_mean                              float64\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_gmean                             float64\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_hmean                             float64\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_med                               float64\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_std                               float64\n",
      "baseline_xgb_mcl_cgb_mlp_inter_mean                                   float64\n",
      "baseline_xgb_mcl_cgb_mlp_inter_gmean                                  float64\n",
      "baseline_xgb_mcl_cgb_mlp_inter_hmean                                  float64\n",
      "baseline_xgb_mcl_cgb_mlp_inter_med                                    float64\n",
      "baseline_xgb_mcl_cgb_mlp_inter_std                                    float64\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_mean                              float64\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_gmean                             float64\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_hmean                             float64\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_med                               float64\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_std                               float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean         float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean        float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean        float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med          float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std          float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean              float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean             float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean             float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_med               float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_std               float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean         float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean        float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean        float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med          float64\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std          float64\n",
      "baseline_xgb_text_lgb_mlp_inter_mean                                  float64\n",
      "baseline_xgb_text_lgb_mlp_inter_gmean                                 float64\n",
      "baseline_xgb_text_lgb_mlp_inter_hmean                                 float64\n",
      "baseline_xgb_text_lgb_mlp_inter_med                                   float64\n",
      "baseline_xgb_text_lgb_mlp_inter_std                                   float64\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_mean                             float64\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_gmean                            float64\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_hmean                            float64\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_med                              float64\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_std                              float64\n",
      "baseline_xgb_mlp_lr_l2_01_inter_mean                                  float64\n",
      "baseline_xgb_mlp_lr_l2_01_inter_gmean                                 float64\n",
      "baseline_xgb_mlp_lr_l2_01_inter_hmean                                 float64\n",
      "baseline_xgb_mlp_lr_l2_01_inter_med                                   float64\n",
      "baseline_xgb_mlp_lr_l2_01_inter_std                                   float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean              float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean             float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean             float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med               float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std               float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean                   float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean                  float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean                  float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_med                    float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_std                    float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean              float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean             float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean             float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med               float64\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std               float64\n",
      "mcl_cgb_text_lgb_mlp_inter_mean                                       float64\n",
      "mcl_cgb_text_lgb_mlp_inter_gmean                                      float64\n",
      "mcl_cgb_text_lgb_mlp_inter_hmean                                      float64\n",
      "mcl_cgb_text_lgb_mlp_inter_med                                        float64\n",
      "mcl_cgb_text_lgb_mlp_inter_std                                        float64\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_mean                                  float64\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_gmean                                 float64\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_hmean                                 float64\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_med                                   float64\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_std                                   float64\n",
      "mcl_cgb_mlp_lr_l2_01_inter_mean                                       float64\n",
      "mcl_cgb_mlp_lr_l2_01_inter_gmean                                      float64\n",
      "mcl_cgb_mlp_lr_l2_01_inter_hmean                                      float64\n",
      "mcl_cgb_mlp_lr_l2_01_inter_med                                        float64\n",
      "mcl_cgb_mlp_lr_l2_01_inter_std                                        float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_mean                  float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_gmean                 float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_hmean                 float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_med                   float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_std                   float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_mean             float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_gmean            float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_hmean            float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_med              float64\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_std              float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_mean                  float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_gmean                 float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_hmean                 float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_med                   float64\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_std                   float64\n",
      "text_lgb_mlp_lr_l2_01_inter_mean                                      float64\n",
      "text_lgb_mlp_lr_l2_01_inter_gmean                                     float64\n",
      "text_lgb_mlp_lr_l2_01_inter_hmean                                     float64\n",
      "text_lgb_mlp_lr_l2_01_inter_med                                       float64\n",
      "text_lgb_mlp_lr_l2_01_inter_std                                       float64\n",
      "dtypes: float64(462), int64(2)\n",
      "memory usage: 5.2 GB\n"
     ]
    }
   ],
   "source": [
    "train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1503424, 464), (508438, 464))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum().max()) \n",
    "print(test.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 464/464 [13:47<00:00,  1.78s/it]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(train.columns):\n",
    "    train[col] = train[col].astype(np.float32)\n",
    "    test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503424 entries, 0 to 1503423\n",
      "Data columns (total 464 columns):\n",
      "lgb411_tune                                                           float32\n",
      "plants_lgb                                                            float32\n",
      "plants_with_img_meta_nima_fm_geo_active_lgb                           float32\n",
      "xentropy_small_lr_cat_lgb                                             float32\n",
      "simple_feature_lgb                                                    float32\n",
      "all_mean_enc_lgb                                                      float32\n",
      "all_mean_enc_user_feat_lgb                                            float32\n",
      "all_mean_enc_user_feat2_lgb                                           float32\n",
      "cat_interact_lgb                                                      float32\n",
      "mean_enc_lgb                                                          float32\n",
      "marcus_lgb                                                            float32\n",
      "fused_text_lgb                                                        float32\n",
      "mixed_features_text_proprocessing_lgb                                 float32\n",
      "select_dense_features_lgb                                             float32\n",
      "select_sparse_features_lgb                                            float32\n",
      "lgb_mean                                                              float32\n",
      "lgb_med                                                               float32\n",
      "lgb_max                                                               float32\n",
      "lgb_min                                                               float32\n",
      "lgb_std                                                               float32\n",
      "lgb411_dart_tune                                                      float32\n",
      "poisson_lgb                                                           float32\n",
      "small_features_v5_xgb                                                 float32\n",
      "small_features_v4_xgb                                                 float32\n",
      "nima_features_xgb                                                     float32\n",
      "img_meta_xgb                                                          float32\n",
      "img_meta_nima_xgb                                                     float32\n",
      "xgb_lg_mean                                                           float32\n",
      "xgb_lg_med                                                            float32\n",
      "xgb_lg_max                                                            float32\n",
      "xgb_lg_min                                                            float32\n",
      "xgb_lg_std                                                            float32\n",
      "baseline_xgb                                                          float32\n",
      "ranking_xgb                                                           float32\n",
      "catboost                                                              float32\n",
      "catboost1_without_text                                                float32\n",
      "mcl_cgb                                                               float32\n",
      "catboost_mean                                                         float32\n",
      "catboost_med                                                          float32\n",
      "catboost_max                                                          float32\n",
      "catboost_min                                                          float32\n",
      "catboost_std                                                          float32\n",
      "pretrained_bigru_cv1d_rnn                                             float32\n",
      "pretrained_bigru_attention_rnn                                        float32\n",
      "pretrained_2gru_rnn                                                   float32\n",
      "selftrained_bigru_conv1d_rnn                                          float32\n",
      "rnn_mean                                                              float32\n",
      "rnn_med                                                               float32\n",
      "rnn_max                                                               float32\n",
      "rnn_min                                                               float32\n",
      "rnn_std                                                               float32\n",
      "text_lgb                                                              float32\n",
      "text_cwb_rg                                                           float32\n",
      "text_fm                                                               float32\n",
      "text_rg                                                               float32\n",
      "text_mean                                                             float32\n",
      "text_med                                                              float32\n",
      "text_max                                                              float32\n",
      "text_min                                                              float32\n",
      "text_std                                                              float32\n",
      "mlp                                                                   float32\n",
      "alpha_0001                                                            float32\n",
      "alpha_160                                                             float32\n",
      "alpha_10                                                              float32\n",
      "alpha_320                                                             float32\n",
      "regression_other_mean                                                 float32\n",
      "regression_other_med                                                  float32\n",
      "regression_other_max                                                  float32\n",
      "regression_other_min                                                  float32\n",
      "regression_other_std                                                  float32\n",
      "lr_l1_05                                                              float32\n",
      "lr_l1_1                                                               float32\n",
      "lr_l2_01                                                              float32\n",
      "lr_l2_1                                                               float32\n",
      "cls05_lgb                                                             float32\n",
      "classfication_other_mean                                              float32\n",
      "classfication_other_med                                               float32\n",
      "classfication_other_max                                               float32\n",
      "classfication_other_min                                               float32\n",
      "classfication_other_std                                               float32\n",
      "cls0_lgb                                                              float32\n",
      "multiclass_lgbmulticlass_lgb_pred0                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred1                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred2                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred3                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred4                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred5                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred6                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred7                                    float32\n",
      "multiclass_lgbmulticlass_lgb_pred8                                    float32\n",
      "multiclass_lgbmulticlass_lgbrank                                      float32\n",
      "multiclass3_lgbmulticlass3_lgb_pred0                                  float32\n",
      "multiclass3_lgbmulticlass3_lgb_pred1                                  float32\n",
      "multiclass3_lgbmulticlass3_lgb_pred2                                  float32\n",
      "multiclass3_lgbmulticlass3_lgbrank                                    float32\n",
      "inter_group_mean                                                      float32\n",
      "inter_group_med                                                       float32\n",
      "inter_group_max                                                       float32\n",
      "inter_group_min                                                       float32\n",
      "inter_group_std                                                       float32\n",
      "lgb411_tune_img_meta_xgb_inter_mean                                   float32\n",
      "lgb411_tune_img_meta_xgb_inter_gmean                                  float32\n",
      "lgb411_tune_img_meta_xgb_inter_hmean                                  float32\n",
      "lgb411_tune_baseline_xgb_inter_mean                                   float32\n",
      "lgb411_tune_baseline_xgb_inter_gmean                                  float32\n",
      "lgb411_tune_baseline_xgb_inter_hmean                                  float32\n",
      "lgb411_tune_mcl_cgb_inter_mean                                        float32\n",
      "lgb411_tune_mcl_cgb_inter_gmean                                       float32\n",
      "lgb411_tune_mcl_cgb_inter_hmean                                       float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_mean                   float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_gmean                  float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_inter_hmean                  float32\n",
      "lgb411_tune_text_lgb_inter_mean                                       float32\n",
      "lgb411_tune_text_lgb_inter_gmean                                      float32\n",
      "lgb411_tune_text_lgb_inter_hmean                                      float32\n",
      "lgb411_tune_mlp_inter_mean                                            float32\n",
      "lgb411_tune_mlp_inter_gmean                                           float32\n",
      "lgb411_tune_mlp_inter_hmean                                           float32\n",
      "lgb411_tune_lr_l2_01_inter_mean                                       float32\n",
      "lgb411_tune_lr_l2_01_inter_gmean                                      float32\n",
      "lgb411_tune_lr_l2_01_inter_hmean                                      float32\n",
      "img_meta_xgb_baseline_xgb_inter_mean                                  float32\n",
      "img_meta_xgb_baseline_xgb_inter_gmean                                 float32\n",
      "img_meta_xgb_baseline_xgb_inter_hmean                                 float32\n",
      "img_meta_xgb_mcl_cgb_inter_mean                                       float32\n",
      "img_meta_xgb_mcl_cgb_inter_gmean                                      float32\n",
      "img_meta_xgb_mcl_cgb_inter_hmean                                      float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_mean                  float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_gmean                 float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_hmean                 float32\n",
      "img_meta_xgb_text_lgb_inter_mean                                      float32\n",
      "img_meta_xgb_text_lgb_inter_gmean                                     float32\n",
      "img_meta_xgb_text_lgb_inter_hmean                                     float32\n",
      "img_meta_xgb_mlp_inter_mean                                           float32\n",
      "img_meta_xgb_mlp_inter_gmean                                          float32\n",
      "img_meta_xgb_mlp_inter_hmean                                          float32\n",
      "img_meta_xgb_lr_l2_01_inter_mean                                      float32\n",
      "img_meta_xgb_lr_l2_01_inter_gmean                                     float32\n",
      "img_meta_xgb_lr_l2_01_inter_hmean                                     float32\n",
      "baseline_xgb_mcl_cgb_inter_mean                                       float32\n",
      "baseline_xgb_mcl_cgb_inter_gmean                                      float32\n",
      "baseline_xgb_mcl_cgb_inter_hmean                                      float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean                  float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean                 float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean                 float32\n",
      "baseline_xgb_text_lgb_inter_mean                                      float32\n",
      "baseline_xgb_text_lgb_inter_gmean                                     float32\n",
      "baseline_xgb_text_lgb_inter_hmean                                     float32\n",
      "baseline_xgb_mlp_inter_mean                                           float32\n",
      "baseline_xgb_mlp_inter_gmean                                          float32\n",
      "baseline_xgb_mlp_inter_hmean                                          float32\n",
      "baseline_xgb_lr_l2_01_inter_mean                                      float32\n",
      "baseline_xgb_lr_l2_01_inter_gmean                                     float32\n",
      "baseline_xgb_lr_l2_01_inter_hmean                                     float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean                       float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean                      float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean                      float32\n",
      "mcl_cgb_text_lgb_inter_mean                                           float32\n",
      "mcl_cgb_text_lgb_inter_gmean                                          float32\n",
      "mcl_cgb_text_lgb_inter_hmean                                          float32\n",
      "mcl_cgb_mlp_inter_mean                                                float32\n",
      "mcl_cgb_mlp_inter_gmean                                               float32\n",
      "mcl_cgb_mlp_inter_hmean                                               float32\n",
      "mcl_cgb_lr_l2_01_inter_mean                                           float32\n",
      "mcl_cgb_lr_l2_01_inter_gmean                                          float32\n",
      "mcl_cgb_lr_l2_01_inter_hmean                                          float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_mean                      float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean                     float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean                     float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_mean                           float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_gmean                          float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_inter_hmean                          float32\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean                      float32\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean                     float32\n",
      "selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean                     float32\n",
      "text_lgb_mlp_inter_mean                                               float32\n",
      "text_lgb_mlp_inter_gmean                                              float32\n",
      "text_lgb_mlp_inter_hmean                                              float32\n",
      "text_lgb_lr_l2_01_inter_mean                                          float32\n",
      "text_lgb_lr_l2_01_inter_gmean                                         float32\n",
      "text_lgb_lr_l2_01_inter_hmean                                         float32\n",
      "mlp_lr_l2_01_inter_mean                                               float32\n",
      "mlp_lr_l2_01_inter_gmean                                              float32\n",
      "mlp_lr_l2_01_inter_hmean                                              float32\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_mean                      float32\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_gmean                     float32\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_hmean                     float32\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_med                       float32\n",
      "lgb411_tune_img_meta_xgb_baseline_xgb_inter_std                       float32\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_mean                           float32\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_gmean                          float32\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_hmean                          float32\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_med                            float32\n",
      "lgb411_tune_img_meta_xgb_mcl_cgb_inter_std                            float32\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_mean      float32\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_gmean     float32\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_hmean     float32\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_med       float32\n",
      "lgb411_tune_img_meta_xgb_selftrained_bigru_conv1d_rnn_inter_std       float32\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_mean                          float32\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_gmean                         float32\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_hmean                         float32\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_med                           float32\n",
      "lgb411_tune_img_meta_xgb_text_lgb_inter_std                           float32\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_mean                               float32\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_gmean                              float32\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_hmean                              float32\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_med                                float32\n",
      "lgb411_tune_img_meta_xgb_mlp_inter_std                                float32\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_mean                          float32\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_gmean                         float32\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_hmean                         float32\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_med                           float32\n",
      "lgb411_tune_img_meta_xgb_lr_l2_01_inter_std                           float32\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_mean                           float32\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_gmean                          float32\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_hmean                          float32\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_med                            float32\n",
      "lgb411_tune_baseline_xgb_mcl_cgb_inter_std                            float32\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean      float32\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean     float32\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean     float32\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_med       float32\n",
      "lgb411_tune_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_std       float32\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_mean                          float32\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_gmean                         float32\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_hmean                         float32\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_med                           float32\n",
      "lgb411_tune_baseline_xgb_text_lgb_inter_std                           float32\n",
      "lgb411_tune_baseline_xgb_mlp_inter_mean                               float32\n",
      "lgb411_tune_baseline_xgb_mlp_inter_gmean                              float32\n",
      "lgb411_tune_baseline_xgb_mlp_inter_hmean                              float32\n",
      "lgb411_tune_baseline_xgb_mlp_inter_med                                float32\n",
      "lgb411_tune_baseline_xgb_mlp_inter_std                                float32\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_mean                          float32\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_gmean                         float32\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_hmean                         float32\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_med                           float32\n",
      "lgb411_tune_baseline_xgb_lr_l2_01_inter_std                           float32\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean           float32\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean          float32\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean          float32\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med            float32\n",
      "lgb411_tune_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std            float32\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_mean                               float32\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_gmean                              float32\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_hmean                              float32\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_med                                float32\n",
      "lgb411_tune_mcl_cgb_text_lgb_inter_std                                float32\n",
      "lgb411_tune_mcl_cgb_mlp_inter_mean                                    float32\n",
      "lgb411_tune_mcl_cgb_mlp_inter_gmean                                   float32\n",
      "lgb411_tune_mcl_cgb_mlp_inter_hmean                                   float32\n",
      "lgb411_tune_mcl_cgb_mlp_inter_med                                     float32\n",
      "lgb411_tune_mcl_cgb_mlp_inter_std                                     float32\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_mean                               float32\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_gmean                              float32\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_hmean                              float32\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_med                                float32\n",
      "lgb411_tune_mcl_cgb_lr_l2_01_inter_std                                float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean          float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean         float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean         float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_med           float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_text_lgb_inter_std           float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_mean               float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_gmean              float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_hmean              float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_med                float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_mlp_inter_std                float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean          float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean         float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean         float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med           float32\n",
      "lgb411_tune_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std           float32\n",
      "lgb411_tune_text_lgb_mlp_inter_mean                                   float32\n",
      "lgb411_tune_text_lgb_mlp_inter_gmean                                  float32\n",
      "lgb411_tune_text_lgb_mlp_inter_hmean                                  float32\n",
      "lgb411_tune_text_lgb_mlp_inter_med                                    float32\n",
      "lgb411_tune_text_lgb_mlp_inter_std                                    float32\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_mean                              float32\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_gmean                             float32\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_hmean                             float32\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_med                               float32\n",
      "lgb411_tune_text_lgb_lr_l2_01_inter_std                               float32\n",
      "lgb411_tune_mlp_lr_l2_01_inter_mean                                   float32\n",
      "lgb411_tune_mlp_lr_l2_01_inter_gmean                                  float32\n",
      "lgb411_tune_mlp_lr_l2_01_inter_hmean                                  float32\n",
      "lgb411_tune_mlp_lr_l2_01_inter_med                                    float32\n",
      "lgb411_tune_mlp_lr_l2_01_inter_std                                    float32\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_mean                          float32\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_gmean                         float32\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_hmean                         float32\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_med                           float32\n",
      "img_meta_xgb_baseline_xgb_mcl_cgb_inter_std                           float32\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_mean     float32\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_gmean    float32\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_hmean    float32\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_med      float32\n",
      "img_meta_xgb_baseline_xgb_selftrained_bigru_conv1d_rnn_inter_std      float32\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_mean                         float32\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_gmean                        float32\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_hmean                        float32\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_med                          float32\n",
      "img_meta_xgb_baseline_xgb_text_lgb_inter_std                          float32\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_mean                              float32\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_gmean                             float32\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_hmean                             float32\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_med                               float32\n",
      "img_meta_xgb_baseline_xgb_mlp_inter_std                               float32\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_mean                         float32\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_gmean                        float32\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_hmean                        float32\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_med                          float32\n",
      "img_meta_xgb_baseline_xgb_lr_l2_01_inter_std                          float32\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean          float32\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean         float32\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean         float32\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med           float32\n",
      "img_meta_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std           float32\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_mean                              float32\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_gmean                             float32\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_hmean                             float32\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_med                               float32\n",
      "img_meta_xgb_mcl_cgb_text_lgb_inter_std                               float32\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_mean                                   float32\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_gmean                                  float32\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_hmean                                  float32\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_med                                    float32\n",
      "img_meta_xgb_mcl_cgb_mlp_inter_std                                    float32\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_mean                              float32\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_gmean                             float32\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_hmean                             float32\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_med                               float32\n",
      "img_meta_xgb_mcl_cgb_lr_l2_01_inter_std                               float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean         float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean        float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean        float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med          float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std          float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean              float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean             float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean             float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_med               float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_std               float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean         float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean        float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean        float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med          float32\n",
      "img_meta_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std          float32\n",
      "img_meta_xgb_text_lgb_mlp_inter_mean                                  float32\n",
      "img_meta_xgb_text_lgb_mlp_inter_gmean                                 float32\n",
      "img_meta_xgb_text_lgb_mlp_inter_hmean                                 float32\n",
      "img_meta_xgb_text_lgb_mlp_inter_med                                   float32\n",
      "img_meta_xgb_text_lgb_mlp_inter_std                                   float32\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_mean                             float32\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_gmean                            float32\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_hmean                            float32\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_med                              float32\n",
      "img_meta_xgb_text_lgb_lr_l2_01_inter_std                              float32\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_mean                                  float32\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_gmean                                 float32\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_hmean                                 float32\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_med                                   float32\n",
      "img_meta_xgb_mlp_lr_l2_01_inter_std                                   float32\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_mean          float32\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_gmean         float32\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_hmean         float32\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_med           float32\n",
      "baseline_xgb_mcl_cgb_selftrained_bigru_conv1d_rnn_inter_std           float32\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_mean                              float32\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_gmean                             float32\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_hmean                             float32\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_med                               float32\n",
      "baseline_xgb_mcl_cgb_text_lgb_inter_std                               float32\n",
      "baseline_xgb_mcl_cgb_mlp_inter_mean                                   float32\n",
      "baseline_xgb_mcl_cgb_mlp_inter_gmean                                  float32\n",
      "baseline_xgb_mcl_cgb_mlp_inter_hmean                                  float32\n",
      "baseline_xgb_mcl_cgb_mlp_inter_med                                    float32\n",
      "baseline_xgb_mcl_cgb_mlp_inter_std                                    float32\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_mean                              float32\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_gmean                             float32\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_hmean                             float32\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_med                               float32\n",
      "baseline_xgb_mcl_cgb_lr_l2_01_inter_std                               float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean         float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean        float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean        float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med          float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std          float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean              float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean             float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean             float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_med               float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_mlp_inter_std               float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean         float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean        float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean        float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med          float32\n",
      "baseline_xgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std          float32\n",
      "baseline_xgb_text_lgb_mlp_inter_mean                                  float32\n",
      "baseline_xgb_text_lgb_mlp_inter_gmean                                 float32\n",
      "baseline_xgb_text_lgb_mlp_inter_hmean                                 float32\n",
      "baseline_xgb_text_lgb_mlp_inter_med                                   float32\n",
      "baseline_xgb_text_lgb_mlp_inter_std                                   float32\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_mean                             float32\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_gmean                            float32\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_hmean                            float32\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_med                              float32\n",
      "baseline_xgb_text_lgb_lr_l2_01_inter_std                              float32\n",
      "baseline_xgb_mlp_lr_l2_01_inter_mean                                  float32\n",
      "baseline_xgb_mlp_lr_l2_01_inter_gmean                                 float32\n",
      "baseline_xgb_mlp_lr_l2_01_inter_hmean                                 float32\n",
      "baseline_xgb_mlp_lr_l2_01_inter_med                                   float32\n",
      "baseline_xgb_mlp_lr_l2_01_inter_std                                   float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_mean              float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_gmean             float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_hmean             float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_med               float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_text_lgb_inter_std               float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_mean                   float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_gmean                  float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_hmean                  float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_med                    float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_mlp_inter_std                    float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_mean              float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_gmean             float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_hmean             float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_med               float32\n",
      "mcl_cgb_selftrained_bigru_conv1d_rnn_lr_l2_01_inter_std               float32\n",
      "mcl_cgb_text_lgb_mlp_inter_mean                                       float32\n",
      "mcl_cgb_text_lgb_mlp_inter_gmean                                      float32\n",
      "mcl_cgb_text_lgb_mlp_inter_hmean                                      float32\n",
      "mcl_cgb_text_lgb_mlp_inter_med                                        float32\n",
      "mcl_cgb_text_lgb_mlp_inter_std                                        float32\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_mean                                  float32\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_gmean                                 float32\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_hmean                                 float32\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_med                                   float32\n",
      "mcl_cgb_text_lgb_lr_l2_01_inter_std                                   float32\n",
      "mcl_cgb_mlp_lr_l2_01_inter_mean                                       float32\n",
      "mcl_cgb_mlp_lr_l2_01_inter_gmean                                      float32\n",
      "mcl_cgb_mlp_lr_l2_01_inter_hmean                                      float32\n",
      "mcl_cgb_mlp_lr_l2_01_inter_med                                        float32\n",
      "mcl_cgb_mlp_lr_l2_01_inter_std                                        float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_mean                  float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_gmean                 float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_hmean                 float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_med                   float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_mlp_inter_std                   float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_mean             float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_gmean            float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_hmean            float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_med              float32\n",
      "selftrained_bigru_conv1d_rnn_text_lgb_lr_l2_01_inter_std              float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_mean                  float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_gmean                 float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_hmean                 float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_med                   float32\n",
      "selftrained_bigru_conv1d_rnn_mlp_lr_l2_01_inter_std                   float32\n",
      "text_lgb_mlp_lr_l2_01_inter_mean                                      float32\n",
      "text_lgb_mlp_lr_l2_01_inter_gmean                                     float32\n",
      "text_lgb_mlp_lr_l2_01_inter_hmean                                     float32\n",
      "text_lgb_mlp_lr_l2_01_inter_med                                       float32\n",
      "text_lgb_mlp_lr_l2_01_inter_std                                       float32\n",
      "dtypes: float32(464)\n",
      "memory usage: 2.6 GB\n"
     ]
    }
   ],
   "source": [
    "train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('meta_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(train, handle)\n",
    "    \n",
    "with open('meta_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(test, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gc; gc.enable()\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from GridSearcher import data_loader, model_loader, fit_params, get_oof_predictions, clip_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED=411\n",
    "train_y = pd.read_csv(\"regression_target.csv\").deal_probability.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('meta_train.pickle', 'rb') as handle:\n",
    "    train = pickle.load(handle)\n",
    "    \n",
    "with open('meta_test.pickle', 'rb') as handle:\n",
    "    test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm-gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_split_gain': 0.0} train loss: 0.208859, valid loss:0.209726, loss_diff:0.000867\n",
      "{'min_split_gain': 0.0} train loss: 0.208673, valid loss:0.210375, loss_diff:0.001702\n",
      "{'min_split_gain': 0.0} train loss: 0.208943, valid loss:0.209207, loss_diff:0.000264\n",
      "{'min_split_gain': 0.0} train loss: 0.208888, valid loss:0.209462, loss_diff:0.000574\n",
      "{'min_split_gain': 0.0} train loss: 0.208679, valid loss:0.210262, loss_diff:0.001583\n",
      "=================>{'min_split_gain': 0.0} loss:0.209807\n",
      "{'min_split_gain': 0.1} train loss: 0.208846, valid loss:0.209716, loss_diff:0.000869\n",
      "{'min_split_gain': 0.1} train loss: 0.208672, valid loss:0.210384, loss_diff:0.001713\n",
      "{'min_split_gain': 0.1} train loss: 0.208957, valid loss:0.209256, loss_diff:0.000299\n",
      "{'min_split_gain': 0.1} train loss: 0.208903, valid loss:0.209489, loss_diff:0.000586\n",
      "{'min_split_gain': 0.1} train loss: 0.208678, valid loss:0.210273, loss_diff:0.001595\n",
      "=================>{'min_split_gain': 0.1} loss:0.209823\n",
      "{'min_split_gain': 0.2} train loss: 0.208872, valid loss:0.209699, loss_diff:0.000827\n",
      "{'min_split_gain': 0.2} train loss: 0.208675, valid loss:0.210403, loss_diff:0.001728\n",
      "{'min_split_gain': 0.2} train loss: 0.208972, valid loss:0.209221, loss_diff:0.000249\n",
      "{'min_split_gain': 0.2} train loss: 0.208907, valid loss:0.209468, loss_diff:0.000561\n",
      "{'min_split_gain': 0.2} train loss: 0.208717, valid loss:0.210292, loss_diff:0.001576\n",
      "=================>{'min_split_gain': 0.2} loss:0.209817\n",
      "{'min_split_gain': 0.3} train loss: 0.208922, valid loss:0.209709, loss_diff:0.000787\n",
      "{'min_split_gain': 0.3} train loss: 0.208746, valid loss:0.210429, loss_diff:0.001682\n",
      "{'min_split_gain': 0.3} train loss: 0.209047, valid loss:0.209266, loss_diff:0.000219\n",
      "{'min_split_gain': 0.3} train loss: 0.208991, valid loss:0.209521, loss_diff:0.000530\n",
      "{'min_split_gain': 0.3} train loss: 0.208772, valid loss:0.210340, loss_diff:0.001568\n",
      "=================>{'min_split_gain': 0.3} loss:0.209853\n",
      "{'min_split_gain': 0.4} train loss: 0.208971, valid loss:0.209754, loss_diff:0.000783\n",
      "{'min_split_gain': 0.4} train loss: 0.208807, valid loss:0.210463, loss_diff:0.001656\n",
      "{'min_split_gain': 0.4} train loss: 0.209123, valid loss:0.209299, loss_diff:0.000176\n",
      "{'min_split_gain': 0.4} train loss: 0.209016, valid loss:0.209515, loss_diff:0.000499\n",
      "{'min_split_gain': 0.4} train loss: 0.208812, valid loss:0.210333, loss_diff:0.001521\n",
      "=================>{'min_split_gain': 0.4} loss:0.209873\n",
      "Best params: {'min_split_gain': 0.0} \tbest loss: 0.209806544932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'min_split_gain': 0.0}</td>\n",
       "      <td>0.209807</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'min_split_gain': 0.1}</td>\n",
       "      <td>0.209823</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'min_split_gain': 0.2}</td>\n",
       "      <td>0.209817</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'min_split_gain': 0.3}</td>\n",
       "      <td>0.209853</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'min_split_gain': 0.4}</td>\n",
       "      <td>0.209873</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     param  val_loss_mean  val_loss_std\n",
       "0  {'min_split_gain': 0.0}       0.209807      0.000450\n",
       "1  {'min_split_gain': 0.1}       0.209823      0.000439\n",
       "2  {'min_split_gain': 0.2}       0.209817      0.000461\n",
       "3  {'min_split_gain': 0.3}       0.209853      0.000457\n",
       "4  {'min_split_gain': 0.4}       0.209873      0.000454"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'min_split_gain': [.0, .1, .2, .3, .4]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6} train loss: 0.208955, valid loss:0.209728, loss_diff:0.000773\n",
      "{'colsample_bytree': 0.6} train loss: 0.208763, valid loss:0.210461, loss_diff:0.001699\n",
      "{'colsample_bytree': 0.6} train loss: 0.209065, valid loss:0.209305, loss_diff:0.000240\n",
      "{'colsample_bytree': 0.6} train loss: 0.209010, valid loss:0.209545, loss_diff:0.000536\n",
      "{'colsample_bytree': 0.6} train loss: 0.208774, valid loss:0.210333, loss_diff:0.001559\n",
      "=================>{'colsample_bytree': 0.6} loss:0.209875\n",
      "{'colsample_bytree': 0.7} train loss: 0.208914, valid loss:0.209713, loss_diff:0.000799\n",
      "{'colsample_bytree': 0.7} train loss: 0.208749, valid loss:0.210465, loss_diff:0.001716\n",
      "{'colsample_bytree': 0.7} train loss: 0.209005, valid loss:0.209262, loss_diff:0.000257\n",
      "{'colsample_bytree': 0.7} train loss: 0.208978, valid loss:0.209517, loss_diff:0.000539\n",
      "{'colsample_bytree': 0.7} train loss: 0.208781, valid loss:0.210323, loss_diff:0.001543\n",
      "=================>{'colsample_bytree': 0.7} loss:0.209856\n",
      "{'colsample_bytree': 0.8} train loss: 0.208892, valid loss:0.209695, loss_diff:0.000804\n",
      "{'colsample_bytree': 0.8} train loss: 0.208731, valid loss:0.210452, loss_diff:0.001721\n",
      "{'colsample_bytree': 0.8} train loss: 0.208992, valid loss:0.209289, loss_diff:0.000296\n",
      "{'colsample_bytree': 0.8} train loss: 0.208948, valid loss:0.209495, loss_diff:0.000547\n",
      "{'colsample_bytree': 0.8} train loss: 0.208736, valid loss:0.210299, loss_diff:0.001563\n",
      "=================>{'colsample_bytree': 0.8} loss:0.209846\n",
      "{'colsample_bytree': 0.9} train loss: 0.208876, valid loss:0.209722, loss_diff:0.000846\n",
      "{'colsample_bytree': 0.9} train loss: 0.208691, valid loss:0.210421, loss_diff:0.001730\n",
      "{'colsample_bytree': 0.9} train loss: 0.208995, valid loss:0.209267, loss_diff:0.000272\n",
      "{'colsample_bytree': 0.9} train loss: 0.208916, valid loss:0.209457, loss_diff:0.000541\n",
      "{'colsample_bytree': 0.9} train loss: 0.208704, valid loss:0.210270, loss_diff:0.001566\n",
      "=================>{'colsample_bytree': 0.9} loss:0.209827\n",
      "{'colsample_bytree': 1.0} train loss: 0.208859, valid loss:0.209726, loss_diff:0.000867\n",
      "{'colsample_bytree': 1.0} train loss: 0.208673, valid loss:0.210375, loss_diff:0.001702\n",
      "{'colsample_bytree': 1.0} train loss: 0.208943, valid loss:0.209207, loss_diff:0.000264\n",
      "{'colsample_bytree': 1.0} train loss: 0.208888, valid loss:0.209462, loss_diff:0.000574\n",
      "{'colsample_bytree': 1.0} train loss: 0.208679, valid loss:0.210262, loss_diff:0.001583\n",
      "=================>{'colsample_bytree': 1.0} loss:0.209807\n",
      "Best params: {'colsample_bytree': 1.0} \tbest loss: 0.209806544932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'colsample_bytree': 0.6}</td>\n",
       "      <td>0.209875</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'colsample_bytree': 0.7}</td>\n",
       "      <td>0.209856</td>\n",
       "      <td>0.000464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'colsample_bytree': 0.8}</td>\n",
       "      <td>0.209846</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'colsample_bytree': 0.9}</td>\n",
       "      <td>0.209827</td>\n",
       "      <td>0.000449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'colsample_bytree': 1.0}</td>\n",
       "      <td>0.209807</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       param  val_loss_mean  val_loss_std\n",
       "0  {'colsample_bytree': 0.6}       0.209875      0.000449\n",
       "1  {'colsample_bytree': 0.7}       0.209856      0.000464\n",
       "2  {'colsample_bytree': 0.8}       0.209846      0.000454\n",
       "3  {'colsample_bytree': 0.9}       0.209827      0.000449\n",
       "4  {'colsample_bytree': 1.0}       0.209807      0.000450"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.6} train loss: 0.208898, valid loss:0.209697, loss_diff:0.000800\n",
      "{'subsample': 0.6} train loss: 0.208723, valid loss:0.210410, loss_diff:0.001687\n",
      "{'subsample': 0.6} train loss: 0.208999, valid loss:0.209241, loss_diff:0.000241\n",
      "{'subsample': 0.6} train loss: 0.208936, valid loss:0.209458, loss_diff:0.000522\n",
      "{'subsample': 0.6} train loss: 0.208718, valid loss:0.210266, loss_diff:0.001548\n",
      "=================>{'subsample': 0.6} loss:0.209814\n",
      "{'subsample': 0.7} train loss: 0.208874, valid loss:0.209687, loss_diff:0.000814\n",
      "{'subsample': 0.7} train loss: 0.208664, valid loss:0.210372, loss_diff:0.001707\n",
      "{'subsample': 0.7} train loss: 0.208947, valid loss:0.209205, loss_diff:0.000258\n",
      "{'subsample': 0.7} train loss: 0.208899, valid loss:0.209434, loss_diff:0.000534\n",
      "{'subsample': 0.7} train loss: 0.208697, valid loss:0.210274, loss_diff:0.001577\n",
      "=================>{'subsample': 0.7} loss:0.209794\n",
      "{'subsample': 0.8} train loss: 0.208858, valid loss:0.209697, loss_diff:0.000838\n",
      "{'subsample': 0.8} train loss: 0.208686, valid loss:0.210366, loss_diff:0.001681\n",
      "{'subsample': 0.8} train loss: 0.208938, valid loss:0.209206, loss_diff:0.000268\n",
      "{'subsample': 0.8} train loss: 0.208926, valid loss:0.209447, loss_diff:0.000521\n",
      "{'subsample': 0.8} train loss: 0.208681, valid loss:0.210283, loss_diff:0.001601\n",
      "=================>{'subsample': 0.8} loss:0.209800\n",
      "{'subsample': 0.9} train loss: 0.208869, valid loss:0.209687, loss_diff:0.000818\n",
      "{'subsample': 0.9} train loss: 0.208653, valid loss:0.210401, loss_diff:0.001748\n",
      "{'subsample': 0.9} train loss: 0.208975, valid loss:0.209287, loss_diff:0.000312\n",
      "{'subsample': 0.9} train loss: 0.208887, valid loss:0.209429, loss_diff:0.000542\n",
      "{'subsample': 0.9} train loss: 0.208698, valid loss:0.210268, loss_diff:0.001570\n",
      "=================>{'subsample': 0.9} loss:0.209814\n",
      "{'subsample': 1.0} train loss: 0.208859, valid loss:0.209726, loss_diff:0.000867\n",
      "{'subsample': 1.0} train loss: 0.208673, valid loss:0.210375, loss_diff:0.001702\n",
      "{'subsample': 1.0} train loss: 0.208943, valid loss:0.209207, loss_diff:0.000264\n",
      "{'subsample': 1.0} train loss: 0.208888, valid loss:0.209462, loss_diff:0.000574\n",
      "{'subsample': 1.0} train loss: 0.208679, valid loss:0.210262, loss_diff:0.001583\n",
      "=================>{'subsample': 1.0} loss:0.209807\n",
      "Best params: {'subsample': 0.7} \tbest loss: 0.209794324578\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'subsample': 0.6}</td>\n",
       "      <td>0.209814</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'subsample': 0.7}</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'subsample': 0.8}</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'subsample': 0.9}</td>\n",
       "      <td>0.209814</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'subsample': 1.0}</td>\n",
       "      <td>0.209807</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param  val_loss_mean  val_loss_std\n",
       "0  {'subsample': 0.6}       0.209814      0.000454\n",
       "1  {'subsample': 0.7}       0.209794      0.000459\n",
       "2  {'subsample': 0.8}       0.209800      0.000456\n",
       "3  {'subsample': 0.9}       0.209814      0.000446\n",
       "4  {'subsample': 1.0}       0.209807      0.000450"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'subsample':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 1.0} train loss: 0.208835, valid loss:0.209693, loss_diff:0.000858\n",
      "{'reg_alpha': 1.0} train loss: 0.208651, valid loss:0.210392, loss_diff:0.001740\n",
      "{'reg_alpha': 1.0} train loss: 0.208926, valid loss:0.209249, loss_diff:0.000323\n",
      "{'reg_alpha': 1.0} train loss: 0.208878, valid loss:0.209455, loss_diff:0.000577\n",
      "{'reg_alpha': 1.0} train loss: 0.208672, valid loss:0.210265, loss_diff:0.001593\n",
      "=================>{'reg_alpha': 1.0} loss:0.209811\n",
      "{'reg_alpha': 1.5} train loss: 0.208843, valid loss:0.209639, loss_diff:0.000795\n",
      "{'reg_alpha': 1.5} train loss: 0.208665, valid loss:0.210401, loss_diff:0.001736\n",
      "{'reg_alpha': 1.5} train loss: 0.208932, valid loss:0.209257, loss_diff:0.000324\n",
      "{'reg_alpha': 1.5} train loss: 0.208865, valid loss:0.209422, loss_diff:0.000557\n",
      "{'reg_alpha': 1.5} train loss: 0.208711, valid loss:0.210298, loss_diff:0.001587\n",
      "=================>{'reg_alpha': 1.5} loss:0.209803\n",
      "{'reg_alpha': 2.0} train loss: 0.208874, valid loss:0.209687, loss_diff:0.000814\n",
      "{'reg_alpha': 2.0} train loss: 0.208664, valid loss:0.210372, loss_diff:0.001707\n",
      "{'reg_alpha': 2.0} train loss: 0.208947, valid loss:0.209205, loss_diff:0.000258\n",
      "{'reg_alpha': 2.0} train loss: 0.208899, valid loss:0.209434, loss_diff:0.000534\n",
      "{'reg_alpha': 2.0} train loss: 0.208697, valid loss:0.210274, loss_diff:0.001577\n",
      "=================>{'reg_alpha': 2.0} loss:0.209794\n",
      "{'reg_alpha': 2.5} train loss: 0.208890, valid loss:0.209666, loss_diff:0.000776\n",
      "{'reg_alpha': 2.5} train loss: 0.208726, valid loss:0.210401, loss_diff:0.001675\n",
      "{'reg_alpha': 2.5} train loss: 0.208972, valid loss:0.209231, loss_diff:0.000259\n",
      "{'reg_alpha': 2.5} train loss: 0.208925, valid loss:0.209476, loss_diff:0.000552\n",
      "{'reg_alpha': 2.5} train loss: 0.208728, valid loss:0.210252, loss_diff:0.001524\n",
      "=================>{'reg_alpha': 2.5} loss:0.209805\n",
      "{'reg_alpha': 3.0} train loss: 0.208879, valid loss:0.209649, loss_diff:0.000770\n",
      "{'reg_alpha': 3.0} train loss: 0.208732, valid loss:0.210378, loss_diff:0.001646\n",
      "{'reg_alpha': 3.0} train loss: 0.208991, valid loss:0.209223, loss_diff:0.000232\n",
      "{'reg_alpha': 3.0} train loss: 0.208932, valid loss:0.209408, loss_diff:0.000476\n",
      "{'reg_alpha': 3.0} train loss: 0.208730, valid loss:0.210266, loss_diff:0.001537\n",
      "=================>{'reg_alpha': 3.0} loss:0.209785\n",
      "Best params: {'reg_alpha': 3.0} \tbest loss: 0.209784773842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'reg_alpha': 1.0}</td>\n",
       "      <td>0.209811</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'reg_alpha': 1.5}</td>\n",
       "      <td>0.209803</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'reg_alpha': 2.0}</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'reg_alpha': 2.5}</td>\n",
       "      <td>0.209805</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'reg_alpha': 3.0}</td>\n",
       "      <td>0.209785</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param  val_loss_mean  val_loss_std\n",
       "0  {'reg_alpha': 1.0}       0.209811      0.000447\n",
       "1  {'reg_alpha': 1.5}       0.209803      0.000463\n",
       "2  {'reg_alpha': 2.0}       0.209794      0.000459\n",
       "3  {'reg_alpha': 2.5}       0.209805      0.000450\n",
       "4  {'reg_alpha': 3.0}       0.209785      0.000460"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_alpha':[1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 3.0} train loss: 0.208879, valid loss:0.209649, loss_diff:0.000770\n",
      "{'reg_alpha': 3.0} train loss: 0.208732, valid loss:0.210378, loss_diff:0.001646\n",
      "{'reg_alpha': 3.0} train loss: 0.208991, valid loss:0.209223, loss_diff:0.000232\n",
      "{'reg_alpha': 3.0} train loss: 0.208932, valid loss:0.209408, loss_diff:0.000476\n",
      "{'reg_alpha': 3.0} train loss: 0.208730, valid loss:0.210266, loss_diff:0.001537\n",
      "=================>{'reg_alpha': 3.0} loss:0.209785\n",
      "{'reg_alpha': 3.5} train loss: 0.208910, valid loss:0.209681, loss_diff:0.000772\n",
      "{'reg_alpha': 3.5} train loss: 0.208761, valid loss:0.210416, loss_diff:0.001655\n",
      "{'reg_alpha': 3.5} train loss: 0.209030, valid loss:0.209252, loss_diff:0.000222\n",
      "{'reg_alpha': 3.5} train loss: 0.208949, valid loss:0.209423, loss_diff:0.000474\n",
      "{'reg_alpha': 3.5} train loss: 0.208732, valid loss:0.210279, loss_diff:0.001547\n",
      "=================>{'reg_alpha': 3.5} loss:0.209810\n",
      "{'reg_alpha': 4.0} train loss: 0.208930, valid loss:0.209668, loss_diff:0.000738\n",
      "{'reg_alpha': 4.0} train loss: 0.208730, valid loss:0.210366, loss_diff:0.001636\n",
      "{'reg_alpha': 4.0} train loss: 0.209013, valid loss:0.209200, loss_diff:0.000187\n",
      "{'reg_alpha': 4.0} train loss: 0.208962, valid loss:0.209422, loss_diff:0.000460\n",
      "{'reg_alpha': 4.0} train loss: 0.208768, valid loss:0.210253, loss_diff:0.001484\n",
      "=================>{'reg_alpha': 4.0} loss:0.209782\n",
      "{'reg_alpha': 4.5} train loss: 0.208943, valid loss:0.209681, loss_diff:0.000738\n",
      "{'reg_alpha': 4.5} train loss: 0.208775, valid loss:0.210397, loss_diff:0.001622\n",
      "{'reg_alpha': 4.5} train loss: 0.209024, valid loss:0.209228, loss_diff:0.000204\n",
      "{'reg_alpha': 4.5} train loss: 0.208965, valid loss:0.209422, loss_diff:0.000458\n",
      "{'reg_alpha': 4.5} train loss: 0.208762, valid loss:0.210249, loss_diff:0.001486\n",
      "=================>{'reg_alpha': 4.5} loss:0.209796\n",
      "Best params: {'reg_alpha': 4.0} \tbest loss: 0.209781653973\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'reg_alpha': 3.0}</td>\n",
       "      <td>0.209785</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'reg_alpha': 3.5}</td>\n",
       "      <td>0.209810</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'reg_alpha': 4.0}</td>\n",
       "      <td>0.209782</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'reg_alpha': 4.5}</td>\n",
       "      <td>0.209796</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param  val_loss_mean  val_loss_std\n",
       "0  {'reg_alpha': 3.0}       0.209785      0.000460\n",
       "1  {'reg_alpha': 3.5}       0.209810      0.000462\n",
       "2  {'reg_alpha': 4.0}       0.209782      0.000457\n",
       "3  {'reg_alpha': 4.5}       0.209796      0.000456"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_alpha':[3.0, 3.5, 4.0, 4.5]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_lambda': 0.0} train loss: 0.208930, valid loss:0.209668, loss_diff:0.000738\n",
      "{'reg_lambda': 0.0} train loss: 0.208730, valid loss:0.210366, loss_diff:0.001636\n",
      "{'reg_lambda': 0.0} train loss: 0.209013, valid loss:0.209200, loss_diff:0.000187\n",
      "{'reg_lambda': 0.0} train loss: 0.208962, valid loss:0.209422, loss_diff:0.000460\n",
      "{'reg_lambda': 0.0} train loss: 0.208768, valid loss:0.210253, loss_diff:0.001484\n",
      "=================>{'reg_lambda': 0.0} loss:0.209782\n",
      "{'reg_lambda': 0.1} train loss: 0.208930, valid loss:0.209670, loss_diff:0.000740\n",
      "{'reg_lambda': 0.1} train loss: 0.208741, valid loss:0.210401, loss_diff:0.001660\n",
      "{'reg_lambda': 0.1} train loss: 0.209003, valid loss:0.209192, loss_diff:0.000189\n",
      "{'reg_lambda': 0.1} train loss: 0.208966, valid loss:0.209417, loss_diff:0.000451\n",
      "{'reg_lambda': 0.1} train loss: 0.208766, valid loss:0.210260, loss_diff:0.001494\n",
      "=================>{'reg_lambda': 0.1} loss:0.209788\n",
      "{'reg_lambda': 0.2} train loss: 0.208929, valid loss:0.209669, loss_diff:0.000740\n",
      "{'reg_lambda': 0.2} train loss: 0.208741, valid loss:0.210401, loss_diff:0.001660\n",
      "{'reg_lambda': 0.2} train loss: 0.209003, valid loss:0.209192, loss_diff:0.000189\n",
      "{'reg_lambda': 0.2} train loss: 0.208966, valid loss:0.209417, loss_diff:0.000451\n",
      "{'reg_lambda': 0.2} train loss: 0.208768, valid loss:0.210287, loss_diff:0.001518\n",
      "=================>{'reg_lambda': 0.2} loss:0.209793\n",
      "{'reg_lambda': 0.3} train loss: 0.208932, valid loss:0.209665, loss_diff:0.000733\n",
      "{'reg_lambda': 0.3} train loss: 0.208741, valid loss:0.210401, loss_diff:0.001660\n",
      "{'reg_lambda': 0.3} train loss: 0.209026, valid loss:0.209219, loss_diff:0.000193\n",
      "{'reg_lambda': 0.3} train loss: 0.208960, valid loss:0.209415, loss_diff:0.000455\n",
      "{'reg_lambda': 0.3} train loss: 0.208767, valid loss:0.210242, loss_diff:0.001475\n",
      "=================>{'reg_lambda': 0.3} loss:0.209789\n",
      "{'reg_lambda': 0.4} train loss: 0.208927, valid loss:0.209665, loss_diff:0.000737\n",
      "{'reg_lambda': 0.4} train loss: 0.208746, valid loss:0.210399, loss_diff:0.001653\n",
      "{'reg_lambda': 0.4} train loss: 0.209026, valid loss:0.209218, loss_diff:0.000192\n",
      "{'reg_lambda': 0.4} train loss: 0.208986, valid loss:0.209423, loss_diff:0.000437\n",
      "{'reg_lambda': 0.4} train loss: 0.208760, valid loss:0.210268, loss_diff:0.001508\n",
      "=================>{'reg_lambda': 0.4} loss:0.209795\n",
      "{'reg_lambda': 0.5} train loss: 0.208925, valid loss:0.209663, loss_diff:0.000737\n",
      "{'reg_lambda': 0.5} train loss: 0.208743, valid loss:0.210393, loss_diff:0.001650\n",
      "{'reg_lambda': 0.5} train loss: 0.209027, valid loss:0.209220, loss_diff:0.000194\n",
      "{'reg_lambda': 0.5} train loss: 0.208986, valid loss:0.209424, loss_diff:0.000438\n",
      "{'reg_lambda': 0.5} train loss: 0.208760, valid loss:0.210268, loss_diff:0.001508\n",
      "=================>{'reg_lambda': 0.5} loss:0.209794\n",
      "{'reg_lambda': 0.6} train loss: 0.208929, valid loss:0.209690, loss_diff:0.000761\n",
      "{'reg_lambda': 0.6} train loss: 0.208757, valid loss:0.210419, loss_diff:0.001663\n",
      "{'reg_lambda': 0.6} train loss: 0.209014, valid loss:0.209234, loss_diff:0.000220\n",
      "{'reg_lambda': 0.6} train loss: 0.208961, valid loss:0.209437, loss_diff:0.000476\n",
      "{'reg_lambda': 0.6} train loss: 0.208760, valid loss:0.210268, loss_diff:0.001508\n",
      "=================>{'reg_lambda': 0.6} loss:0.209809\n",
      "{'reg_lambda': 0.7} train loss: 0.208929, valid loss:0.209690, loss_diff:0.000761\n",
      "{'reg_lambda': 0.7} train loss: 0.208753, valid loss:0.210395, loss_diff:0.001641\n",
      "{'reg_lambda': 0.7} train loss: 0.209014, valid loss:0.209234, loss_diff:0.000220\n",
      "{'reg_lambda': 0.7} train loss: 0.208981, valid loss:0.209436, loss_diff:0.000454\n",
      "{'reg_lambda': 0.7} train loss: 0.208772, valid loss:0.210257, loss_diff:0.001485\n",
      "=================>{'reg_lambda': 0.7} loss:0.209802\n",
      "{'reg_lambda': 0.8} train loss: 0.208908, valid loss:0.209633, loss_diff:0.000725\n",
      "{'reg_lambda': 0.8} train loss: 0.208767, valid loss:0.210386, loss_diff:0.001619\n",
      "{'reg_lambda': 0.8} train loss: 0.209009, valid loss:0.209187, loss_diff:0.000178\n",
      "{'reg_lambda': 0.8} train loss: 0.208984, valid loss:0.209473, loss_diff:0.000489\n",
      "{'reg_lambda': 0.8} train loss: 0.208772, valid loss:0.210257, loss_diff:0.001485\n",
      "=================>{'reg_lambda': 0.8} loss:0.209787\n",
      "{'reg_lambda': 0.9} train loss: 0.208908, valid loss:0.209633, loss_diff:0.000725\n",
      "{'reg_lambda': 0.9} train loss: 0.208769, valid loss:0.210385, loss_diff:0.001617\n",
      "{'reg_lambda': 0.9} train loss: 0.209009, valid loss:0.209187, loss_diff:0.000178\n",
      "{'reg_lambda': 0.9} train loss: 0.208966, valid loss:0.209423, loss_diff:0.000457\n",
      "{'reg_lambda': 0.9} train loss: 0.208756, valid loss:0.210246, loss_diff:0.001490\n",
      "=================>{'reg_lambda': 0.9} loss:0.209775\n",
      "{'reg_lambda': 1.0} train loss: 0.208908, valid loss:0.209633, loss_diff:0.000725\n",
      "{'reg_lambda': 1.0} train loss: 0.208769, valid loss:0.210385, loss_diff:0.001616\n",
      "{'reg_lambda': 1.0} train loss: 0.209009, valid loss:0.209187, loss_diff:0.000178\n",
      "{'reg_lambda': 1.0} train loss: 0.208970, valid loss:0.209423, loss_diff:0.000453\n",
      "{'reg_lambda': 1.0} train loss: 0.208757, valid loss:0.210257, loss_diff:0.001500\n",
      "=================>{'reg_lambda': 1.0} loss:0.209777\n",
      "Best params: {'reg_lambda': 0.9} \tbest loss: 0.209775030553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'reg_lambda': 0.0}</td>\n",
       "      <td>0.209782</td>\n",
       "      <td>0.000457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'reg_lambda': 0.1}</td>\n",
       "      <td>0.209788</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'reg_lambda': 0.2}</td>\n",
       "      <td>0.209793</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'reg_lambda': 0.3}</td>\n",
       "      <td>0.209789</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'reg_lambda': 0.4}</td>\n",
       "      <td>0.209795</td>\n",
       "      <td>0.000464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'reg_lambda': 0.5}</td>\n",
       "      <td>0.209794</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'reg_lambda': 0.6}</td>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'reg_lambda': 0.7}</td>\n",
       "      <td>0.209802</td>\n",
       "      <td>0.000453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'reg_lambda': 0.8}</td>\n",
       "      <td>0.209787</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'reg_lambda': 0.9}</td>\n",
       "      <td>0.209775</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'reg_lambda': 1.0}</td>\n",
       "      <td>0.209777</td>\n",
       "      <td>0.000467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  param  val_loss_mean  val_loss_std\n",
       "0   {'reg_lambda': 0.0}       0.209782      0.000457\n",
       "1   {'reg_lambda': 0.1}       0.209788      0.000470\n",
       "2   {'reg_lambda': 0.2}       0.209793      0.000476\n",
       "3   {'reg_lambda': 0.3}       0.209789      0.000460\n",
       "4   {'reg_lambda': 0.4}       0.209795      0.000464\n",
       "5   {'reg_lambda': 0.5}       0.209794      0.000462\n",
       "6   {'reg_lambda': 0.6}       0.209809      0.000462\n",
       "7   {'reg_lambda': 0.7}       0.209802      0.000453\n",
       "8   {'reg_lambda': 0.8}       0.209787      0.000461\n",
       "9   {'reg_lambda': 0.9}       0.209775      0.000465\n",
       "10  {'reg_lambda': 1.0}       0.209777      0.000467"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':4.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_lambda':[i/10.0 for i in range(0,11,1)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'gbdt', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.02, \n",
    "    'n_estimators':3000, \n",
    "    'min_split_gain':0.1, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':1.5, \n",
    "    'reg_lambda':0.7, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = {\n",
    "    'early_stopping_rounds': 50,\n",
    "    'verbose': 100,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "_, ret_test, _ = get_oof_predictions(train, train_y, test, ml, \n",
    "                                     default_params, seed=19, fit_params=fit_param, use_eval_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\", usecols=['item_id'])\n",
    "pd.DataFrame(np.clip(ret_test,0,1), \n",
    "             index=test_df.item_id,\n",
    "             columns=['deal_probability']).to_csv('lgb_meta_no_bagging_exclude_knn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm-dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_split_gain': 0.0} train loss: 0.213927, valid loss:0.214264, loss_diff:0.000337\n",
      "{'min_split_gain': 0.0} train loss: 0.213748, valid loss:0.215104, loss_diff:0.001356\n",
      "{'min_split_gain': 0.0} train loss: 0.214077, valid loss:0.213627, loss_diff:-0.000450\n",
      "{'min_split_gain': 0.0} train loss: 0.214023, valid loss:0.213778, loss_diff:-0.000245\n",
      "{'min_split_gain': 0.0} train loss: 0.213804, valid loss:0.214721, loss_diff:0.000917\n",
      "=================>{'min_split_gain': 0.0} loss:0.214299\n",
      "{'min_split_gain': 0.1} train loss: 0.213920, valid loss:0.214252, loss_diff:0.000331\n",
      "{'min_split_gain': 0.1} train loss: 0.213755, valid loss:0.215107, loss_diff:0.001352\n",
      "{'min_split_gain': 0.1} train loss: 0.214055, valid loss:0.213606, loss_diff:-0.000449\n",
      "{'min_split_gain': 0.1} train loss: 0.214021, valid loss:0.213776, loss_diff:-0.000245\n",
      "{'min_split_gain': 0.1} train loss: 0.213790, valid loss:0.214729, loss_diff:0.000938\n",
      "=================>{'min_split_gain': 0.1} loss:0.214294\n",
      "{'min_split_gain': 0.2} train loss: 0.213925, valid loss:0.214274, loss_diff:0.000349\n",
      "{'min_split_gain': 0.2} train loss: 0.213742, valid loss:0.215098, loss_diff:0.001357\n",
      "{'min_split_gain': 0.2} train loss: 0.214065, valid loss:0.213638, loss_diff:-0.000427\n",
      "{'min_split_gain': 0.2} train loss: 0.214018, valid loss:0.213760, loss_diff:-0.000258\n",
      "{'min_split_gain': 0.2} train loss: 0.213804, valid loss:0.214729, loss_diff:0.000925\n",
      "=================>{'min_split_gain': 0.2} loss:0.214300\n",
      "{'min_split_gain': 0.3} train loss: 0.213934, valid loss:0.214260, loss_diff:0.000327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-debc7f84c536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mfit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_eval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Kaggle Competitions\\Kaggle-avito-demand-prediction\\GridSearcher.py\u001b[0m in \u001b[0;36mfit_params\u001b[1;34m(X, y, model_loader, default_params, try_params, use_eval_set, fit_params, seed, loss_func, predict_proba)\u001b[0m\n\u001b[0;32m    303\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    610\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m                                        callbacks=callbacks)\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    457\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    197\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1437\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1438\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'dart', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.0, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'min_split_gain': [.0, .1, .2, .3, .4]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.6} train loss: 0.212777, valid loss:0.212907, loss_diff:0.000130\n",
      "{'colsample_bytree': 0.6} train loss: 0.212682, valid loss:0.213379, loss_diff:0.000696\n",
      "{'colsample_bytree': 0.6} train loss: 0.212666, valid loss:0.213448, loss_diff:0.000782\n",
      "{'colsample_bytree': 0.6} train loss: 0.212721, valid loss:0.213123, loss_diff:0.000403\n",
      "{'colsample_bytree': 0.6} train loss: 0.212778, valid loss:0.212892, loss_diff:0.000114\n",
      "=================>{'colsample_bytree': 0.6} loss:0.213150\n",
      "{'colsample_bytree': 0.7} train loss: 0.212761, valid loss:0.212934, loss_diff:0.000172\n",
      "{'colsample_bytree': 0.7} train loss: 0.212644, valid loss:0.213358, loss_diff:0.000714\n",
      "{'colsample_bytree': 0.7} train loss: 0.212643, valid loss:0.213420, loss_diff:0.000777\n",
      "{'colsample_bytree': 0.7} train loss: 0.212692, valid loss:0.213109, loss_diff:0.000417\n",
      "{'colsample_bytree': 0.7} train loss: 0.212771, valid loss:0.212889, loss_diff:0.000118\n",
      "=================>{'colsample_bytree': 0.7} loss:0.213142\n",
      "{'colsample_bytree': 0.8} train loss: 0.212740, valid loss:0.212878, loss_diff:0.000138\n",
      "{'colsample_bytree': 0.8} train loss: 0.212634, valid loss:0.213355, loss_diff:0.000721\n",
      "{'colsample_bytree': 0.8} train loss: 0.212615, valid loss:0.213405, loss_diff:0.000790\n",
      "{'colsample_bytree': 0.8} train loss: 0.212686, valid loss:0.213088, loss_diff:0.000401\n",
      "{'colsample_bytree': 0.8} train loss: 0.212766, valid loss:0.212889, loss_diff:0.000123\n",
      "=================>{'colsample_bytree': 0.8} loss:0.213123\n",
      "{'colsample_bytree': 0.9} train loss: 0.212744, valid loss:0.212931, loss_diff:0.000187\n",
      "{'colsample_bytree': 0.9} train loss: 0.212640, valid loss:0.213351, loss_diff:0.000711\n",
      "{'colsample_bytree': 0.9} train loss: 0.212613, valid loss:0.213417, loss_diff:0.000803\n",
      "{'colsample_bytree': 0.9} train loss: 0.212687, valid loss:0.213103, loss_diff:0.000417\n",
      "{'colsample_bytree': 0.9} train loss: 0.212759, valid loss:0.212897, loss_diff:0.000139\n",
      "=================>{'colsample_bytree': 0.9} loss:0.213140\n",
      "{'colsample_bytree': 1.0} train loss: 0.212707, valid loss:0.212882, loss_diff:0.000174\n",
      "{'colsample_bytree': 1.0} train loss: 0.212610, valid loss:0.213326, loss_diff:0.000716\n",
      "{'colsample_bytree': 1.0} train loss: 0.212597, valid loss:0.213393, loss_diff:0.000795\n",
      "{'colsample_bytree': 1.0} train loss: 0.212668, valid loss:0.213069, loss_diff:0.000401\n",
      "{'colsample_bytree': 1.0} train loss: 0.212739, valid loss:0.212878, loss_diff:0.000139\n",
      "=================>{'colsample_bytree': 1.0} loss:0.213110\n",
      "Best params: {'colsample_bytree': 1.0} \tbest loss: 0.213109551841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'colsample_bytree': 0.6}</td>\n",
       "      <td>0.213150</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'colsample_bytree': 0.7}</td>\n",
       "      <td>0.213142</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'colsample_bytree': 0.8}</td>\n",
       "      <td>0.213123</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'colsample_bytree': 0.9}</td>\n",
       "      <td>0.213140</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'colsample_bytree': 1.0}</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       param  val_loss_mean  val_loss_std\n",
       "0  {'colsample_bytree': 0.6}       0.213150      0.000231\n",
       "1  {'colsample_bytree': 0.7}       0.213142      0.000216\n",
       "2  {'colsample_bytree': 0.8}       0.213123      0.000223\n",
       "3  {'colsample_bytree': 0.9}       0.213140      0.000212\n",
       "4  {'colsample_bytree': 1.0}       0.213110      0.000217"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'dart', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.3, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.6} train loss: 0.212709, valid loss:0.212853, loss_diff:0.000144\n",
      "{'subsample': 0.6} train loss: 0.212645, valid loss:0.213329, loss_diff:0.000684\n",
      "{'subsample': 0.6} train loss: 0.212630, valid loss:0.213384, loss_diff:0.000754\n",
      "{'subsample': 0.6} train loss: 0.212655, valid loss:0.213066, loss_diff:0.000410\n",
      "{'subsample': 0.6} train loss: 0.212758, valid loss:0.212853, loss_diff:0.000095\n",
      "=================>{'subsample': 0.6} loss:0.213097\n",
      "{'subsample': 0.7} train loss: 0.212711, valid loss:0.212867, loss_diff:0.000157\n",
      "{'subsample': 0.7} train loss: 0.212632, valid loss:0.213330, loss_diff:0.000698\n",
      "{'subsample': 0.7} train loss: 0.212620, valid loss:0.213375, loss_diff:0.000755\n",
      "{'subsample': 0.7} train loss: 0.212664, valid loss:0.213054, loss_diff:0.000390\n",
      "{'subsample': 0.7} train loss: 0.212737, valid loss:0.212844, loss_diff:0.000107\n",
      "=================>{'subsample': 0.7} loss:0.213094\n",
      "{'subsample': 0.8} train loss: 0.212731, valid loss:0.212885, loss_diff:0.000154\n",
      "{'subsample': 0.8} train loss: 0.212635, valid loss:0.213348, loss_diff:0.000713\n",
      "{'subsample': 0.8} train loss: 0.212602, valid loss:0.213379, loss_diff:0.000777\n",
      "{'subsample': 0.8} train loss: 0.212661, valid loss:0.213069, loss_diff:0.000408\n",
      "{'subsample': 0.8} train loss: 0.212738, valid loss:0.212860, loss_diff:0.000122\n",
      "=================>{'subsample': 0.8} loss:0.213108\n",
      "{'subsample': 0.9} train loss: 0.212753, valid loss:0.212929, loss_diff:0.000176\n",
      "{'subsample': 0.9} train loss: 0.212634, valid loss:0.213344, loss_diff:0.000710\n",
      "{'subsample': 0.9} train loss: 0.212616, valid loss:0.213407, loss_diff:0.000791\n",
      "{'subsample': 0.9} train loss: 0.212685, valid loss:0.213092, loss_diff:0.000407\n",
      "{'subsample': 0.9} train loss: 0.212733, valid loss:0.212856, loss_diff:0.000123\n",
      "=================>{'subsample': 0.9} loss:0.213126\n",
      "{'subsample': 1.0} train loss: 0.212707, valid loss:0.212882, loss_diff:0.000174\n",
      "{'subsample': 1.0} train loss: 0.212610, valid loss:0.213326, loss_diff:0.000716\n",
      "{'subsample': 1.0} train loss: 0.212597, valid loss:0.213393, loss_diff:0.000795\n",
      "{'subsample': 1.0} train loss: 0.212668, valid loss:0.213069, loss_diff:0.000401\n",
      "{'subsample': 1.0} train loss: 0.212739, valid loss:0.212878, loss_diff:0.000139\n",
      "=================>{'subsample': 1.0} loss:0.213110\n",
      "Best params: {'subsample': 0.7} \tbest loss: 0.213094093686\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'subsample': 0.6}</td>\n",
       "      <td>0.213097</td>\n",
       "      <td>0.000226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'subsample': 0.7}</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'subsample': 0.8}</td>\n",
       "      <td>0.213108</td>\n",
       "      <td>0.000221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'subsample': 0.9}</td>\n",
       "      <td>0.213126</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'subsample': 1.0}</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param  val_loss_mean  val_loss_std\n",
       "0  {'subsample': 0.6}       0.213097      0.000226\n",
       "1  {'subsample': 0.7}       0.213094      0.000223\n",
       "2  {'subsample': 0.8}       0.213108      0.000221\n",
       "3  {'subsample': 0.9}       0.213126      0.000219\n",
       "4  {'subsample': 1.0}       0.213110      0.000217"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'dart', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.3, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'subsample':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 1.0} train loss: 0.212702, valid loss:0.212864, loss_diff:0.000162\n",
      "{'reg_alpha': 1.0} train loss: 0.212614, valid loss:0.213305, loss_diff:0.000691\n",
      "{'reg_alpha': 1.0} train loss: 0.212582, valid loss:0.213371, loss_diff:0.000788\n",
      "{'reg_alpha': 1.0} train loss: 0.212675, valid loss:0.213072, loss_diff:0.000397\n",
      "{'reg_alpha': 1.0} train loss: 0.212702, valid loss:0.212821, loss_diff:0.000118\n",
      "=================>{'reg_alpha': 1.0} loss:0.213086\n",
      "{'reg_alpha': 1.5} train loss: 0.212732, valid loss:0.212871, loss_diff:0.000139\n",
      "{'reg_alpha': 1.5} train loss: 0.212631, valid loss:0.213312, loss_diff:0.000681\n",
      "{'reg_alpha': 1.5} train loss: 0.212605, valid loss:0.213374, loss_diff:0.000770\n",
      "{'reg_alpha': 1.5} train loss: 0.212669, valid loss:0.213057, loss_diff:0.000388\n",
      "{'reg_alpha': 1.5} train loss: 0.212743, valid loss:0.212854, loss_diff:0.000111\n",
      "=================>{'reg_alpha': 1.5} loss:0.213093\n",
      "{'reg_alpha': 2.0} train loss: 0.212711, valid loss:0.212867, loss_diff:0.000157\n",
      "{'reg_alpha': 2.0} train loss: 0.212632, valid loss:0.213330, loss_diff:0.000698\n",
      "{'reg_alpha': 2.0} train loss: 0.212620, valid loss:0.213375, loss_diff:0.000755\n",
      "{'reg_alpha': 2.0} train loss: 0.212664, valid loss:0.213054, loss_diff:0.000390\n",
      "{'reg_alpha': 2.0} train loss: 0.212737, valid loss:0.212844, loss_diff:0.000107\n",
      "=================>{'reg_alpha': 2.0} loss:0.213094\n",
      "{'reg_alpha': 2.5} train loss: 0.212723, valid loss:0.212877, loss_diff:0.000154\n",
      "{'reg_alpha': 2.5} train loss: 0.212632, valid loss:0.213304, loss_diff:0.000672\n",
      "{'reg_alpha': 2.5} train loss: 0.212617, valid loss:0.213390, loss_diff:0.000773\n",
      "{'reg_alpha': 2.5} train loss: 0.212677, valid loss:0.213087, loss_diff:0.000410\n",
      "{'reg_alpha': 2.5} train loss: 0.212740, valid loss:0.212861, loss_diff:0.000122\n",
      "=================>{'reg_alpha': 2.5} loss:0.213104\n",
      "{'reg_alpha': 3.0} train loss: 0.212738, valid loss:0.212883, loss_diff:0.000145\n",
      "{'reg_alpha': 3.0} train loss: 0.212641, valid loss:0.213324, loss_diff:0.000683\n",
      "{'reg_alpha': 3.0} train loss: 0.212635, valid loss:0.213402, loss_diff:0.000767\n",
      "{'reg_alpha': 3.0} train loss: 0.212694, valid loss:0.213084, loss_diff:0.000390\n",
      "{'reg_alpha': 3.0} train loss: 0.212756, valid loss:0.212859, loss_diff:0.000103\n",
      "=================>{'reg_alpha': 3.0} loss:0.213110\n",
      "Best params: {'reg_alpha': 1.0} \tbest loss: 0.213086329661\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'reg_alpha': 1.0}</td>\n",
       "      <td>0.213086</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'reg_alpha': 1.5}</td>\n",
       "      <td>0.213093</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'reg_alpha': 2.0}</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'reg_alpha': 2.5}</td>\n",
       "      <td>0.213104</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'reg_alpha': 3.0}</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                param  val_loss_mean  val_loss_std\n",
       "0  {'reg_alpha': 1.0}       0.213086      0.000223\n",
       "1  {'reg_alpha': 1.5}       0.213093      0.000217\n",
       "2  {'reg_alpha': 2.0}       0.213094      0.000223\n",
       "3  {'reg_alpha': 2.5}       0.213104      0.000216\n",
       "4  {'reg_alpha': 3.0}       0.213110      0.000222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'dart', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.3, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_alpha':[1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_lambda': 0.0} train loss: 0.212702, valid loss:0.212864, loss_diff:0.000162\n",
      "{'reg_lambda': 0.0} train loss: 0.212614, valid loss:0.213305, loss_diff:0.000691\n",
      "{'reg_lambda': 0.0} train loss: 0.212582, valid loss:0.213371, loss_diff:0.000788\n",
      "{'reg_lambda': 0.0} train loss: 0.212675, valid loss:0.213072, loss_diff:0.000397\n",
      "{'reg_lambda': 0.0} train loss: 0.212702, valid loss:0.212821, loss_diff:0.000118\n",
      "=================>{'reg_lambda': 0.0} loss:0.213086\n",
      "{'reg_lambda': 0.1} train loss: 0.212705, valid loss:0.212876, loss_diff:0.000171\n",
      "{'reg_lambda': 0.1} train loss: 0.212607, valid loss:0.213300, loss_diff:0.000693\n",
      "{'reg_lambda': 0.1} train loss: 0.212582, valid loss:0.213371, loss_diff:0.000788\n",
      "{'reg_lambda': 0.1} train loss: 0.212675, valid loss:0.213072, loss_diff:0.000397\n",
      "{'reg_lambda': 0.1} train loss: 0.212718, valid loss:0.212836, loss_diff:0.000118\n",
      "=================>{'reg_lambda': 0.1} loss:0.213091\n",
      "{'reg_lambda': 0.2} train loss: 0.212700, valid loss:0.212859, loss_diff:0.000159\n",
      "{'reg_lambda': 0.2} train loss: 0.212611, valid loss:0.213313, loss_diff:0.000701\n",
      "{'reg_lambda': 0.2} train loss: 0.212601, valid loss:0.213387, loss_diff:0.000786\n",
      "{'reg_lambda': 0.2} train loss: 0.212661, valid loss:0.213062, loss_diff:0.000401\n",
      "{'reg_lambda': 0.2} train loss: 0.212728, valid loss:0.212852, loss_diff:0.000124\n",
      "=================>{'reg_lambda': 0.2} loss:0.213094\n",
      "{'reg_lambda': 0.3} train loss: 0.212700, valid loss:0.212859, loss_diff:0.000159\n",
      "{'reg_lambda': 0.3} train loss: 0.212606, valid loss:0.213308, loss_diff:0.000701\n",
      "{'reg_lambda': 0.3} train loss: 0.212595, valid loss:0.213378, loss_diff:0.000783\n",
      "{'reg_lambda': 0.3} train loss: 0.212668, valid loss:0.213075, loss_diff:0.000406\n",
      "{'reg_lambda': 0.3} train loss: 0.212719, valid loss:0.212839, loss_diff:0.000120\n",
      "=================>{'reg_lambda': 0.3} loss:0.213092\n",
      "{'reg_lambda': 0.4} train loss: 0.212717, valid loss:0.212890, loss_diff:0.000173\n",
      "{'reg_lambda': 0.4} train loss: 0.212604, valid loss:0.213312, loss_diff:0.000708\n",
      "{'reg_lambda': 0.4} train loss: 0.212588, valid loss:0.213375, loss_diff:0.000787\n",
      "{'reg_lambda': 0.4} train loss: 0.212668, valid loss:0.213075, loss_diff:0.000406\n",
      "{'reg_lambda': 0.4} train loss: 0.212718, valid loss:0.212833, loss_diff:0.000115\n",
      "=================>{'reg_lambda': 0.4} loss:0.213097\n",
      "{'reg_lambda': 0.5} train loss: 0.212698, valid loss:0.212876, loss_diff:0.000178\n",
      "{'reg_lambda': 0.5} train loss: 0.212606, valid loss:0.213309, loss_diff:0.000703\n",
      "{'reg_lambda': 0.5} train loss: 0.212587, valid loss:0.213374, loss_diff:0.000787\n",
      "{'reg_lambda': 0.5} train loss: 0.212657, valid loss:0.213066, loss_diff:0.000409\n",
      "{'reg_lambda': 0.5} train loss: 0.212725, valid loss:0.212845, loss_diff:0.000121\n",
      "=================>{'reg_lambda': 0.5} loss:0.213094\n",
      "{'reg_lambda': 0.6} train loss: 0.212715, valid loss:0.212878, loss_diff:0.000163\n",
      "{'reg_lambda': 0.6} train loss: 0.212618, valid loss:0.213318, loss_diff:0.000700\n",
      "{'reg_lambda': 0.6} train loss: 0.212605, valid loss:0.213366, loss_diff:0.000761\n",
      "{'reg_lambda': 0.6} train loss: 0.212670, valid loss:0.213075, loss_diff:0.000405\n",
      "{'reg_lambda': 0.6} train loss: 0.212734, valid loss:0.212846, loss_diff:0.000112\n",
      "=================>{'reg_lambda': 0.6} loss:0.213097\n",
      "{'reg_lambda': 0.7} train loss: 0.212704, valid loss:0.212867, loss_diff:0.000162\n",
      "{'reg_lambda': 0.7} train loss: 0.212616, valid loss:0.213310, loss_diff:0.000695\n",
      "{'reg_lambda': 0.7} train loss: 0.212588, valid loss:0.213362, loss_diff:0.000774\n",
      "{'reg_lambda': 0.7} train loss: 0.212660, valid loss:0.213078, loss_diff:0.000418\n",
      "{'reg_lambda': 0.7} train loss: 0.212742, valid loss:0.212861, loss_diff:0.000119\n",
      "=================>{'reg_lambda': 0.7} loss:0.213096\n",
      "{'reg_lambda': 0.8} train loss: 0.212705, valid loss:0.212866, loss_diff:0.000161\n",
      "{'reg_lambda': 0.8} train loss: 0.212618, valid loss:0.213323, loss_diff:0.000705\n",
      "{'reg_lambda': 0.8} train loss: 0.212589, valid loss:0.213362, loss_diff:0.000774\n",
      "{'reg_lambda': 0.8} train loss: 0.212647, valid loss:0.213074, loss_diff:0.000427\n",
      "{'reg_lambda': 0.8} train loss: 0.212713, valid loss:0.212831, loss_diff:0.000118\n",
      "=================>{'reg_lambda': 0.8} loss:0.213091\n",
      "{'reg_lambda': 0.9} train loss: 0.212715, valid loss:0.212878, loss_diff:0.000163\n",
      "{'reg_lambda': 0.9} train loss: 0.212607, valid loss:0.213290, loss_diff:0.000683\n",
      "{'reg_lambda': 0.9} train loss: 0.212589, valid loss:0.213362, loss_diff:0.000774\n",
      "{'reg_lambda': 0.9} train loss: 0.212635, valid loss:0.213059, loss_diff:0.000424\n",
      "{'reg_lambda': 0.9} train loss: 0.212719, valid loss:0.212825, loss_diff:0.000106\n",
      "=================>{'reg_lambda': 0.9} loss:0.213083\n",
      "{'reg_lambda': 1.0} train loss: 0.212703, valid loss:0.212847, loss_diff:0.000144\n",
      "{'reg_lambda': 1.0} train loss: 0.212619, valid loss:0.213316, loss_diff:0.000697\n",
      "{'reg_lambda': 1.0} train loss: 0.212577, valid loss:0.213344, loss_diff:0.000767\n",
      "{'reg_lambda': 1.0} train loss: 0.212635, valid loss:0.213059, loss_diff:0.000424\n",
      "{'reg_lambda': 1.0} train loss: 0.212732, valid loss:0.212845, loss_diff:0.000113\n",
      "=================>{'reg_lambda': 1.0} loss:0.213082\n",
      "Best params: {'reg_lambda': 1.0} \tbest loss: 0.213082108106\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'reg_lambda': 0.0}</td>\n",
       "      <td>0.213086</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'reg_lambda': 0.1}</td>\n",
       "      <td>0.213091</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'reg_lambda': 0.2}</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'reg_lambda': 0.3}</td>\n",
       "      <td>0.213092</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'reg_lambda': 0.4}</td>\n",
       "      <td>0.213097</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'reg_lambda': 0.5}</td>\n",
       "      <td>0.213094</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'reg_lambda': 0.6}</td>\n",
       "      <td>0.213097</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'reg_lambda': 0.7}</td>\n",
       "      <td>0.213096</td>\n",
       "      <td>0.000212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'reg_lambda': 0.8}</td>\n",
       "      <td>0.213091</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'reg_lambda': 0.9}</td>\n",
       "      <td>0.213083</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'reg_lambda': 1.0}</td>\n",
       "      <td>0.213082</td>\n",
       "      <td>0.000217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  param  val_loss_mean  val_loss_std\n",
       "0   {'reg_lambda': 0.0}       0.213086      0.000223\n",
       "1   {'reg_lambda': 0.1}       0.213091      0.000216\n",
       "2   {'reg_lambda': 0.2}       0.213094      0.000223\n",
       "3   {'reg_lambda': 0.3}       0.213092      0.000222\n",
       "4   {'reg_lambda': 0.4}       0.213097      0.000217\n",
       "5   {'reg_lambda': 0.5}       0.213094      0.000217\n",
       "6   {'reg_lambda': 0.6}       0.213097      0.000216\n",
       "7   {'reg_lambda': 0.7}       0.213096      0.000212\n",
       "8   {'reg_lambda': 0.8}       0.213091      0.000222\n",
       "9   {'reg_lambda': 0.9}       0.213083      0.000214\n",
       "10  {'reg_lambda': 1.0}       0.213082      0.000217"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = {\n",
    "    'boosting_type':'dart', \n",
    "    'num_leaves':31, \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'min_split_gain':0.3, \n",
    "    'min_child_weight':0.001, \n",
    "    'min_child_samples':20, \n",
    "    'subsample':.7,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':1.0, \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_lambda':[i/10.0 for i in range(0,11,1)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB-gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'min_child_weight':[0.001, 0.1, 2, 4, 8]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'gamma':[.0, .1, .2, .3, .4]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'subsample':[i/10.0 for i in range(6,11)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_alpha':[1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml = model_loader(model_type='xgb')\n",
    "\n",
    "default_params = {\n",
    "    'booster':'gbtree', \n",
    "    'max_depth':5, \n",
    "    'learning_rate':0.1, \n",
    "    'n_estimators':100, \n",
    "    'gamma':0.0, \n",
    "    'min_child_weight':0.001,\n",
    "    'subsample':1.,  \n",
    "    'colsample_bytree':1., \n",
    "    'reg_alpha':2., \n",
    "    'reg_lambda':0.0, \n",
    "    'random_state':SEED, \n",
    "    'n_jobs': 3\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'reg_lambda':[i/10.0 for i in range(0,11,1)]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rigde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1} train loss: 0.209819, valid loss:0.209670, loss_diff:-0.000149\n",
      "{'alpha': 1} train loss: 0.209629, valid loss:0.210411, loss_diff:0.000782\n",
      "{'alpha': 1} train loss: 0.209860, valid loss:0.209322, loss_diff:-0.000538\n",
      "{'alpha': 1} train loss: 0.209857, valid loss:0.209452, loss_diff:-0.000405\n",
      "{'alpha': 1} train loss: 0.209675, valid loss:0.210227, loss_diff:0.000552\n",
      "=================>{'alpha': 1} loss:0.209816\n",
      "{'alpha': 2} train loss: 0.209827, valid loss:0.209679, loss_diff:-0.000148\n",
      "{'alpha': 2} train loss: 0.209637, valid loss:0.210420, loss_diff:0.000783\n",
      "{'alpha': 2} train loss: 0.209869, valid loss:0.209325, loss_diff:-0.000544\n",
      "{'alpha': 2} train loss: 0.209866, valid loss:0.209459, loss_diff:-0.000406\n",
      "{'alpha': 2} train loss: 0.209683, valid loss:0.210233, loss_diff:0.000550\n",
      "=================>{'alpha': 2} loss:0.209823\n",
      "{'alpha': 4} train loss: 0.209842, valid loss:0.209697, loss_diff:-0.000146\n",
      "{'alpha': 4} train loss: 0.209652, valid loss:0.210436, loss_diff:0.000784\n",
      "{'alpha': 4} train loss: 0.209886, valid loss:0.209334, loss_diff:-0.000552\n",
      "{'alpha': 4} train loss: 0.209881, valid loss:0.209473, loss_diff:-0.000408\n",
      "{'alpha': 4} train loss: 0.209699, valid loss:0.210248, loss_diff:0.000548\n",
      "=================>{'alpha': 4} loss:0.209838\n",
      "{'alpha': 8} train loss: 0.209869, valid loss:0.209726, loss_diff:-0.000143\n",
      "{'alpha': 8} train loss: 0.209679, valid loss:0.210463, loss_diff:0.000784\n",
      "{'alpha': 8} train loss: 0.209915, valid loss:0.209354, loss_diff:-0.000561\n",
      "{'alpha': 8} train loss: 0.209909, valid loss:0.209498, loss_diff:-0.000411\n",
      "{'alpha': 8} train loss: 0.209727, valid loss:0.210274, loss_diff:0.000547\n",
      "=================>{'alpha': 8} loss:0.209863\n",
      "Best params: {'alpha': 1} \tbest loss: 0.209816389105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.209816</td>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'alpha': 2}</td>\n",
       "      <td>0.209823</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'alpha': 4}</td>\n",
       "      <td>0.209838</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'alpha': 8}</td>\n",
       "      <td>0.209863</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          param  val_loss_mean  val_loss_std\n",
       "0  {'alpha': 1}       0.209816      0.000429\n",
       "1  {'alpha': 2}       0.209823      0.000430\n",
       "2  {'alpha': 4}       0.209838      0.000432\n",
       "3  {'alpha': 8}       0.209863      0.000433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = model_loader(model_type='rg')\n",
    "\n",
    "default_params = {\n",
    "    'alpha': 1.0, \n",
    "    'fit_intercept': True, \n",
    "    'normalize': False, \n",
    "    'copy_X': True, \n",
    "    'max_iter': None, \n",
    "    'tol': 0.001, \n",
    "    'solver':'auto', \n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'alpha':[1,2,4,8]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.05} train loss: 0.209811, valid loss:0.209662, loss_diff:-0.000149\n",
      "{'alpha': 0.05} train loss: 0.209621, valid loss:0.210403, loss_diff:0.000782\n",
      "{'alpha': 0.05} train loss: 0.209850, valid loss:0.209325, loss_diff:-0.000525\n",
      "{'alpha': 0.05} train loss: 0.209850, valid loss:0.209446, loss_diff:-0.000404\n",
      "{'alpha': 0.05} train loss: 0.209666, valid loss:0.210224, loss_diff:0.000558\n",
      "=================>{'alpha': 0.05} loss:0.209812\n",
      "{'alpha': 0.1} train loss: 0.209812, valid loss:0.209662, loss_diff:-0.000149\n",
      "{'alpha': 0.1} train loss: 0.209622, valid loss:0.210403, loss_diff:0.000781\n",
      "{'alpha': 0.1} train loss: 0.209850, valid loss:0.209324, loss_diff:-0.000527\n",
      "{'alpha': 0.1} train loss: 0.209850, valid loss:0.209446, loss_diff:-0.000404\n",
      "{'alpha': 0.1} train loss: 0.209666, valid loss:0.210224, loss_diff:0.000557\n",
      "=================>{'alpha': 0.1} loss:0.209812\n",
      "{'alpha': 0.5} train loss: 0.209815, valid loss:0.209666, loss_diff:-0.000149\n",
      "{'alpha': 0.5} train loss: 0.209625, valid loss:0.210407, loss_diff:0.000781\n",
      "{'alpha': 0.5} train loss: 0.209855, valid loss:0.209322, loss_diff:-0.000533\n",
      "{'alpha': 0.5} train loss: 0.209853, valid loss:0.209449, loss_diff:-0.000405\n",
      "{'alpha': 0.5} train loss: 0.209670, valid loss:0.210224, loss_diff:0.000554\n",
      "=================>{'alpha': 0.5} loss:0.209813\n",
      "Best params: {'alpha': 0.1} \tbest loss: 0.209811922909\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'alpha': 0.05}</td>\n",
       "      <td>0.209812</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.209812</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.209813</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             param  val_loss_mean  val_loss_std\n",
       "0  {'alpha': 0.05}       0.209812      0.000427\n",
       "1   {'alpha': 0.1}       0.209812      0.000427\n",
       "2   {'alpha': 0.5}       0.209813      0.000428"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = model_loader(model_type='rg')\n",
    "\n",
    "default_params = {\n",
    "    'alpha': 1.0, \n",
    "    'fit_intercept': True, \n",
    "    'normalize': False, \n",
    "    'copy_X': True, \n",
    "    'max_iter': None, \n",
    "    'tol': 0.001, \n",
    "    'solver':'auto', \n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'alpha':[0.05, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "fit_params(train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging + Ultimate Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = [19, 23, 37]\n",
    "config = {\n",
    "    'lgb_dart':{\n",
    "        'ml': model_loader(model_type='lgb'),\n",
    "        'param': {\n",
    "            'boosting_type':'dart', \n",
    "            'num_leaves':31, \n",
    "            'max_depth':5, \n",
    "            'learning_rate':0.1, \n",
    "            'n_estimators':5000, \n",
    "            'min_split_gain':0.0, \n",
    "            'min_child_weight':0.001, \n",
    "            'min_child_samples':20, \n",
    "            'subsample':.7,  \n",
    "            'colsample_bytree':1., \n",
    "            'reg_alpha':4.0, \n",
    "            'reg_lambda':.9, \n",
    "            'random_state':SEED, \n",
    "            'n_jobs': 3\n",
    "        },\n",
    "        'fit_param': {\n",
    "            'early_stopping_rounds': 50,\n",
    "            'verbose': 100,\n",
    "            'eval_metric': 'rmse'\n",
    "        }\n",
    "    },\n",
    "    'lgb_gbdt':{\n",
    "        'ml': model_loader(model_type='lgb'),\n",
    "        'param': {\n",
    "            'boosting_type':'gbdt', \n",
    "            'num_leaves':31, \n",
    "            'max_depth':5, \n",
    "            'learning_rate':0.02, \n",
    "            'n_estimators':5000, \n",
    "            'min_split_gain':0.0, \n",
    "            'min_child_weight':0.001, \n",
    "            'min_child_samples':20, \n",
    "            'subsample':.7,  \n",
    "            'colsample_bytree':1., \n",
    "            'reg_alpha':4.0, \n",
    "            'reg_lambda':0.9, \n",
    "            'random_state':SEED, \n",
    "            'n_jobs': 3\n",
    "        },\n",
    "        'fit_param': {\n",
    "            'early_stopping_rounds': 50,\n",
    "            'verbose': 100,\n",
    "            'eval_metric': 'rmse'\n",
    "        }\n",
    "    },\n",
    "    'ridge':{\n",
    "        'ml': model_loader(model_type='rg'),\n",
    "        'param': {\n",
    "            'alpha': 0.05, \n",
    "            'fit_intercept': True, \n",
    "            'normalize': False, \n",
    "            'copy_X': True, \n",
    "            'max_iter': None, \n",
    "            'tol': 0.001, \n",
    "            'solver':'auto', \n",
    "            'random_state': SEED\n",
    "        },\n",
    "        'fit_param': None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & bagging:  lgb_dart\n",
      "Training seed = 19\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211366\tvalid's rmse: 0.211634\n",
      "[200]\ttrain's rmse: 0.209535\tvalid's rmse: 0.210124\n",
      "[300]\ttrain's rmse: 0.208858\tvalid's rmse: 0.209686\n",
      "[400]\ttrain's rmse: 0.208242\tvalid's rmse: 0.20943\n",
      "[500]\ttrain's rmse: 0.208029\tvalid's rmse: 0.209412\n",
      "[600]\ttrain's rmse: 0.207452\tvalid's rmse: 0.209224\n",
      "Early stopping, best iteration is:\n",
      "[617]\ttrain's rmse: 0.207332\tvalid's rmse: 0.209192\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211195\tvalid's rmse: 0.212355\n",
      "[200]\ttrain's rmse: 0.209372\tvalid's rmse: 0.210822\n",
      "[300]\ttrain's rmse: 0.208699\tvalid's rmse: 0.210374\n",
      "[400]\ttrain's rmse: 0.208082\tvalid's rmse: 0.210112\n",
      "[500]\ttrain's rmse: 0.207874\tvalid's rmse: 0.210065\n",
      "[600]\ttrain's rmse: 0.207324\tvalid's rmse: 0.20991\n",
      "[700]\ttrain's rmse: 0.207115\tvalid's rmse: 0.209877\n",
      "[800]\ttrain's rmse: 0.206706\tvalid's rmse: 0.209792\n",
      "Early stopping, best iteration is:\n",
      "[827]\ttrain's rmse: 0.206634\tvalid's rmse: 0.20978\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211457\tvalid's rmse: 0.211144\n",
      "[200]\ttrain's rmse: 0.209638\tvalid's rmse: 0.209712\n",
      "[300]\ttrain's rmse: 0.208988\tvalid's rmse: 0.209345\n",
      "[400]\ttrain's rmse: 0.208331\tvalid's rmse: 0.209033\n",
      "[500]\ttrain's rmse: 0.208122\tvalid's rmse: 0.209\n",
      "[600]\ttrain's rmse: 0.207537\tvalid's rmse: 0.20883\n",
      "Early stopping, best iteration is:\n",
      "[622]\ttrain's rmse: 0.207417\tvalid's rmse: 0.208805\n",
      "Fold 3 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211423\tvalid's rmse: 0.211311\n",
      "[200]\ttrain's rmse: 0.209591\tvalid's rmse: 0.209853\n",
      "[300]\ttrain's rmse: 0.208901\tvalid's rmse: 0.209461\n",
      "[400]\ttrain's rmse: 0.208261\tvalid's rmse: 0.209203\n",
      "Early stopping, best iteration is:\n",
      "[443]\ttrain's rmse: 0.208086\tvalid's rmse: 0.209157\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211267\tvalid's rmse: 0.212157\n",
      "[200]\ttrain's rmse: 0.209388\tvalid's rmse: 0.210648\n",
      "[300]\ttrain's rmse: 0.208729\tvalid's rmse: 0.210259\n",
      "[400]\ttrain's rmse: 0.20808\tvalid's rmse: 0.209969\n",
      "[500]\ttrain's rmse: 0.207891\tvalid's rmse: 0.20997\n",
      "[600]\ttrain's rmse: 0.207315\tvalid's rmse: 0.20977\n",
      "Early stopping, best iteration is:\n",
      "[619]\ttrain's rmse: 0.207178\tvalid's rmse: 0.209733\n",
      "Fold 5 completed.\n",
      "Training seed = 23\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.210265\tvalid's rmse: 0.210645\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 0.210044\tvalid's rmse: 0.21043\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.210093\tvalid's rmse: 0.211353\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 0.20988\tvalid's rmse: 0.211161\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.210347\tvalid's rmse: 0.210216\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 0.210126\tvalid's rmse: 0.210026\n",
      "Fold 3 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.210311\tvalid's rmse: 0.210338\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 0.210103\tvalid's rmse: 0.21018\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.210145\tvalid's rmse: 0.211172\n",
      "Early stopping, best iteration is:\n",
      "[103]\ttrain's rmse: 0.209925\tvalid's rmse: 0.210981\n",
      "Fold 5 completed.\n",
      "Training seed = 37\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212306\tvalid's rmse: 0.212691\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.210388\tvalid's rmse: 0.210665\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212113\tvalid's rmse: 0.213302\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.210181\tvalid's rmse: 0.211386\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212369\tvalid's rmse: 0.212094\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.210449\tvalid's rmse: 0.210221\n",
      "Fold 3 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212315\tvalid's rmse: 0.21223\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.210377\tvalid's rmse: 0.210377\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212116\tvalid's rmse: 0.213138\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttrain's rmse: 0.210216\tvalid's rmse: 0.211187\n",
      "Fold 5 completed.\n",
      "Training & bagging:  lgb_gbdt\n",
      "Training seed = 19\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212196\tvalid's rmse: 0.212313\n",
      "[200]\ttrain's rmse: 0.21014\tvalid's rmse: 0.210427\n",
      "[300]\ttrain's rmse: 0.209475\tvalid's rmse: 0.209945\n",
      "[400]\ttrain's rmse: 0.209063\tvalid's rmse: 0.209711\n",
      "[500]\ttrain's rmse: 0.208724\tvalid's rmse: 0.209546\n",
      "[600]\ttrain's rmse: 0.208416\tvalid's rmse: 0.209421\n",
      "[700]\ttrain's rmse: 0.208146\tvalid's rmse: 0.209347\n",
      "[800]\ttrain's rmse: 0.207899\tvalid's rmse: 0.209282\n",
      "[900]\ttrain's rmse: 0.207656\tvalid's rmse: 0.209226\n",
      "[1000]\ttrain's rmse: 0.207432\tvalid's rmse: 0.209194\n",
      "[1100]\ttrain's rmse: 0.207214\tvalid's rmse: 0.209158\n",
      "[1200]\ttrain's rmse: 0.207002\tvalid's rmse: 0.209131\n",
      "[1300]\ttrain's rmse: 0.206791\tvalid's rmse: 0.20911\n",
      "[1400]\ttrain's rmse: 0.206585\tvalid's rmse: 0.209083\n",
      "[1500]\ttrain's rmse: 0.206384\tvalid's rmse: 0.209068\n",
      "[1600]\ttrain's rmse: 0.206187\tvalid's rmse: 0.209057\n",
      "[1700]\ttrain's rmse: 0.205996\tvalid's rmse: 0.209046\n",
      "[1800]\ttrain's rmse: 0.205806\tvalid's rmse: 0.209037\n",
      "[1900]\ttrain's rmse: 0.205624\tvalid's rmse: 0.209027\n",
      "[2000]\ttrain's rmse: 0.205438\tvalid's rmse: 0.209018\n",
      "[2100]\ttrain's rmse: 0.205252\tvalid's rmse: 0.209002\n",
      "[2200]\ttrain's rmse: 0.205073\tvalid's rmse: 0.208993\n",
      "Early stopping, best iteration is:\n",
      "[2214]\ttrain's rmse: 0.205048\tvalid's rmse: 0.208989\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212016\tvalid's rmse: 0.213099\n",
      "[200]\ttrain's rmse: 0.209949\tvalid's rmse: 0.21116\n",
      "[300]\ttrain's rmse: 0.20931\tvalid's rmse: 0.210679\n",
      "[400]\ttrain's rmse: 0.208895\tvalid's rmse: 0.210431\n",
      "[500]\ttrain's rmse: 0.208553\tvalid's rmse: 0.210262\n",
      "[600]\ttrain's rmse: 0.208247\tvalid's rmse: 0.21013\n",
      "[700]\ttrain's rmse: 0.20798\tvalid's rmse: 0.210039\n",
      "[800]\ttrain's rmse: 0.207726\tvalid's rmse: 0.209968\n",
      "[900]\ttrain's rmse: 0.207494\tvalid's rmse: 0.209918\n",
      "[1000]\ttrain's rmse: 0.207272\tvalid's rmse: 0.209872\n",
      "[1100]\ttrain's rmse: 0.207052\tvalid's rmse: 0.209829\n",
      "[1200]\ttrain's rmse: 0.206839\tvalid's rmse: 0.209798\n",
      "[1300]\ttrain's rmse: 0.206631\tvalid's rmse: 0.209773\n",
      "[1400]\ttrain's rmse: 0.206432\tvalid's rmse: 0.209757\n",
      "[1500]\ttrain's rmse: 0.206232\tvalid's rmse: 0.209741\n",
      "[1600]\ttrain's rmse: 0.20604\tvalid's rmse: 0.209724\n",
      "[1700]\ttrain's rmse: 0.205846\tvalid's rmse: 0.209705\n",
      "[1800]\ttrain's rmse: 0.205663\tvalid's rmse: 0.209695\n",
      "Early stopping, best iteration is:\n",
      "[1821]\ttrain's rmse: 0.205622\tvalid's rmse: 0.209692\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212308\tvalid's rmse: 0.211828\n",
      "[200]\ttrain's rmse: 0.210226\tvalid's rmse: 0.209994\n",
      "[300]\ttrain's rmse: 0.20957\tvalid's rmse: 0.209543\n",
      "[400]\ttrain's rmse: 0.209147\tvalid's rmse: 0.209309\n",
      "[500]\ttrain's rmse: 0.208797\tvalid's rmse: 0.20915\n",
      "[600]\ttrain's rmse: 0.208493\tvalid's rmse: 0.209042\n",
      "[700]\ttrain's rmse: 0.208225\tvalid's rmse: 0.208975\n",
      "[800]\ttrain's rmse: 0.207976\tvalid's rmse: 0.208915\n",
      "[900]\ttrain's rmse: 0.207742\tvalid's rmse: 0.208873\n",
      "[1000]\ttrain's rmse: 0.207512\tvalid's rmse: 0.208828\n",
      "[1100]\ttrain's rmse: 0.207291\tvalid's rmse: 0.208797\n",
      "[1200]\ttrain's rmse: 0.207076\tvalid's rmse: 0.208773\n",
      "[1300]\ttrain's rmse: 0.206873\tvalid's rmse: 0.208757\n",
      "[1400]\ttrain's rmse: 0.206673\tvalid's rmse: 0.208741\n",
      "[1500]\ttrain's rmse: 0.20647\tvalid's rmse: 0.208727\n",
      "[1600]\ttrain's rmse: 0.206277\tvalid's rmse: 0.208717\n",
      "Early stopping, best iteration is:\n",
      "[1618]\ttrain's rmse: 0.206241\tvalid's rmse: 0.208717\n",
      "Fold 3 completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212258\tvalid's rmse: 0.212027\n",
      "[200]\ttrain's rmse: 0.210177\tvalid's rmse: 0.210188\n",
      "[300]\ttrain's rmse: 0.209526\tvalid's rmse: 0.209747\n",
      "[400]\ttrain's rmse: 0.209105\tvalid's rmse: 0.209511\n",
      "[500]\ttrain's rmse: 0.208745\tvalid's rmse: 0.209343\n",
      "[600]\ttrain's rmse: 0.208451\tvalid's rmse: 0.209235\n",
      "[700]\ttrain's rmse: 0.208179\tvalid's rmse: 0.209152\n",
      "[800]\ttrain's rmse: 0.207925\tvalid's rmse: 0.209092\n",
      "[900]\ttrain's rmse: 0.207683\tvalid's rmse: 0.209043\n",
      "[1000]\ttrain's rmse: 0.207456\tvalid's rmse: 0.209002\n",
      "[1100]\ttrain's rmse: 0.207241\tvalid's rmse: 0.208969\n",
      "[1200]\ttrain's rmse: 0.207026\tvalid's rmse: 0.208941\n",
      "[1300]\ttrain's rmse: 0.206814\tvalid's rmse: 0.208917\n",
      "[1400]\ttrain's rmse: 0.206614\tvalid's rmse: 0.208899\n",
      "[1500]\ttrain's rmse: 0.206414\tvalid's rmse: 0.208883\n",
      "[1600]\ttrain's rmse: 0.206213\tvalid's rmse: 0.208859\n",
      "[1700]\ttrain's rmse: 0.20602\tvalid's rmse: 0.208844\n",
      "[1800]\ttrain's rmse: 0.205824\tvalid's rmse: 0.208833\n",
      "[1900]\ttrain's rmse: 0.205633\tvalid's rmse: 0.208825\n",
      "[2000]\ttrain's rmse: 0.205444\tvalid's rmse: 0.208815\n",
      "[2100]\ttrain's rmse: 0.205259\tvalid's rmse: 0.208806\n",
      "Early stopping, best iteration is:\n",
      "[2064]\ttrain's rmse: 0.205324\tvalid's rmse: 0.208805\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.21209\tvalid's rmse: 0.212775\n",
      "[200]\ttrain's rmse: 0.209994\tvalid's rmse: 0.210946\n",
      "[300]\ttrain's rmse: 0.209322\tvalid's rmse: 0.210497\n",
      "[400]\ttrain's rmse: 0.208895\tvalid's rmse: 0.210272\n",
      "[500]\ttrain's rmse: 0.208556\tvalid's rmse: 0.210134\n",
      "[600]\ttrain's rmse: 0.208253\tvalid's rmse: 0.210023\n",
      "[700]\ttrain's rmse: 0.207986\tvalid's rmse: 0.209949\n",
      "[800]\ttrain's rmse: 0.207737\tvalid's rmse: 0.209886\n",
      "[900]\ttrain's rmse: 0.207504\tvalid's rmse: 0.209838\n",
      "[1000]\ttrain's rmse: 0.20728\tvalid's rmse: 0.2098\n",
      "[1100]\ttrain's rmse: 0.207065\tvalid's rmse: 0.209772\n",
      "[1200]\ttrain's rmse: 0.206854\tvalid's rmse: 0.20975\n",
      "[1300]\ttrain's rmse: 0.206653\tvalid's rmse: 0.209736\n",
      "[1400]\ttrain's rmse: 0.206448\tvalid's rmse: 0.209723\n",
      "[1500]\ttrain's rmse: 0.20625\tvalid's rmse: 0.209703\n",
      "[1600]\ttrain's rmse: 0.206052\tvalid's rmse: 0.209684\n",
      "[1700]\ttrain's rmse: 0.205863\tvalid's rmse: 0.209673\n",
      "Early stopping, best iteration is:\n",
      "[1729]\ttrain's rmse: 0.205808\tvalid's rmse: 0.20967\n",
      "Fold 5 completed.\n",
      "Training seed = 23\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212189\tvalid's rmse: 0.212303\n",
      "[200]\ttrain's rmse: 0.210105\tvalid's rmse: 0.210387\n",
      "[300]\ttrain's rmse: 0.209474\tvalid's rmse: 0.209946\n",
      "[400]\ttrain's rmse: 0.209061\tvalid's rmse: 0.209719\n",
      "[500]\ttrain's rmse: 0.208721\tvalid's rmse: 0.209561\n",
      "[600]\ttrain's rmse: 0.208414\tvalid's rmse: 0.209433\n",
      "[700]\ttrain's rmse: 0.208146\tvalid's rmse: 0.209359\n",
      "[800]\ttrain's rmse: 0.207895\tvalid's rmse: 0.209292\n",
      "[900]\ttrain's rmse: 0.207654\tvalid's rmse: 0.209239\n",
      "[1000]\ttrain's rmse: 0.207428\tvalid's rmse: 0.209197\n",
      "[1100]\ttrain's rmse: 0.207207\tvalid's rmse: 0.20916\n",
      "[1200]\ttrain's rmse: 0.206995\tvalid's rmse: 0.209131\n",
      "[1300]\ttrain's rmse: 0.206786\tvalid's rmse: 0.209108\n",
      "[1400]\ttrain's rmse: 0.20658\tvalid's rmse: 0.20908\n",
      "[1500]\ttrain's rmse: 0.206384\tvalid's rmse: 0.209066\n",
      "[1600]\ttrain's rmse: 0.206186\tvalid's rmse: 0.209046\n",
      "[1700]\ttrain's rmse: 0.205994\tvalid's rmse: 0.209034\n",
      "[1800]\ttrain's rmse: 0.2058\tvalid's rmse: 0.209025\n",
      "[1900]\ttrain's rmse: 0.205616\tvalid's rmse: 0.209016\n",
      "[2000]\ttrain's rmse: 0.205429\tvalid's rmse: 0.209009\n",
      "[2100]\ttrain's rmse: 0.205248\tvalid's rmse: 0.209\n",
      "[2200]\ttrain's rmse: 0.20507\tvalid's rmse: 0.208987\n",
      "[2300]\ttrain's rmse: 0.204889\tvalid's rmse: 0.208979\n",
      "[2400]\ttrain's rmse: 0.204712\tvalid's rmse: 0.208974\n",
      "Early stopping, best iteration is:\n",
      "[2411]\ttrain's rmse: 0.204692\tvalid's rmse: 0.208973\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.211994\tvalid's rmse: 0.213067\n",
      "[200]\ttrain's rmse: 0.209937\tvalid's rmse: 0.211148\n",
      "[300]\ttrain's rmse: 0.20929\tvalid's rmse: 0.210658\n",
      "[400]\ttrain's rmse: 0.20889\tvalid's rmse: 0.210419\n",
      "[500]\ttrain's rmse: 0.20855\tvalid's rmse: 0.210245\n",
      "[600]\ttrain's rmse: 0.208256\tvalid's rmse: 0.210125\n",
      "[700]\ttrain's rmse: 0.207992\tvalid's rmse: 0.210036\n",
      "[800]\ttrain's rmse: 0.20774\tvalid's rmse: 0.209961\n",
      "[900]\ttrain's rmse: 0.207503\tvalid's rmse: 0.209901\n",
      "[1000]\ttrain's rmse: 0.207285\tvalid's rmse: 0.209863\n",
      "[1100]\ttrain's rmse: 0.20707\tvalid's rmse: 0.209818\n",
      "[1200]\ttrain's rmse: 0.206858\tvalid's rmse: 0.209798\n",
      "[1300]\ttrain's rmse: 0.206653\tvalid's rmse: 0.209773\n",
      "[1400]\ttrain's rmse: 0.206455\tvalid's rmse: 0.209751\n",
      "[1500]\ttrain's rmse: 0.206256\tvalid's rmse: 0.209726\n",
      "[1600]\ttrain's rmse: 0.206064\tvalid's rmse: 0.209709\n",
      "[1700]\ttrain's rmse: 0.205865\tvalid's rmse: 0.209694\n",
      "[1800]\ttrain's rmse: 0.205676\tvalid's rmse: 0.209677\n",
      "[1900]\ttrain's rmse: 0.205488\tvalid's rmse: 0.209666\n",
      "[2000]\ttrain's rmse: 0.205308\tvalid's rmse: 0.209662\n",
      "[2100]\ttrain's rmse: 0.205123\tvalid's rmse: 0.209646\n",
      "[2200]\ttrain's rmse: 0.204945\tvalid's rmse: 0.209644\n",
      "Early stopping, best iteration is:\n",
      "[2151]\ttrain's rmse: 0.205033\tvalid's rmse: 0.209643\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212308\tvalid's rmse: 0.211836\n",
      "[200]\ttrain's rmse: 0.210239\tvalid's rmse: 0.210014\n",
      "[300]\ttrain's rmse: 0.209577\tvalid's rmse: 0.20956\n",
      "[400]\ttrain's rmse: 0.209138\tvalid's rmse: 0.209306\n",
      "[500]\ttrain's rmse: 0.20879\tvalid's rmse: 0.209144\n",
      "[600]\ttrain's rmse: 0.208491\tvalid's rmse: 0.209041\n",
      "[700]\ttrain's rmse: 0.208223\tvalid's rmse: 0.20896\n",
      "[800]\ttrain's rmse: 0.207974\tvalid's rmse: 0.208905\n",
      "[900]\ttrain's rmse: 0.207735\tvalid's rmse: 0.208855\n",
      "[1000]\ttrain's rmse: 0.207506\tvalid's rmse: 0.208808\n",
      "[1100]\ttrain's rmse: 0.207286\tvalid's rmse: 0.208784\n",
      "[1200]\ttrain's rmse: 0.207071\tvalid's rmse: 0.208762\n",
      "[1300]\ttrain's rmse: 0.20687\tvalid's rmse: 0.208741\n",
      "[1400]\ttrain's rmse: 0.20667\tvalid's rmse: 0.208719\n",
      "[1500]\ttrain's rmse: 0.206474\tvalid's rmse: 0.20871\n",
      "[1600]\ttrain's rmse: 0.206286\tvalid's rmse: 0.2087\n",
      "[1700]\ttrain's rmse: 0.20609\tvalid's rmse: 0.208686\n",
      "[1800]\ttrain's rmse: 0.2059\tvalid's rmse: 0.208677\n",
      "[1900]\ttrain's rmse: 0.205715\tvalid's rmse: 0.208674\n",
      "[2000]\ttrain's rmse: 0.205526\tvalid's rmse: 0.208668\n",
      "[2100]\ttrain's rmse: 0.205345\tvalid's rmse: 0.208663\n",
      "[2200]\ttrain's rmse: 0.205162\tvalid's rmse: 0.208658\n",
      "[2300]\ttrain's rmse: 0.204986\tvalid's rmse: 0.20864\n",
      "Early stopping, best iteration is:\n",
      "[2322]\ttrain's rmse: 0.204945\tvalid's rmse: 0.208637\n",
      "Fold 3 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212266\tvalid's rmse: 0.212035\n",
      "[200]\ttrain's rmse: 0.210179\tvalid's rmse: 0.210182\n",
      "[300]\ttrain's rmse: 0.209528\tvalid's rmse: 0.209732\n",
      "[400]\ttrain's rmse: 0.209108\tvalid's rmse: 0.209498\n",
      "[500]\ttrain's rmse: 0.20876\tvalid's rmse: 0.209341\n",
      "[600]\ttrain's rmse: 0.208456\tvalid's rmse: 0.209236\n",
      "[700]\ttrain's rmse: 0.20819\tvalid's rmse: 0.209164\n",
      "[800]\ttrain's rmse: 0.207936\tvalid's rmse: 0.209098\n",
      "[900]\ttrain's rmse: 0.207697\tvalid's rmse: 0.209048\n",
      "[1000]\ttrain's rmse: 0.207468\tvalid's rmse: 0.209\n",
      "[1100]\ttrain's rmse: 0.207251\tvalid's rmse: 0.208971\n",
      "[1200]\ttrain's rmse: 0.207038\tvalid's rmse: 0.20894\n",
      "[1300]\ttrain's rmse: 0.206833\tvalid's rmse: 0.208916\n",
      "[1400]\ttrain's rmse: 0.206632\tvalid's rmse: 0.2089\n",
      "[1500]\ttrain's rmse: 0.206434\tvalid's rmse: 0.208884\n",
      "[1600]\ttrain's rmse: 0.206241\tvalid's rmse: 0.20887\n",
      "[1700]\ttrain's rmse: 0.206048\tvalid's rmse: 0.208861\n",
      "[1800]\ttrain's rmse: 0.205855\tvalid's rmse: 0.208853\n",
      "[1900]\ttrain's rmse: 0.205666\tvalid's rmse: 0.208838\n",
      "[2000]\ttrain's rmse: 0.205482\tvalid's rmse: 0.208834\n",
      "[2100]\ttrain's rmse: 0.205304\tvalid's rmse: 0.208825\n",
      "[2200]\ttrain's rmse: 0.205119\tvalid's rmse: 0.208824\n",
      "Early stopping, best iteration is:\n",
      "[2180]\ttrain's rmse: 0.205156\tvalid's rmse: 0.208822\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212076\tvalid's rmse: 0.212774\n",
      "[200]\ttrain's rmse: 0.209999\tvalid's rmse: 0.210967\n",
      "[300]\ttrain's rmse: 0.209317\tvalid's rmse: 0.210501\n",
      "[400]\ttrain's rmse: 0.2089\tvalid's rmse: 0.210276\n",
      "[500]\ttrain's rmse: 0.208556\tvalid's rmse: 0.210125\n",
      "[600]\ttrain's rmse: 0.20826\tvalid's rmse: 0.210022\n",
      "[700]\ttrain's rmse: 0.207984\tvalid's rmse: 0.209931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800]\ttrain's rmse: 0.207739\tvalid's rmse: 0.209878\n",
      "[900]\ttrain's rmse: 0.207506\tvalid's rmse: 0.209835\n",
      "[1000]\ttrain's rmse: 0.207275\tvalid's rmse: 0.209792\n",
      "[1100]\ttrain's rmse: 0.207064\tvalid's rmse: 0.209759\n",
      "[1200]\ttrain's rmse: 0.206853\tvalid's rmse: 0.209733\n",
      "[1300]\ttrain's rmse: 0.206648\tvalid's rmse: 0.20971\n",
      "[1400]\ttrain's rmse: 0.206446\tvalid's rmse: 0.209691\n",
      "[1500]\ttrain's rmse: 0.206246\tvalid's rmse: 0.209669\n",
      "[1600]\ttrain's rmse: 0.206051\tvalid's rmse: 0.209659\n",
      "[1700]\ttrain's rmse: 0.205865\tvalid's rmse: 0.209656\n",
      "[1800]\ttrain's rmse: 0.205675\tvalid's rmse: 0.209646\n",
      "[1900]\ttrain's rmse: 0.205492\tvalid's rmse: 0.20964\n",
      "[2000]\ttrain's rmse: 0.205308\tvalid's rmse: 0.20963\n",
      "[2100]\ttrain's rmse: 0.20512\tvalid's rmse: 0.209623\n",
      "[2200]\ttrain's rmse: 0.204941\tvalid's rmse: 0.209619\n",
      "Early stopping, best iteration is:\n",
      "[2218]\ttrain's rmse: 0.204906\tvalid's rmse: 0.209615\n",
      "Fold 5 completed.\n",
      "Training seed = 37\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212203\tvalid's rmse: 0.212332\n",
      "[200]\ttrain's rmse: 0.210138\tvalid's rmse: 0.210423\n",
      "[300]\ttrain's rmse: 0.209492\tvalid's rmse: 0.209971\n",
      "[400]\ttrain's rmse: 0.209073\tvalid's rmse: 0.209732\n",
      "[500]\ttrain's rmse: 0.208732\tvalid's rmse: 0.209577\n",
      "[600]\ttrain's rmse: 0.208434\tvalid's rmse: 0.209469\n",
      "[700]\ttrain's rmse: 0.208154\tvalid's rmse: 0.209376\n",
      "[800]\ttrain's rmse: 0.207904\tvalid's rmse: 0.20932\n",
      "[900]\ttrain's rmse: 0.207662\tvalid's rmse: 0.209265\n",
      "[1000]\ttrain's rmse: 0.207433\tvalid's rmse: 0.209222\n",
      "[1100]\ttrain's rmse: 0.207209\tvalid's rmse: 0.209193\n",
      "[1200]\ttrain's rmse: 0.206991\tvalid's rmse: 0.209164\n",
      "[1300]\ttrain's rmse: 0.206783\tvalid's rmse: 0.209136\n",
      "[1400]\ttrain's rmse: 0.206582\tvalid's rmse: 0.209122\n",
      "[1500]\ttrain's rmse: 0.206384\tvalid's rmse: 0.209107\n",
      "[1600]\ttrain's rmse: 0.206192\tvalid's rmse: 0.2091\n",
      "[1700]\ttrain's rmse: 0.206002\tvalid's rmse: 0.209081\n",
      "[1800]\ttrain's rmse: 0.205814\tvalid's rmse: 0.209069\n",
      "[1900]\ttrain's rmse: 0.205629\tvalid's rmse: 0.209063\n",
      "[2000]\ttrain's rmse: 0.205448\tvalid's rmse: 0.209058\n",
      "[2100]\ttrain's rmse: 0.205265\tvalid's rmse: 0.20905\n",
      "Early stopping, best iteration is:\n",
      "[2108]\ttrain's rmse: 0.20525\tvalid's rmse: 0.209048\n",
      "Fold 1 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212009\tvalid's rmse: 0.213082\n",
      "[200]\ttrain's rmse: 0.20993\tvalid's rmse: 0.211133\n",
      "[300]\ttrain's rmse: 0.209309\tvalid's rmse: 0.210677\n",
      "[400]\ttrain's rmse: 0.208887\tvalid's rmse: 0.21042\n",
      "[500]\ttrain's rmse: 0.208538\tvalid's rmse: 0.210241\n",
      "[600]\ttrain's rmse: 0.208242\tvalid's rmse: 0.210121\n",
      "[700]\ttrain's rmse: 0.20798\tvalid's rmse: 0.21004\n",
      "[800]\ttrain's rmse: 0.207741\tvalid's rmse: 0.209972\n",
      "[900]\ttrain's rmse: 0.207505\tvalid's rmse: 0.209913\n",
      "[1000]\ttrain's rmse: 0.207281\tvalid's rmse: 0.209867\n",
      "[1100]\ttrain's rmse: 0.207066\tvalid's rmse: 0.209832\n",
      "[1200]\ttrain's rmse: 0.206859\tvalid's rmse: 0.209796\n",
      "[1300]\ttrain's rmse: 0.20665\tvalid's rmse: 0.209769\n",
      "[1400]\ttrain's rmse: 0.206448\tvalid's rmse: 0.209746\n",
      "[1500]\ttrain's rmse: 0.206255\tvalid's rmse: 0.209724\n",
      "[1600]\ttrain's rmse: 0.206057\tvalid's rmse: 0.20971\n",
      "[1700]\ttrain's rmse: 0.205869\tvalid's rmse: 0.209702\n",
      "[1800]\ttrain's rmse: 0.205679\tvalid's rmse: 0.209683\n",
      "[1900]\ttrain's rmse: 0.205492\tvalid's rmse: 0.209676\n",
      "Early stopping, best iteration is:\n",
      "[1860]\ttrain's rmse: 0.205565\tvalid's rmse: 0.209676\n",
      "Fold 2 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212304\tvalid's rmse: 0.211827\n",
      "[200]\ttrain's rmse: 0.210233\tvalid's rmse: 0.210002\n",
      "[300]\ttrain's rmse: 0.209574\tvalid's rmse: 0.209533\n",
      "[400]\ttrain's rmse: 0.209167\tvalid's rmse: 0.209314\n",
      "[500]\ttrain's rmse: 0.208819\tvalid's rmse: 0.209156\n",
      "[600]\ttrain's rmse: 0.208516\tvalid's rmse: 0.209044\n",
      "[700]\ttrain's rmse: 0.208245\tvalid's rmse: 0.208965\n",
      "[800]\ttrain's rmse: 0.207991\tvalid's rmse: 0.208906\n",
      "[900]\ttrain's rmse: 0.20776\tvalid's rmse: 0.208864\n",
      "[1000]\ttrain's rmse: 0.207533\tvalid's rmse: 0.208828\n",
      "[1100]\ttrain's rmse: 0.207312\tvalid's rmse: 0.208801\n",
      "[1200]\ttrain's rmse: 0.207101\tvalid's rmse: 0.208774\n",
      "[1300]\ttrain's rmse: 0.206895\tvalid's rmse: 0.208757\n",
      "[1400]\ttrain's rmse: 0.206689\tvalid's rmse: 0.208727\n",
      "[1500]\ttrain's rmse: 0.206496\tvalid's rmse: 0.208717\n",
      "[1600]\ttrain's rmse: 0.206291\tvalid's rmse: 0.208702\n",
      "[1700]\ttrain's rmse: 0.206102\tvalid's rmse: 0.208694\n",
      "[1800]\ttrain's rmse: 0.205912\tvalid's rmse: 0.208686\n",
      "[1900]\ttrain's rmse: 0.205726\tvalid's rmse: 0.208672\n",
      "[2000]\ttrain's rmse: 0.20554\tvalid's rmse: 0.208671\n",
      "Early stopping, best iteration is:\n",
      "[1964]\ttrain's rmse: 0.205605\tvalid's rmse: 0.208668\n",
      "Fold 3 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212256\tvalid's rmse: 0.212026\n",
      "[200]\ttrain's rmse: 0.210185\tvalid's rmse: 0.210189\n",
      "[300]\ttrain's rmse: 0.209519\tvalid's rmse: 0.209727\n",
      "[400]\ttrain's rmse: 0.209106\tvalid's rmse: 0.209499\n",
      "[500]\ttrain's rmse: 0.208754\tvalid's rmse: 0.209342\n",
      "[600]\ttrain's rmse: 0.208452\tvalid's rmse: 0.209215\n",
      "[700]\ttrain's rmse: 0.208186\tvalid's rmse: 0.209136\n",
      "[800]\ttrain's rmse: 0.207931\tvalid's rmse: 0.209071\n",
      "[900]\ttrain's rmse: 0.207696\tvalid's rmse: 0.209032\n",
      "[1000]\ttrain's rmse: 0.20747\tvalid's rmse: 0.208996\n",
      "[1100]\ttrain's rmse: 0.207252\tvalid's rmse: 0.208966\n",
      "[1200]\ttrain's rmse: 0.207035\tvalid's rmse: 0.208937\n",
      "[1300]\ttrain's rmse: 0.206825\tvalid's rmse: 0.208912\n",
      "[1400]\ttrain's rmse: 0.206623\tvalid's rmse: 0.208891\n",
      "[1500]\ttrain's rmse: 0.206423\tvalid's rmse: 0.208884\n",
      "[1600]\ttrain's rmse: 0.206227\tvalid's rmse: 0.208868\n",
      "[1700]\ttrain's rmse: 0.206038\tvalid's rmse: 0.208854\n",
      "[1800]\ttrain's rmse: 0.20585\tvalid's rmse: 0.208847\n",
      "[1900]\ttrain's rmse: 0.205665\tvalid's rmse: 0.208844\n",
      "[2000]\ttrain's rmse: 0.205479\tvalid's rmse: 0.208836\n",
      "[2100]\ttrain's rmse: 0.205298\tvalid's rmse: 0.208831\n",
      "[2200]\ttrain's rmse: 0.205116\tvalid's rmse: 0.208819\n",
      "Early stopping, best iteration is:\n",
      "[2204]\ttrain's rmse: 0.205108\tvalid's rmse: 0.208818\n",
      "Fold 4 completed.\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttrain's rmse: 0.212087\tvalid's rmse: 0.212786\n",
      "[200]\ttrain's rmse: 0.210007\tvalid's rmse: 0.210979\n",
      "[300]\ttrain's rmse: 0.209341\tvalid's rmse: 0.210532\n",
      "[400]\ttrain's rmse: 0.208908\tvalid's rmse: 0.210299\n",
      "[500]\ttrain's rmse: 0.208567\tvalid's rmse: 0.21014\n",
      "[600]\ttrain's rmse: 0.208275\tvalid's rmse: 0.210038\n",
      "[700]\ttrain's rmse: 0.207996\tvalid's rmse: 0.20996\n",
      "[800]\ttrain's rmse: 0.207738\tvalid's rmse: 0.209895\n",
      "[900]\ttrain's rmse: 0.2075\tvalid's rmse: 0.209841\n",
      "[1000]\ttrain's rmse: 0.207277\tvalid's rmse: 0.209804\n",
      "[1100]\ttrain's rmse: 0.20706\tvalid's rmse: 0.209776\n",
      "[1200]\ttrain's rmse: 0.206849\tvalid's rmse: 0.209753\n",
      "[1300]\ttrain's rmse: 0.206643\tvalid's rmse: 0.209729\n",
      "[1400]\ttrain's rmse: 0.206443\tvalid's rmse: 0.209715\n",
      "[1500]\ttrain's rmse: 0.206245\tvalid's rmse: 0.209702\n",
      "[1600]\ttrain's rmse: 0.206052\tvalid's rmse: 0.209683\n",
      "[1700]\ttrain's rmse: 0.205861\tvalid's rmse: 0.209669\n",
      "[1800]\ttrain's rmse: 0.205669\tvalid's rmse: 0.209661\n",
      "[1900]\ttrain's rmse: 0.205484\tvalid's rmse: 0.209652\n",
      "[2000]\ttrain's rmse: 0.205302\tvalid's rmse: 0.209649\n",
      "[2100]\ttrain's rmse: 0.205118\tvalid's rmse: 0.209644\n",
      "[2200]\ttrain's rmse: 0.20494\tvalid's rmse: 0.209639\n",
      "Early stopping, best iteration is:\n",
      "[2168]\ttrain's rmse: 0.204996\tvalid's rmse: 0.209638\n",
      "Fold 5 completed.\n",
      "Training & bagging:  ridge\n",
      "Training seed = 19\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n",
      "Training seed = 23\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n",
      "Training seed = 37\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for k,v in config.items():\n",
    "    print('Training & bagging: ', k)\n",
    "    res = {\n",
    "        'val_oof': np.zeros((len(train_y),)),\n",
    "        'test_oof': np.zeros((test.shape[0],))\n",
    "    }\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print('Training seed =', seed)\n",
    "        if 'random_state' in v['param']:\n",
    "            v['param']['random_state'] = seed\n",
    "            \n",
    "        oof_val_pred, oof_test_pred, _ = get_oof_predictions(train, train_y, test, v['ml'], \n",
    "                                                          v['param'], seed=SEED, fit_params=v['fit_param'], \n",
    "                                                          use_eval_set= v['fit_param'] is not None)\n",
    "        \n",
    "        res['val_oof'] += oof_val_pred\n",
    "        res['test_oof'] += oof_test_pred\n",
    "    \n",
    "    res['val_oof'] /= len(seeds)\n",
    "    res['test_oof'] /= len(seeds)\n",
    "    \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\", usecols=['item_id'])\n",
    "pd.DataFrame(np.clip(results[1]['test_oof'],0,1), \n",
    "             index=test_df.item_id,\n",
    "             columns=['deal_probability']).to_csv('lgb_gbdt_meta_bagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1} train loss: 0.208867, valid loss:0.208642, loss_diff:-0.000225\n",
      "{'alpha': 1} train loss: 0.208712, valid loss:0.209269, loss_diff:0.000557\n",
      "{'alpha': 1} train loss: 0.208930, valid loss:0.208379, loss_diff:-0.000551\n",
      "{'alpha': 1} train loss: 0.208890, valid loss:0.208555, loss_diff:-0.000335\n",
      "{'alpha': 1} train loss: 0.208708, valid loss:0.209281, loss_diff:0.000573\n",
      "=================>{'alpha': 1} loss:0.208825\n",
      "{'alpha': 2} train loss: 0.208868, valid loss:0.208642, loss_diff:-0.000226\n",
      "{'alpha': 2} train loss: 0.208712, valid loss:0.209270, loss_diff:0.000558\n",
      "{'alpha': 2} train loss: 0.208931, valid loss:0.208378, loss_diff:-0.000552\n",
      "{'alpha': 2} train loss: 0.208890, valid loss:0.208555, loss_diff:-0.000335\n",
      "{'alpha': 2} train loss: 0.208708, valid loss:0.209281, loss_diff:0.000573\n",
      "=================>{'alpha': 2} loss:0.208825\n",
      "{'alpha': 4} train loss: 0.208868, valid loss:0.208643, loss_diff:-0.000226\n",
      "{'alpha': 4} train loss: 0.208713, valid loss:0.209273, loss_diff:0.000560\n",
      "{'alpha': 4} train loss: 0.208931, valid loss:0.208377, loss_diff:-0.000554\n",
      "{'alpha': 4} train loss: 0.208891, valid loss:0.208556, loss_diff:-0.000335\n",
      "{'alpha': 4} train loss: 0.208709, valid loss:0.209282, loss_diff:0.000573\n",
      "=================>{'alpha': 4} loss:0.208826\n",
      "{'alpha': 8} train loss: 0.208870, valid loss:0.208644, loss_diff:-0.000226\n",
      "{'alpha': 8} train loss: 0.208714, valid loss:0.209277, loss_diff:0.000563\n",
      "{'alpha': 8} train loss: 0.208933, valid loss:0.208375, loss_diff:-0.000558\n",
      "{'alpha': 8} train loss: 0.208892, valid loss:0.208558, loss_diff:-0.000334\n",
      "{'alpha': 8} train loss: 0.208710, valid loss:0.209283, loss_diff:0.000572\n",
      "=================>{'alpha': 8} loss:0.208827\n",
      "Best params: {'alpha': 1} \tbest loss: 0.208825270033\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>val_loss_mean</th>\n",
       "      <th>val_loss_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'alpha': 2}</td>\n",
       "      <td>0.208825</td>\n",
       "      <td>0.000377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'alpha': 4}</td>\n",
       "      <td>0.208826</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'alpha': 8}</td>\n",
       "      <td>0.208827</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          param  val_loss_mean  val_loss_std\n",
       "0  {'alpha': 1}       0.208825      0.000377\n",
       "1  {'alpha': 2}       0.208825      0.000377\n",
       "2  {'alpha': 4}       0.208826      0.000378\n",
       "3  {'alpha': 8}       0.208827      0.000380"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train = pd.DataFrame()\n",
    "new_test = pd.DataFrame()\n",
    "\n",
    "new_train['f1'] = results[0]['val_oof']\n",
    "new_train['f2'] = results[1]['val_oof']\n",
    "new_train['f3'] = results[2]['val_oof']\n",
    "\n",
    "new_test['f1'] = results[0]['test_oof']\n",
    "new_test['f2'] = results[1]['test_oof']\n",
    "new_test['f3'] = results[2]['test_oof']\n",
    "\n",
    "ml = model_loader(model_type='rg')\n",
    "default_params = {\n",
    "    'alpha': 1.0, \n",
    "    'fit_intercept': True, \n",
    "    'normalize': False, \n",
    "    'copy_X': True, \n",
    "    'max_iter': None, \n",
    "    'tol': 0.001, \n",
    "    'solver':'auto', \n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "fit_param = None\n",
    "\n",
    "try_params = {\n",
    "    'alpha':[1,2,4,8]\n",
    "}\n",
    "\n",
    "fit_params(new_train, train_y, ml, default_params, try_params, fit_params=fit_param, seed=SEED, use_eval_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994878</td>\n",
       "      <td>0.989767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>0.994878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3</th>\n",
       "      <td>0.989767</td>\n",
       "      <td>0.988676</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1        f2        f3\n",
       "f1  1.000000  0.994878  0.989767\n",
       "f2  0.994878  1.000000  0.988676\n",
       "f3  0.989767  0.988676  1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train[['f1', 'f2', 'f3']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n"
     ]
    }
   ],
   "source": [
    "_, oof_test_pred, _ = get_oof_predictions( new_train, train_y, new_test, ml, \n",
    "                                                      default_params, seed=SEED, fit_params=fit_param, \n",
    "                                                      use_eval_set= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\", usecols=['item_id'])\n",
    "pd.DataFrame(np.clip(oof_test_pred,0,1), \n",
    "             index=test_df.item_id,\n",
    "             columns=['deal_probability']).to_csv('stack_bagging_blend_no_xgb_meta_ridge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 0/98 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score updated: 0.209673  coefficient=> 0.01, 0.01, 0.98\n",
      "best score updated: 0.209658  coefficient=> 0.01, 0.02, 0.97\n",
      "best score updated: 0.209643  coefficient=> 0.01, 0.03, 0.96\n",
      "best score updated: 0.209628  coefficient=> 0.01, 0.04, 0.95\n",
      "best score updated: 0.209614  coefficient=> 0.01, 0.05, 0.94\n",
      "best score updated: 0.209599  coefficient=> 0.01, 0.060000000000000005, 0.9299999999999999\n",
      "best score updated: 0.209585  coefficient=> 0.01, 0.06999999999999999, 0.92\n",
      "best score updated: 0.209571  coefficient=> 0.01, 0.08, 0.91\n",
      "best score updated: 0.209557  coefficient=> 0.01, 0.09, 0.9\n",
      "best score updated: 0.209543  coefficient=> 0.01, 0.09999999999999999, 0.89\n",
      "best score updated: 0.209529  coefficient=> 0.01, 0.11, 0.88\n",
      "best score updated: 0.209516  coefficient=> 0.01, 0.12, 0.87\n",
      "best score updated: 0.209502  coefficient=> 0.01, 0.13, 0.86\n",
      "best score updated: 0.209489  coefficient=> 0.01, 0.14, 0.85\n",
      "best score updated: 0.209476  coefficient=> 0.01, 0.15000000000000002, 0.84\n",
      "best score updated: 0.209464  coefficient=> 0.01, 0.16, 0.83\n",
      "best score updated: 0.209451  coefficient=> 0.01, 0.17, 0.82\n",
      "best score updated: 0.209439  coefficient=> 0.01, 0.18000000000000002, 0.8099999999999999\n",
      "best score updated: 0.209427  coefficient=> 0.01, 0.19, 0.8\n",
      "best score updated: 0.209415  coefficient=> 0.01, 0.2, 0.79\n",
      "best score updated: 0.209403  coefficient=> 0.01, 0.21000000000000002, 0.78\n",
      "best score updated: 0.209392  coefficient=> 0.01, 0.22, 0.77\n",
      "best score updated: 0.209380  coefficient=> 0.01, 0.23, 0.76\n",
      "best score updated: 0.209369  coefficient=> 0.01, 0.24000000000000002, 0.75\n",
      "best score updated: 0.209358  coefficient=> 0.01, 0.25, 0.74\n",
      "best score updated: 0.209347  coefficient=> 0.01, 0.26, 0.73\n",
      "best score updated: 0.209337  coefficient=> 0.01, 0.27, 0.72\n",
      "best score updated: 0.209326  coefficient=> 0.01, 0.28, 0.71\n",
      "best score updated: 0.209316  coefficient=> 0.01, 0.29000000000000004, 0.7\n",
      "best score updated: 0.209306  coefficient=> 0.01, 0.3, 0.69\n",
      "best score updated: 0.209296  coefficient=> 0.01, 0.31, 0.6799999999999999\n",
      "best score updated: 0.209287  coefficient=> 0.01, 0.32, 0.6699999999999999\n",
      "best score updated: 0.209277  coefficient=> 0.01, 0.33, 0.6599999999999999\n",
      "best score updated: 0.209268  coefficient=> 0.01, 0.34, 0.6499999999999999\n",
      "best score updated: 0.209259  coefficient=> 0.01, 0.35000000000000003, 0.6399999999999999\n",
      "best score updated: 0.209250  coefficient=> 0.01, 0.36000000000000004, 0.6299999999999999\n",
      "best score updated: 0.209242  coefficient=> 0.01, 0.37, 0.62\n",
      "best score updated: 0.209233  coefficient=> 0.01, 0.38, 0.61\n",
      "best score updated: 0.209225  coefficient=> 0.01, 0.39, 0.6\n",
      "best score updated: 0.209217  coefficient=> 0.01, 0.4, 0.59\n",
      "best score updated: 0.209209  coefficient=> 0.01, 0.41000000000000003, 0.58\n",
      "best score updated: 0.209202  coefficient=> 0.01, 0.42000000000000004, 0.57\n",
      "best score updated: 0.209195  coefficient=> 0.01, 0.43, 0.56\n",
      "best score updated: 0.209187  coefficient=> 0.01, 0.44, 0.55\n",
      "best score updated: 0.209180  coefficient=> 0.01, 0.45, 0.54\n",
      "best score updated: 0.209174  coefficient=> 0.01, 0.46, 0.53\n",
      "best score updated: 0.209167  coefficient=> 0.01, 0.47000000000000003, 0.52\n",
      "best score updated: 0.209161  coefficient=> 0.01, 0.48000000000000004, 0.51\n",
      "best score updated: 0.209155  coefficient=> 0.01, 0.49, 0.5\n",
      "best score updated: 0.209149  coefficient=> 0.01, 0.5, 0.49\n",
      "best score updated: 0.209143  coefficient=> 0.01, 0.51, 0.48\n",
      "best score updated: 0.209137  coefficient=> 0.01, 0.52, 0.47\n",
      "best score updated: 0.209132  coefficient=> 0.01, 0.53, 0.45999999999999996\n",
      "best score updated: 0.209127  coefficient=> 0.01, 0.54, 0.44999999999999996\n",
      "best score updated: 0.209122  coefficient=> 0.01, 0.55, 0.43999999999999995\n",
      "best score updated: 0.209117  coefficient=> 0.01, 0.56, 0.42999999999999994\n",
      "best score updated: 0.209113  coefficient=> 0.01, 0.5700000000000001, 0.41999999999999993\n",
      "best score updated: 0.209108  coefficient=> 0.01, 0.5800000000000001, 0.4099999999999999\n",
      "best score updated: 0.209104  coefficient=> 0.01, 0.59, 0.4\n",
      "best score updated: 0.209101  coefficient=> 0.01, 0.6, 0.39\n",
      "best score updated: 0.209097  coefficient=> 0.01, 0.61, 0.38\n",
      "best score updated: 0.209094  coefficient=> 0.01, 0.62, 0.37\n",
      "best score updated: 0.209090  coefficient=> 0.01, 0.63, 0.36\n",
      "best score updated: 0.209087  coefficient=> 0.01, 0.64, 0.35\n",
      "best score updated: 0.209084  coefficient=> 0.01, 0.65, 0.33999999999999997\n",
      "best score updated: 0.209082  coefficient=> 0.01, 0.66, 0.32999999999999996\n",
      "best score updated: 0.209080  coefficient=> 0.01, 0.67, 0.31999999999999995\n",
      "best score updated: 0.209077  coefficient=> 0.01, 0.68, 0.30999999999999994\n",
      "best score updated: 0.209076  coefficient=> 0.01, 0.6900000000000001, 0.29999999999999993\n",
      "best score updated: 0.209074  coefficient=> 0.01, 0.7000000000000001, 0.2899999999999999\n",
      "best score updated: 0.209072  coefficient=> 0.01, 0.7100000000000001, 0.2799999999999999\n",
      "best score updated: 0.209071  coefficient=> 0.01, 0.72, 0.27\n",
      "best score updated: 0.209070  coefficient=> 0.01, 0.73, 0.26\n",
      "best score updated: 0.209069  coefficient=> 0.01, 0.74, 0.25\n",
      "best score updated: 0.209069  coefficient=> 0.01, 0.75, 0.24\n",
      "best score updated: 0.209068  coefficient=> 0.01, 0.76, 0.22999999999999998\n",
      "best score updated: 0.209068  coefficient=> 0.01, 0.77, 0.21999999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▍                                            | 42/98 [02:36<03:28,  3.72s/it]"
     ]
    }
   ],
   "source": [
    "best_blend_test = None\n",
    "best_score = None\n",
    "min_w = 0.01\n",
    "for a in tqdm(np.arange(min_w, 1+min_w-min_w*2, min_w)):\n",
    "    for b in np.arange(min_w, 1-a+min_w-min_w*1, min_w):\n",
    "        c = 1-a-b\n",
    "        combined_res = a*results[0]['val_oof'] + \\\n",
    "                       b*results[1]['val_oof'] + \\\n",
    "                       c*results[2]['val_oof']\n",
    "\n",
    "        score = clip_rmse(train_y, combined_res)\n",
    "        if best_score is None or score < best_score:\n",
    "            best_score = score\n",
    "            print('best score updated: {:.6f}'.format(best_score), ' coefficient=> {}, {}, {}'.format(a, b, c))\n",
    "            best_blend_test =  a*results[0]['test_oof'] + \\\n",
    "                               b*results[1]['test_oof'] + \\\n",
    "                               c*results[2]['test_oof']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_blend_test = None\n",
    "best_score = None\n",
    "min_w = 0.01\n",
    "for a in np.arange(min_w, 1+min_w-min_w*4, min_w):\n",
    "    for b in np.arange(min_w, 1-a+min_w-min_w*3, min_w):\n",
    "        for c in np.arange(min_w, 1-a-b+min_w-min_w*2, min_w):\n",
    "            for d in np.arange(min_w, 1-a-b-c+min_w-min_w*1, min_w):\n",
    "                e = 1-a-b-c-d\n",
    "                combined_res = a*results[0]['val_oof'] + \\\n",
    "                               b*results[1]['val_oof'] + \\\n",
    "                               c*results[2]['val_oof'] + \\\n",
    "                               d*results[3]['val_oof'] + \\ \n",
    "                               e*results[4]['val_oof']\n",
    "                \n",
    "                score = clip_rmse(train_y, combined_res)\n",
    "                if best_score is None or score < best_score:\n",
    "                    best_score = score\n",
    "                    print('best score updated:', best_score)\n",
    "                    best_blend_test =  a*results[0]['test_oof'] + \\\n",
    "                                       b*results[1]['test_oof'] + \\\n",
    "                                       c*results[2]['test_oof'] + \\\n",
    "                                       d*results[3]['test_oof'] + \\ \n",
    "                                       e*results[4]['test_oof']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\", usecols=['item_id'])\n",
    "pd.DataFrame(np.clip(best_blend_test,0,1), \n",
    "             index=test_df.item_id,\n",
    "             columns=['deal_probability']).to_csv('stack_bagging_blend_no_xgb_meta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
